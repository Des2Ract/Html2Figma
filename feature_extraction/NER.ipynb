{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1370 JSON files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   4%|â–Ž         | 49/1370 [01:22<47:36,  2.16s/it] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import h5py\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FigmaHTMLFeatureExtractor:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_model_name: str = 'all-MiniLM-L6-v2',\n",
    "        node_type_embedding_dim: int = 50,\n",
    "        output_format: str = 'hdf5'  # 'hdf5' or 'csv'\n",
    "    ):\n",
    "        # Models and embeddings configuration\n",
    "        self.semantic_model = SentenceTransformer(semantic_model_name)\n",
    "        self.text_embedding_dim = 384  # Dimension of 'all-MiniLM-L6-v2'\n",
    "        self.node_name_embedding_dim = 384  # Using the same model for node names\n",
    "        \n",
    "        # Initialize node type embedding\n",
    "        self.node_types = self._get_node_types()\n",
    "        self.node_type_to_idx = {node_type: idx for idx, node_type in enumerate(self.node_types)}\n",
    "        self.node_type_embedding_dim = node_type_embedding_dim\n",
    "        self.node_type_embedding_layer = nn.Embedding(len(self.node_types), self.node_type_embedding_dim)\n",
    "        \n",
    "        # Tag mapping and cleaning configuration\n",
    "        self.tag_mapping = self._get_tag_mapping()\n",
    "        self.custom_tag_removal_pattern = self._get_custom_tag_removal_pattern()\n",
    "        self.default_tag = \"DIV\"\n",
    "        self.icon_like_node_types = {\"VECTOR\", \"INSTANCE\", \"COMPONENT\", \"SHAPE\", \"SVG_ICON\"}\n",
    "        \n",
    "        # Output configuration\n",
    "        self.output_format = output_format\n",
    "        \n",
    "        # Counters for statistics\n",
    "        self.stats = {\n",
    "            \"files_processed\": 0,\n",
    "            \"nodes_processed\": 0,\n",
    "            \"json_errors\": 0,\n",
    "            \"tag_mappings\": {},\n",
    "            \"unique_node_types\": set()\n",
    "        }\n",
    "\n",
    "    def _get_node_types(self) -> List[str]:\n",
    "        \"\"\"Define the node types for embedding.\"\"\"\n",
    "        return [\n",
    "            \"TEXT\", \"RECTANGLE\", \"GROUP\", \"ELLIPSE\", \"FRAME\", \"VECTOR\", \"STAR\", \"LINE\", \n",
    "            \"POLYGON\", \"BOOLEAN_OPERATION\", \"SLICE\", \"COMPONENT\", \"INSTANCE\", \"COMPONENT_SET\", \n",
    "            \"DOCUMENT\", \"CANVAS\", \"SECTION\", \"SHAPE_WITH_TEXT\", \"STICKY\", \"TABLE\", \"WASHI_TAPE\", \n",
    "            \"CONNECTOR\", \"HIGHLIGHT\", \"WIDGET\", \"EMBED\", \"LINK\", \"LINK_UNFURL\", \"MEDIA\", \"CODE_BLOCK\", \n",
    "            \"STAMP\", \"COMMENT\", \"FREEFORM\", \"TIMELINE\", \"STICKER\", \"SHAPE\", \"ARROW\", \"CALL_OUT\", \n",
    "            \"FLOW\", \"TEXT_AREA\", \"TEXT_FIELD\", \"BUTTON\", \"CHECKBOX\", \"RADIO\", \"TOGGLE\", \"SLIDER\", \n",
    "            \"DROPDOWN\", \"COMBOBOX\", \"LIST\", \"TABLE_CELL\", \"TABLE_ROW\", \"TABLE_COLUMN\", \"TABLE_SECTION\", \n",
    "            \"TABLE_HEADER\", \"TABLE_FOOTER\", \"TABLE_BODY\", \"TABLE_CAPTION\", \"TABLE_COLGROUP\", \"TABLE_COL\", \n",
    "            \"TABLE_THEAD\", \"TABLE_TBODY\", \"TABLE_TFOOT\", \"TABLE_TR\", \"TABLE_TH\", \"TABLE_TD\", \n",
    "            \"UNKNOWN_TYPE\"\n",
    "        ]\n",
    "\n",
    "    def _get_tag_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Define mapping for tag consolidation.\"\"\"\n",
    "        return {\n",
    "            \"ARTICLE\": \"DIV\", \"DIV\": \"DIV\", \"FIGURE\": \"DIV\", \"FOOTER\": \"DIV\", \"HEADER\": \"DIV\", \"NAV\": \"DIV\", \"MAIN\": \"DIV\", \"IFRAME\": \"DIV\",\n",
    "            \"BODY\" : \"DIV\", \"FORM\" : \"DIV\", \"TABLE\": \"DIV\", \"THEAD\":\"DIV\" , \"TBODY\": \"DIV\", \"SECTION\": \"DIV\",\"ASIDE\":\"DIV\", \n",
    "            \n",
    "            \"UL\" : \"LIST\", \"OL\" : \"LIST\", \"DL\": \"LIST\",\n",
    "            \n",
    "            \"H1\": \"P\", \"H2\": \"P\", \"H3\": \"P\", \"H4\": \"P\", \"H5\": \"P\", \"H6\": \"P\",\"SUP\": \"P\",\"SUB\": \"P\", \"BIG\": \"P\",\n",
    "            \"P\": \"P\", \"CAPTION\": \"P\", \"FIGCAPTION\": \"P\", \"B\": \"P\", \"EM\": \"P\", \"I\": \"P\", \"TD\": \"P\", \"TH\": \"P\", \"TR\": \"P\",\"PRE\":\"P\",\n",
    "            \"U\": \"P\", \"TIME\": \"P\", \"TXT\": \"P\", \"ABBR\": \"P\",\"SMALL\": \"P\",\"STRONG\": \"P\",\"SUMMARY\": \"P\",\"SPAN\": \"P\", \"LABEL\": \"P\",\"LI\":\"P\",\"DD\":\"P\",\n",
    "            \"A\":\"P\",\"BLOCKQUOTE\":\"P\",\"CODE\":\"P\",\n",
    "            \n",
    "            \"PICTURE\": \"IMG\" , \"VIDEO\": \"IMG\",\n",
    "            \"SELECT\": \"INPUT\",\"TEXTAREA\": \"INPUT\",\n",
    "            \"VECTOR\": \"SVG\",\"ICON\":\"SVG\",\n",
    "            \n",
    "            \"UNK\": \"CONTAINER\"\n",
    "        }\n",
    "\n",
    "    def _get_custom_tag_removal_pattern(self) -> str:\n",
    "        \"\"\"Define regex pattern for removing problematic tags.\"\"\"\n",
    "        return r'[-:]|\\b(DETAILS|CANVAS|FIELDSET|COLGROUP|COL|CNX|ADDRESS|CITE|S|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|FRONT|Q|SEARCH|SLOT|AD|ADSLOT|BLINK|BOLD|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|PATH|PROGRESS|STRIKE|SWAL|TEXT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1)\\b'\n",
    "\n",
    "    def clean_and_map_tag(self, raw_tag: str) -> str:\n",
    "        \"\"\"Clean and map a raw HTML tag to a canonical form.\"\"\"\n",
    "        if not raw_tag:\n",
    "            return self.default_tag\n",
    "        raw_tag = raw_tag.upper()\n",
    "        cleaned_tag = self.tag_mapping.get(raw_tag, raw_tag)\n",
    "        if re.search(self.custom_tag_removal_pattern, cleaned_tag, re.IGNORECASE):\n",
    "            cleaned_tag = self.default_tag\n",
    "        final_tag = self.tag_mapping.get(cleaned_tag, cleaned_tag)\n",
    "        if final_tag != raw_tag:\n",
    "            self.stats[\"tag_mappings\"][raw_tag] = self.stats[\"tag_mappings\"].get(raw_tag, 0) + 1\n",
    "        return final_tag\n",
    "\n",
    "    def determine_bioes_label(self, base_tag: str) -> Tuple[str, Optional[str]]:\n",
    "        bioes_label = \"\"\n",
    "        \n",
    "        if base_tag == \"CONTAINER\":\n",
    "            bioes_label = \"B_CONTAINER\"\n",
    "        else:\n",
    "            bioes_label = base_tag\n",
    "        return bioes_label\n",
    "\n",
    "    def extract_features(self, \n",
    "                        node_data_item: Dict, \n",
    "                        current_body_width: float,\n",
    "                        sequence_id: str,\n",
    "                        parent_node_height: Optional[float] = None,\n",
    "                        parent_base_tag: Optional[str] = None,\n",
    "                        depth: int = 0,\n",
    "                        position_in_siblings: int = 0,\n",
    "                        total_siblings: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract features from a node and its children recursively.\n",
    "        Appends 'E_CONTAINER' after processing children of 'B_CONTAINER' nodes.\n",
    "        \"\"\"\n",
    "        features_and_labels_list = []\n",
    "        node_dict = node_data_item.get(\"node\", {})\n",
    "        raw_tag = node_data_item.get(\"tag\", \"UNK\").upper()\n",
    "\n",
    "        # Determine base tag\n",
    "        has_children = bool(node_data_item.get(\"children\"))\n",
    "        base_tag = self.clean_and_map_tag(raw_tag)\n",
    "        \n",
    "        # Determine label\n",
    "        bioes_label = self.determine_bioes_label(base_tag)\n",
    "        \n",
    "        # Node type embedding\n",
    "        node_type_str = node_dict.get(\"type\", \"UNKNOWN_TYPE\")\n",
    "        self.stats[\"unique_node_types\"].add(node_type_str)\n",
    "        node_type_idx = self.node_type_to_idx.get(node_type_str, self.node_type_to_idx.get(\"UNKNOWN_TYPE\", 0))\n",
    "        node_type_emb = self.node_type_embedding_layer(torch.tensor(node_type_idx)).detach().numpy()\n",
    "        \n",
    "        # Text embedding\n",
    "        text_content = node_dict.get(\"characters\", \"\").strip()\n",
    "        text_emb = self.semantic_model.encode(text_content) if node_type_str == \"TEXT\" and text_content else np.zeros(self.text_embedding_dim)\n",
    "        \n",
    "        # Node name embedding\n",
    "        node_name = node_data_item.get(\"name\", \"\").strip()\n",
    "        node_name_emb = self.semantic_model.encode(node_name) if node_name and (node_type_str in self.icon_like_node_types or \"icon\" in node_name.lower()) else np.zeros(self.node_name_embedding_dim)\n",
    "        \n",
    "        # Numerical & structural features\n",
    "        eps = 1e-6\n",
    "        node_width = float(node_dict.get(\"width\", 0))\n",
    "        node_height = float(node_dict.get(\"height\", 0))\n",
    "        aspect_ratio = node_width / (node_height + eps) if node_height > 0 else 0\n",
    "        normalized_width = node_width / (current_body_width + eps) if current_body_width > 0 else 0\n",
    "        normalized_height = node_height / (parent_node_height + eps) if parent_node_height and parent_node_height > 0 else 0\n",
    "        \n",
    "        x_position = float(node_dict.get(\"x\", 0))\n",
    "        y_position = float(node_dict.get(\"y\", 0))\n",
    "        normalized_x = x_position / (current_body_width + eps) if current_body_width > 0 else 0\n",
    "        normalized_y = y_position / (parent_node_height + eps) if parent_node_height and parent_node_height > 0 else 0\n",
    "        \n",
    "        normalized_depth = min(depth / 20.0, 1.0)\n",
    "        normalized_position = position_in_siblings / (total_siblings + eps)\n",
    "        \n",
    "        bg_color = [0, 0, 0, 0]\n",
    "        fills = node_dict.get(\"fills\", [])\n",
    "        if fills and isinstance(fills, list) and len(fills) > 0 :\n",
    "            color = fills[0][\"color\"]\n",
    "            if color:\n",
    "                bg_color = [color.get(k, 0) for k in (\"r\", \"g\", \"b\", \"a\")]\n",
    "        \n",
    "        font_size = float(node_dict.get(\"fontSize\", 0)) / 100.0\n",
    "        flex_direction = 1 if node_dict.get(\"flexDirection\", \"\") == \"column\" else 0\n",
    "        \n",
    "        # Combine features\n",
    "        feature_vector = np.concatenate([\n",
    "            node_type_emb,\n",
    "            text_emb,\n",
    "            node_name_emb,\n",
    "            [normalized_width, normalized_height, aspect_ratio,\n",
    "             normalized_x, normalized_y,\n",
    "             normalized_depth, normalized_position,\n",
    "             *bg_color, font_size, flex_direction]\n",
    "        ])\n",
    "        \n",
    "        # Add node features to list\n",
    "        features_and_labels_list.append({\n",
    "            \"feature_vector\": feature_vector,\n",
    "            \"tag\": bioes_label\n",
    "        })\n",
    "        \n",
    "        self.stats[\"nodes_processed\"] += 1\n",
    "        \n",
    "        # Process children if any\n",
    "        if has_children:\n",
    "            children = node_data_item[\"children\"]\n",
    "            total_children = len(children)\n",
    "            for child_idx, child_node in enumerate(children):\n",
    "                child_features = self.extract_features(\n",
    "                    node_data_item=child_node,\n",
    "                    current_body_width=current_body_width,\n",
    "                    sequence_id=sequence_id,\n",
    "                    parent_node_height=node_height,\n",
    "                    parent_base_tag=base_tag,\n",
    "                    depth=depth + 1,\n",
    "                    position_in_siblings=child_idx,\n",
    "                    total_siblings=total_children\n",
    "                )\n",
    "                features_and_labels_list.extend(child_features)\n",
    "            \n",
    "            # Append E_CONTAINER after processing children if this is a B_CONTAINER\n",
    "            if bioes_label == \"B_CONTAINER\":\n",
    "                e_container_feature = {\n",
    "                    \"feature_vector\": np.zeros_like(feature_vector),\n",
    "                    \"tag\": \"E_CONTAINER\"\n",
    "                }\n",
    "                features_and_labels_list.append(e_container_feature)\n",
    "                \n",
    "        return features_and_labels_list\n",
    "\n",
    "    def process_file(self, file_path: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Process a single JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "            sequence_id = os.path.basename(file_path).replace(\".json\", \"\")\n",
    "            root_node_info = json_data.get(\"node\", {})\n",
    "            body_width = float(root_node_info.get(\"width\", 1000.0)) or 1000.0\n",
    "            features = self.extract_features(\n",
    "                node_data_item=json_data,\n",
    "                current_body_width=body_width,\n",
    "                sequence_id=sequence_id\n",
    "            )\n",
    "            \n",
    "            # specify end of website\n",
    "            e_website_feature = {\n",
    "                \"feature_vector\": np.ones_like(features[0][\"feature_vector\"]),\n",
    "                \"tag\": \"E_WEBSITE\",\n",
    "            }\n",
    "            features.append(e_website_feature)\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            self.stats[\"json_errors\"] += 1\n",
    "            return None\n",
    "\n",
    "    def process_directory(self, input_dir: str, output_path: str) -> None:\n",
    "        \"\"\"Process and save features from all JSON files incrementally.\"\"\"\n",
    "        if not os.path.exists(input_dir):\n",
    "            print(f\"Error: Input directory '{input_dir}' does not exist.\")\n",
    "            return\n",
    "        json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "        if not json_files:\n",
    "            print(f\"No JSON files found in {input_dir}\")\n",
    "            return\n",
    "        print(f\"Found {len(json_files)} JSON files to process.\")\n",
    "\n",
    "        first_file = True  # Flag to control writing headers or overwriting\n",
    "\n",
    "        for file_name in tqdm(json_files, desc=\"Processing files\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            features = self.process_file(file_path)\n",
    "            if features:\n",
    "                self._save_features(features, output_path, append=not first_file)\n",
    "                self.stats[\"files_processed\"] += 1\n",
    "                first_file = False\n",
    "\n",
    "        self._print_stats()\n",
    "\n",
    "    def _save_features(self, features: List[Dict], output_path: str, append: bool = False) -> None:\n",
    "        \"\"\"Save extracted features to file incrementally.\"\"\"\n",
    "        if not features:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"feature_vector\": [f[\"feature_vector\"].tolist() for f in features],\n",
    "            \"tag\": [f[\"tag\"] for f in features]\n",
    "        })\n",
    "\n",
    "        if self.output_format == 'csv':\n",
    "            df.to_csv(output_path, index=False, header=not append, mode='a' if append else 'w')\n",
    "        elif self.output_format == 'parquet':\n",
    "            df.to_parquet(output_path, index=False, append=append)\n",
    "        elif self.output_format == 'hdf5':\n",
    "            with h5py.File(output_path, 'a' if append else 'w') as f:\n",
    "                for column in df.columns:\n",
    "                    data = df[column].apply(lambda x: x if not isinstance(x, list) else np.array(x)).values\n",
    "                    if column == \"feature_vector\":\n",
    "                        data = np.vstack(data)\n",
    "                        dtype = np.float32\n",
    "                    elif df[column].dtype == object:\n",
    "                        dtype = 'S100'\n",
    "                        data = np.array(data, dtype=dtype)\n",
    "                    else:\n",
    "                        dtype = df[column].dtype\n",
    "                        data = np.array(data)\n",
    "\n",
    "                    if column in f:\n",
    "                        # Resize existing dataset and append\n",
    "                        old_size = f[column].shape[0]\n",
    "                        new_size = old_size + data.shape[0]\n",
    "                        f[column].resize((new_size,) + f[column].shape[1:])\n",
    "                        f[column][old_size:] = data\n",
    "                    else:\n",
    "                        maxshape = (None,) + data.shape[1:] if len(data.shape) > 1 else (None,)\n",
    "                        f.create_dataset(column, data=data, maxshape=maxshape, chunks=True)\n",
    "\n",
    "                f.attrs['num_samples'] = f[column].shape[0]\n",
    "                f.attrs['feature_dim'] = data.shape[1] if len(data.shape) > 1 else 1\n",
    "\n",
    "    def _print_stats(self) -> None:\n",
    "        \"\"\"Print processing statistics.\"\"\"\n",
    "        print(\"\\n--- Processing Statistics ---\")\n",
    "        print(f\"Files processed: {self.stats['files_processed']}\")\n",
    "        print(f\"Nodes processed: {self.stats['nodes_processed']}\")\n",
    "        print(f\"JSON errors: {self.stats['json_errors']}\")\n",
    "        print(f\"Unique node types: {len(self.stats['unique_node_types'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"../modified_json_data\"   # \"../experimental_json\"\n",
    "    output_path = \"figma_dataset_custom.csv\"\n",
    "    extractor = FigmaHTMLFeatureExtractor(\n",
    "        semantic_model_name='all-MiniLM-L6-v2',\n",
    "        node_type_embedding_dim=50,\n",
    "        output_format='csv'\n",
    "    )\n",
    "    extractor.process_directory(input_dir, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef478612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_CONTAINER' 'P' 'DIV' 'HR' 'E_CONTAINER' 'BUTTON' 'SVG' 'LIST' 'IMG'\n",
      " 'INPUT']\n"
     ]
    }
   ],
   "source": [
    "# # Read the CSV file\n",
    "# df = pd.read_csv(output_path)\n",
    "\n",
    "# # Get unique values in the 'tag' column\n",
    "# unique_tags = df['tag'].unique()\n",
    "\n",
    "# # Print the unique tags\n",
    "# print(unique_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
