{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_width = None\n",
    "\n",
    "def extract_semantic_features(node, text_nodes_with_content):\n",
    "    \"\"\"\n",
    "    Calculate the semantic features from the nearest text node.\n",
    "    \n",
    "    Args:\n",
    "    node (dict): Current node being processed\n",
    "    text_nodes_with_content (list): List of text nodes with their x, y coordinates and text content\n",
    "    \n",
    "    Returns:\n",
    "    dict: Semantic feature dictionary\n",
    "    \"\"\"\n",
    "    if not text_nodes_with_content:\n",
    "        return {\n",
    "            \"nearest_text_semantic_vector\": [0] * 384,  # Default zero vector\n",
    "            \"nearest_text_semantic_distance\": 9999999\n",
    "        }\n",
    "    \n",
    "    # Get current node's center coordinates\n",
    "    node_data = node.get(\"node\", {})\n",
    "    x = node_data.get(\"x\", 0) + node_data.get(\"width\", 0) / 2\n",
    "    y = node_data.get(\"y\", 0) + node_data.get(\"height\", 0) / 2\n",
    "    \n",
    "    # Calculate Euclidean distances to all text nodes and find the nearest\n",
    "    min_distance = float('inf')\n",
    "    nearest_text_node = None\n",
    "    \n",
    "    for text_node in text_nodes_with_content:\n",
    "        tx, ty = text_node['x'], text_node['y']\n",
    "        distance = math.sqrt((x - tx)**2 + (y - ty)**2)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_text_node = text_node\n",
    "    \n",
    "    # If a nearest text node is found, get its semantic embedding\n",
    "    if nearest_text_node and nearest_text_node.get('text'):\n",
    "        # Generate semantic embedding for the text\n",
    "        semantic_vector = semantic_model.encode(nearest_text_node['text'])\n",
    "        \n",
    "        return {\n",
    "            \"nearest_text_semantic_vector\": semantic_vector.tolist(),  # Convert to list for JSON serialization\n",
    "            \"nearest_text_semantic_distance\": min_distance\n",
    "        }\n",
    "    \n",
    "    # Fallback if no meaningful text is found\n",
    "    return {\n",
    "        \"nearest_text_semantic_vector\": [0] * 384,  # Default zero vector\n",
    "        \"nearest_text_semantic_distance\": 9999999\n",
    "    }\n",
    "\n",
    "def color_difference(color1, color2):\n",
    "    \"\"\"\n",
    "    Calculate a perceptual color difference between two RGB colors using \n",
    "    a simplified version of the Delta E formula.\n",
    "    Returns a value between 0 and 1, where 0 means identical and 1 means completely different.\n",
    "    \"\"\"\n",
    "    if not all([color1, color2]):\n",
    "        return 0\n",
    "    \n",
    "    # Extract RGB values\n",
    "    r1, g1, b1 = color1\n",
    "    r2, g2, b2 = color2\n",
    "    \n",
    "    # Calculate Euclidean distance in RGB space (simplified)\n",
    "    distance = math.sqrt((r2-r1)**2 + (g2-g1)**2 + (b2-b1)**2)\n",
    "    \n",
    "    # Normalize to 0-1 range (max possible distance is sqrt(3 * 255^2))\n",
    "    max_distance = math.sqrt(3 * 255**2)\n",
    "    normalized_distance = distance / max_distance\n",
    "    \n",
    "    return normalized_distance\n",
    "\n",
    "def extract_features(node, depth=0, parent_tag=None, sibling_count=0, parent_tag_html=None, prev_sibling_tag=None,parent_height=0, parent_bg_color=None, text_nodes=None):\n",
    "    global body_width\n",
    "    # First pass: Collect text nodes if not provided\n",
    "    if text_nodes is None:\n",
    "        def collect_text_nodes(node):\n",
    "            text_nodes_list = []\n",
    "            # Function to check if a node has meaningful text\n",
    "            def has_meaningful_text(node_data):\n",
    "                return node_data.get('type','') == \"TEXT\"\n",
    "            \n",
    "            node_data = node.get(\"node\", {})\n",
    "            # If this node has meaningful text\n",
    "            if has_meaningful_text(node_data):\n",
    "                text_nodes_list.append({\n",
    "                    'x': node_data.get(\"x\", 0) + node_data.get(\"width\", 0) / 2,\n",
    "                    'y': node_data.get(\"y\", 0) + node_data.get(\"height\", 0) / 2,\n",
    "                    'text': node_data.get('characters', '').strip()\n",
    "                })\n",
    "            \n",
    "            # Recursively check children\n",
    "            for child in node.get(\"children\", []):\n",
    "                text_nodes_list.extend(collect_text_nodes(child))\n",
    "            \n",
    "            return text_nodes_list\n",
    "        \n",
    "        text_nodes = collect_text_nodes(node)\n",
    "    \n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    tag = node.get(\"tag\", \"\")\n",
    "    node_data = node.get(\"node\", {})\n",
    "    node_type = str(node_data.get(\"type\", \"\"))\n",
    "\n",
    "    text = node_data.get(\"characters\", \"\")\n",
    "    text_length = len(text)\n",
    "    word_count = len(text.split()) if text else 0\n",
    "    contains_number = any(ch.isdigit() for ch in text)\n",
    "    contains_special_chars = any(not ch.isalnum() and not ch.isspace() for ch in text)\n",
    "    \n",
    "    children = node.get(\"children\", [])\n",
    "    num_direct_children = len(children)\n",
    "    is_leaf = 1 if num_direct_children == 0 else 0\n",
    "    \n",
    "    # Initialize child tag features\n",
    "    child_1_tag = None\n",
    "    child_2_tag = None\n",
    "    child_3_tag = None\n",
    "    child_1_percent = 0\n",
    "    child_2_percent = 0\n",
    "    child_3_percent = 0\n",
    "    \n",
    "    # Calculate node area\n",
    "    node_width = node_data.get(\"width\", 0)\n",
    "    if not body_width or body_width == 0:\n",
    "        body_width = node_width\n",
    "    node_height = node_data.get(\"height\", 0)\n",
    "    node_area = node_width * node_height\n",
    "    \n",
    "    # Extract child information if available\n",
    "    if num_direct_children > 0:\n",
    "        # Child 1\n",
    "        if len(children) >= 1:\n",
    "            child_1_tag = children[0].get(\"tag\", \"\")\n",
    "            child_1_width = children[0].get(\"node\", {}).get(\"width\", 0)\n",
    "            child_1_height = children[0].get(\"node\", {}).get(\"height\", 0)\n",
    "            child_1_area = child_1_width * child_1_height\n",
    "            child_1_percent = (child_1_area / node_area) if node_area > 0 else 0\n",
    "        \n",
    "        # Child 2\n",
    "        if len(children) >= 2:\n",
    "            child_2_tag = children[1].get(\"tag\", \"\")\n",
    "            child_2_width = children[1].get(\"node\", {}).get(\"width\", 0)\n",
    "            child_2_height = children[1].get(\"node\", {}).get(\"height\", 0)\n",
    "            child_2_area = child_2_width * child_2_height\n",
    "            child_2_percent = (child_2_area / node_area) if node_area > 0 else 0\n",
    "        \n",
    "        # Child 3\n",
    "        if len(children) >= 3:\n",
    "            child_3_tag = children[2].get(\"tag\", \"\")\n",
    "            child_3_width = children[2].get(\"node\", {}).get(\"width\", 0)\n",
    "            child_3_height = children[2].get(\"node\", {}).get(\"height\", 0)\n",
    "            child_3_area = child_3_width * child_3_height\n",
    "            child_3_percent = (child_3_area / node_area) if node_area > 0 else 0\n",
    "    \n",
    "    # Count all children in the subtree (recursive count)\n",
    "    def count_all_descendants(node):\n",
    "        count = 0\n",
    "        for child in node.get(\"children\", []):\n",
    "            # Count this child\n",
    "            count += 1\n",
    "            # Add all its descendants\n",
    "            count += count_all_descendants(child)\n",
    "        return count\n",
    "    \n",
    "    # Count chars to the end\n",
    "    def count_chars_to_end(node):\n",
    "        count = 0\n",
    "        for child in node.get(\"children\", []):\n",
    "            # Count this child\n",
    "            node_data = child.get(\"node\", {})\n",
    "            count += len(node_data.get(\"characters\", \"\"))\n",
    "            # Add all its descendants\n",
    "            count += count_chars_to_end(child)\n",
    "        return count\n",
    "    \n",
    "    # get center of weight\n",
    "    def get_center_of_weight(node):\n",
    "        total_area = 0\n",
    "        total = 0\n",
    "        for child in node.get(\"children\", []):\n",
    "            node_data = child.get(\"node\", {})\n",
    "            x_center = node_data.get(\"x\",0) + node_data.get(\"width\",0) / 2\n",
    "            area = node_data.get(\"width\",0) * node_data.get(\"height\",0)\n",
    "            total += area * x_center\n",
    "            total_area += area\n",
    "        weighted_x = total/(total_area if total_area else 1)\n",
    "        diff = abs(node.get('x',0)-weighted_x) / (node.get('width',0) if node.get('width',0) else 1)\n",
    "        return diff if node.get('width',0) else 9999999\n",
    "    \n",
    "    # Calculate total descendants\n",
    "    num_children_to_end = count_all_descendants(node)\n",
    "    chars_count_to_end = count_chars_to_end(node)\n",
    "    bg_color = None\n",
    "    feature = {\n",
    "        \"tag\": tag,\n",
    "        \"type\": node_type,\n",
    "        \"x\": node_data.get(\"x\", 0),\n",
    "        \"y\": node_data.get(\"y\", 0),\n",
    "        \"width\": node_width/(body_width if body_width else 1),\n",
    "        \"height\": node_height/(parent_height if parent_height else node_height if node_height else 1),\n",
    "        \"characters\": text,\n",
    "        \"has_text\": int(bool(text)),\n",
    "        \"depth\": depth,\n",
    "        \"num_direct_children\": num_direct_children,\n",
    "        \"num_children_to_end\": num_children_to_end,  # Total descendants count\n",
    "        \"parent_tag\": parent_tag if parent_tag else \"\",\n",
    "        \"parent_tag_html\": parent_tag_html if parent_tag_html else \"\",\n",
    "        \"sibling_count\": sibling_count,\n",
    "        \"prev_sibling_html_tag\": prev_sibling_tag if prev_sibling_tag else \"\",\n",
    "        \"is_leaf\": is_leaf,\n",
    "        \"font_size\": node_data.get(\"fontSize\", 16),\n",
    "        \"has_font_size\": int(\"fontSize\" in node_data),\n",
    "        \"font_name\": node_data.get(\"fontName\", {}).get(\"style\", \"\") if node_data.get(\"fontName\") else \"normal\",\n",
    "        \"has_text_color\": 0, \"color_r\": 0, \"color_g\": 0, \"color_b\": 0,\n",
    "        \"has_background_color\": 0, \"background_r\": 0, \"background_g\": 0, \"background_b\": 0,\n",
    "        \"border_radius\": 0,\n",
    "        \"border_r\": 0, \"border_g\": 0, \"border_b\": 0,\n",
    "        \"has_border\": 0, \"border_opacity\": 0,\n",
    "        \"border_weight\": node_data.get(\"strokeWeight\", 0),\n",
    "        \"has_shadow\": 0, \"shadow_r\": 0, \"shadow_g\": 0, \"shadow_b\": 0,\n",
    "        \"shadow_radius\": 0, \n",
    "        \"text_length\": text_length,\n",
    "        \"chars_count_to_end\": chars_count_to_end,\n",
    "        \"word_count\": word_count,\n",
    "        \"contains_number\": int(contains_number),\n",
    "        \"contains_special_chars\": int(contains_special_chars),\n",
    "        \"aspect_ratio\": node_width / node_height if node_height > 0 else 0,\n",
    "        \"child_1_html_tag\": child_1_tag,\n",
    "        \"child_2_html_tag\": child_2_tag,\n",
    "        \"child_3_html_tag\": child_3_tag,\n",
    "        \"child_1_percentage_of_parent\": child_1_percent,\n",
    "        \"child_2_percentage_of_parent\": child_2_percent,\n",
    "        \"child_3_percentage_of_parent\": child_3_percent,\n",
    "        \"distinct_background\": 0,\n",
    "        \"center_of_weight_diff\": get_center_of_weight(node),\n",
    "    }\n",
    "    \n",
    "    # Extract fills (background and text color)\n",
    "    fills = node_data.get(\"fills\", [])\n",
    "    for fill in fills:\n",
    "        if fill.get(\"type\") == \"SOLID\" and \"color\" in fill:\n",
    "            r, g, b = (\n",
    "                int(fill[\"color\"].get(\"r\", 0) * 255),\n",
    "                int(fill[\"color\"].get(\"g\", 0) * 255),\n",
    "                int(fill[\"color\"].get(\"b\", 0) * 255),\n",
    "            )\n",
    "            feature[\"color_r\"], feature[\"color_g\"], feature[\"color_b\"] = r, g, b\n",
    "            feature[\"has_text_color\"] = 1  # Flag indicating explicit text color is set\n",
    "            \n",
    "            feature[\"background_r\"], feature[\"background_g\"], feature[\"background_b\"] = r, g, b\n",
    "            feature[\"has_background_color\"] = 1  # Flag for explicit background color\n",
    "            a = min(float(fill[\"color\"].get(\"a\", 1)),float(fill.get(\"opacity\",1)))\n",
    "            \n",
    "            bg_color = (r*a, g*a, b*a)\n",
    "            check = \"NO\"\n",
    "            if parent_bg_color:\n",
    "                bg_difference = color_difference(bg_color, parent_bg_color)               \n",
    "                # If difference is significant (threshold of 0.3 - adjust as needed)\n",
    "                if bg_difference > 0.2:\n",
    "                    feature[\"distinct_background\"] = 1    \n",
    "            break\n",
    "    \n",
    "    # Also check backgrounds for background color\n",
    "    backgrounds = node_data.get(\"backgrounds\", [])\n",
    "    for bg in backgrounds:\n",
    "        if bg.get(\"type\") == \"SOLID\" and \"color\" in bg:\n",
    "            r, g, b = (\n",
    "                int(bg[\"color\"].get(\"r\", 0) * 255),\n",
    "                int(bg[\"color\"].get(\"g\", 0) * 255),\n",
    "                int(bg[\"color\"].get(\"b\", 0) * 255),\n",
    "            )\n",
    "            feature[\"background_r\"], feature[\"background_g\"], feature[\"background_b\"] = r, g, b\n",
    "            feature[\"has_background_color\"] = 1  # Flag for explicit background color\n",
    "            \n",
    "           \n",
    "                \n",
    "            break\n",
    "    \n",
    "    # Extract strokes (borders)\n",
    "    strokes = node_data.get(\"strokes\", [])\n",
    "    if strokes:\n",
    "        stroke = strokes[0]\n",
    "        feature[\"has_border\"] = 1\n",
    "        if \"color\" in stroke:\n",
    "            feature[\"border_r\"], feature[\"border_g\"], feature[\"border_b\"] = (\n",
    "                int(stroke[\"color\"].get(\"r\", 0) * 255),\n",
    "                int(stroke[\"color\"].get(\"g\", 0) * 255),\n",
    "                int(stroke[\"color\"].get(\"b\", 0) * 255),\n",
    "            )\n",
    "        feature[\"border_opacity\"] = stroke.get(\"opacity\", 0)\n",
    "    \n",
    "    # Extract border radius\n",
    "    br_top_left = node_data.get(\"topLeftRadius\", 0)\n",
    "    br_top_right = node_data.get(\"topRightRadius\", 0)\n",
    "    br_bottom_left = node_data.get(\"bottomLeftRadius\", 0)\n",
    "    br_bottom_right = node_data.get(\"bottomRightRadius\", 0)\n",
    "    \n",
    "    if any([br_top_left, br_top_right, br_bottom_left, br_bottom_right]):\n",
    "        feature[\"border_radius\"] = (br_top_left + br_top_right + br_bottom_left + br_bottom_right) / 4\n",
    "        if feature[\"border_radius\"] >= 50:\n",
    "            feature[\"border_radius\"] = 0\n",
    "    \n",
    "    # Extract shadow\n",
    "    effects = node_data.get(\"effects\", [])\n",
    "    for effect in effects:\n",
    "        if effect.get(\"type\") == \"DROP_SHADOW\":\n",
    "            feature[\"has_shadow\"] = 1\n",
    "            if \"color\" in effect:\n",
    "                feature[\"shadow_r\"], feature[\"shadow_g\"], feature[\"shadow_b\"] = (\n",
    "                    int(effect[\"color\"].get(\"r\", 0) * 255),\n",
    "                    int(effect[\"color\"].get(\"g\", 0) * 255),\n",
    "                    int(effect[\"color\"].get(\"b\", 0) * 255),\n",
    "                )\n",
    "            feature[\"shadow_radius\"] = effect.get(\"radius\", 0)\n",
    "            break  \n",
    "    \n",
    "    # Get semantic features for the current node\n",
    "    semantic_features = extract_semantic_features(node, text_nodes)\n",
    "    \n",
    "    # Calculate nearest text node distance\n",
    "    nearest_text_distance = semantic_features.get('nearest_text_semantic_distance',0)\n",
    "    nearest_text_semantic = semantic_features.get('nearest_text_semantic_vector',[0]*384)\n",
    "    \n",
    "    # Add nearest text node distance to the feature dictionary\n",
    "    feature[\"nearest_text_node_dist\"] = (nearest_text_distance) / (math.sqrt((node_width)* (node_height)) if math.sqrt((node_width)*(node_height)) else 1)\n",
    "    feature[\"nearest_text_semantic\"] = nearest_text_semantic\n",
    "    features.append(feature)\n",
    "    \n",
    "    # Process children with previous sibling information\n",
    "    prev_sib_tag = None\n",
    "    for child in children:\n",
    "        features.extend(extract_features(\n",
    "            child, \n",
    "            depth=depth+1, \n",
    "            parent_tag=node_type, \n",
    "            sibling_count=len(children)-1, \n",
    "            parent_tag_html=tag,\n",
    "            prev_sibling_tag=prev_sib_tag,\n",
    "            parent_height= node_height,\n",
    "            parent_bg_color=bg_color if feature[\"has_background_color\"] else parent_bg_color,\n",
    "            text_nodes=text_nodes\n",
    "        ))\n",
    "        prev_sib_tag = child.get(\"tag\", \"\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../json_data\\figmaTree_1.json...\n",
      "Processing ../json_data\\figmaTree_10.json...\n",
      "Processing ../json_data\\figmaTree_100.json...\n",
      "Processing ../json_data\\figmaTree_10000.json...\n",
      "Processing ../json_data\\figmaTree_101.json...\n",
      "Processing ../json_data\\figmaTree_102.json...\n",
      "Processing ../json_data\\figmaTree_103.json...\n",
      "Processing ../json_data\\figmaTree_104.json...\n",
      "Processing ../json_data\\figmaTree_105.json...\n",
      "Processing ../json_data\\figmaTree_106.json...\n",
      "Processing ../json_data\\figmaTree_107.json...\n",
      "Processing ../json_data\\figmaTree_108.json...\n",
      "Processing ../json_data\\figmaTree_109.json...\n",
      "Processing ../json_data\\figmaTree_11.json...\n",
      "Processing ../json_data\\figmaTree_110.json...\n",
      "Processing ../json_data\\figmaTree_111.json...\n",
      "Processing ../json_data\\figmaTree_112.json...\n",
      "Processing ../json_data\\figmaTree_113.json...\n",
      "Processing ../json_data\\figmaTree_114.json...\n",
      "Processing ../json_data\\figmaTree_115.json...\n",
      "Processing ../json_data\\figmaTree_116.json...\n",
      "Processing ../json_data\\figmaTree_117.json...\n",
      "Processing ../json_data\\figmaTree_118.json...\n",
      "Processing ../json_data\\figmaTree_119.json...\n",
      "Processing ../json_data\\figmaTree_12.json...\n",
      "Processing ../json_data\\figmaTree_120.json...\n",
      "Processing ../json_data\\figmaTree_121.json...\n",
      "Processing ../json_data\\figmaTree_122.json...\n",
      "Processing ../json_data\\figmaTree_123.json...\n",
      "Processing ../json_data\\figmaTree_124.json...\n",
      "Processing ../json_data\\figmaTree_125.json...\n",
      "Processing ../json_data\\figmaTree_126.json...\n",
      "Processing ../json_data\\figmaTree_127.json...\n",
      "Processing ../json_data\\figmaTree_128.json...\n",
      "Processing ../json_data\\figmaTree_129.json...\n",
      "Processing ../json_data\\figmaTree_13.json...\n",
      "Processing ../json_data\\figmaTree_130.json...\n",
      "Processing ../json_data\\figmaTree_131.json...\n",
      "Processing ../json_data\\figmaTree_132.json...\n",
      "Processing ../json_data\\figmaTree_133.json...\n",
      "Processing ../json_data\\figmaTree_134.json...\n",
      "Processing ../json_data\\figmaTree_135.json...\n",
      "Processing ../json_data\\figmaTree_136.json...\n",
      "Processing ../json_data\\figmaTree_137.json...\n",
      "Processing ../json_data\\figmaTree_138.json...\n",
      "Processing ../json_data\\figmaTree_139.json...\n",
      "Processing ../json_data\\figmaTree_14.json...\n",
      "Processing ../json_data\\figmaTree_140.json...\n",
      "Processing ../json_data\\figmaTree_141.json...\n",
      "Processing ../json_data\\figmaTree_142.json...\n",
      "Processing ../json_data\\figmaTree_143.json...\n",
      "Processing ../json_data\\figmaTree_144.json...\n",
      "Processing ../json_data\\figmaTree_145.json...\n",
      "Processing ../json_data\\figmaTree_146.json...\n",
      "Processing ../json_data\\figmaTree_147.json...\n",
      "Processing ../json_data\\figmaTree_148.json...\n",
      "Processing ../json_data\\figmaTree_149.json...\n",
      "Processing ../json_data\\figmaTree_15.json...\n",
      "Processing ../json_data\\figmaTree_150.json...\n",
      "Processing ../json_data\\figmaTree_151.json...\n",
      "Processing ../json_data\\figmaTree_152.json...\n",
      "Processing ../json_data\\figmaTree_153.json...\n",
      "Processing ../json_data\\figmaTree_154.json...\n",
      "Processing ../json_data\\figmaTree_155.json...\n",
      "Processing ../json_data\\figmaTree_156.json...\n",
      "Processing ../json_data\\figmaTree_157.json...\n",
      "Processing ../json_data\\figmaTree_158.json...\n",
      "Processing ../json_data\\figmaTree_159.json...\n",
      "Processing ../json_data\\figmaTree_16.json...\n",
      "Processing ../json_data\\figmaTree_160.json...\n",
      "Processing ../json_data\\figmaTree_161.json...\n",
      "Processing ../json_data\\figmaTree_162.json...\n",
      "Processing ../json_data\\figmaTree_163.json...\n",
      "Processing ../json_data\\figmaTree_164.json...\n",
      "Processing ../json_data\\figmaTree_165.json...\n",
      "Processing ../json_data\\figmaTree_166.json...\n",
      "Processing ../json_data\\figmaTree_167.json...\n",
      "Processing ../json_data\\figmaTree_168.json...\n",
      "Processing ../json_data\\figmaTree_169.json...\n",
      "Processing ../json_data\\figmaTree_170.json...\n",
      "Processing ../json_data\\figmaTree_171.json...\n",
      "Processing ../json_data\\figmaTree_172.json...\n",
      "Processing ../json_data\\figmaTree_173.json...\n",
      "Processing ../json_data\\figmaTree_174.json...\n",
      "Processing ../json_data\\figmaTree_175.json...\n",
      "Processing ../json_data\\figmaTree_176.json...\n",
      "Processing ../json_data\\figmaTree_177.json...\n",
      "Processing ../json_data\\figmaTree_178.json...\n",
      "Processing ../json_data\\figmaTree_179.json...\n",
      "Processing ../json_data\\figmaTree_180.json...\n",
      "Processing ../json_data\\figmaTree_181.json...\n",
      "Processing ../json_data\\figmaTree_182.json...\n",
      "Processing ../json_data\\figmaTree_183.json...\n",
      "Processing ../json_data\\figmaTree_184.json...\n",
      "Processing ../json_data\\figmaTree_185.json...\n",
      "Processing ../json_data\\figmaTree_186.json...\n",
      "Processing ../json_data\\figmaTree_187.json...\n",
      "Processing ../json_data\\figmaTree_188.json...\n",
      "Processing ../json_data\\figmaTree_189.json...\n",
      "Processing ../json_data\\figmaTree_19.json...\n",
      "Processing ../json_data\\figmaTree_190.json...\n",
      "Processing ../json_data\\figmaTree_191.json...\n",
      "Processing ../json_data\\figmaTree_192.json...\n",
      "Processing ../json_data\\figmaTree_193.json...\n",
      "Processing ../json_data\\figmaTree_194.json...\n",
      "Processing ../json_data\\figmaTree_195.json...\n",
      "Processing ../json_data\\figmaTree_196.json...\n",
      "Processing ../json_data\\figmaTree_197.json...\n",
      "Processing ../json_data\\figmaTree_198.json...\n",
      "Processing ../json_data\\figmaTree_199.json...\n",
      "Processing ../json_data\\figmaTree_2.json...\n",
      "Processing ../json_data\\figmaTree_20.json...\n",
      "Processing ../json_data\\figmaTree_200.json...\n",
      "Processing ../json_data\\figmaTree_201.json...\n",
      "Processing ../json_data\\figmaTree_202.json...\n",
      "Processing ../json_data\\figmaTree_203.json...\n",
      "Processing ../json_data\\figmaTree_204.json...\n",
      "Processing ../json_data\\figmaTree_205.json...\n",
      "Processing ../json_data\\figmaTree_206.json...\n",
      "Processing ../json_data\\figmaTree_207.json...\n",
      "Processing ../json_data\\figmaTree_208.json...\n",
      "Processing ../json_data\\figmaTree_209.json...\n",
      "Processing ../json_data\\figmaTree_21.json...\n",
      "Processing ../json_data\\figmaTree_210.json...\n",
      "Processing ../json_data\\figmaTree_211.json...\n",
      "Processing ../json_data\\figmaTree_212.json...\n",
      "Processing ../json_data\\figmaTree_213.json...\n",
      "Processing ../json_data\\figmaTree_214.json...\n",
      "Processing ../json_data\\figmaTree_215.json...\n",
      "Processing ../json_data\\figmaTree_216.json...\n",
      "Processing ../json_data\\figmaTree_217.json...\n",
      "Processing ../json_data\\figmaTree_218.json...\n",
      "Processing ../json_data\\figmaTree_219.json...\n",
      "Processing ../json_data\\figmaTree_22.json...\n",
      "Processing ../json_data\\figmaTree_220.json...\n",
      "Processing ../json_data\\figmaTree_221.json...\n",
      "Processing ../json_data\\figmaTree_222.json...\n",
      "Processing ../json_data\\figmaTree_223.json...\n",
      "Processing ../json_data\\figmaTree_224.json...\n",
      "Processing ../json_data\\figmaTree_225.json...\n",
      "Processing ../json_data\\figmaTree_226.json...\n",
      "Processing ../json_data\\figmaTree_227.json...\n",
      "Processing ../json_data\\figmaTree_228.json...\n",
      "Processing ../json_data\\figmaTree_229.json...\n",
      "Processing ../json_data\\figmaTree_23.json...\n",
      "Processing ../json_data\\figmaTree_230.json...\n",
      "Processing ../json_data\\figmaTree_231.json...\n",
      "Processing ../json_data\\figmaTree_232.json...\n",
      "Processing ../json_data\\figmaTree_233.json...\n",
      "Processing ../json_data\\figmaTree_234.json...\n",
      "Processing ../json_data\\figmaTree_235.json...\n",
      "Processing ../json_data\\figmaTree_236.json...\n",
      "Processing ../json_data\\figmaTree_237.json...\n",
      "Processing ../json_data\\figmaTree_238.json...\n",
      "Processing ../json_data\\figmaTree_239.json...\n",
      "Processing ../json_data\\figmaTree_24.json...\n",
      "Processing ../json_data\\figmaTree_240.json...\n",
      "Processing ../json_data\\figmaTree_241.json...\n",
      "Processing ../json_data\\figmaTree_242.json...\n",
      "Processing ../json_data\\figmaTree_243.json...\n",
      "Processing ../json_data\\figmaTree_244.json...\n",
      "Processing ../json_data\\figmaTree_245.json...\n",
      "Processing ../json_data\\figmaTree_246.json...\n",
      "Processing ../json_data\\figmaTree_247.json...\n",
      "Processing ../json_data\\figmaTree_248.json...\n",
      "Processing ../json_data\\figmaTree_249.json...\n",
      "Processing ../json_data\\figmaTree_25.json...\n",
      "Processing ../json_data\\figmaTree_250.json...\n",
      "Processing ../json_data\\figmaTree_251.json...\n",
      "Processing ../json_data\\figmaTree_252.json...\n",
      "Processing ../json_data\\figmaTree_253.json...\n",
      "Processing ../json_data\\figmaTree_254.json...\n",
      "Processing ../json_data\\figmaTree_255.json...\n",
      "Processing ../json_data\\figmaTree_256.json...\n",
      "Processing ../json_data\\figmaTree_257.json...\n",
      "Processing ../json_data\\figmaTree_258.json...\n",
      "Processing ../json_data\\figmaTree_259.json...\n",
      "Processing ../json_data\\figmaTree_260.json...\n",
      "Processing ../json_data\\figmaTree_261.json...\n",
      "Processing ../json_data\\figmaTree_262.json...\n",
      "Processing ../json_data\\figmaTree_263.json...\n",
      "Processing ../json_data\\figmaTree_264.json...\n",
      "Processing ../json_data\\figmaTree_265.json...\n",
      "Processing ../json_data\\figmaTree_266.json...\n",
      "Processing ../json_data\\figmaTree_267.json...\n",
      "Processing ../json_data\\figmaTree_268.json...\n",
      "Processing ../json_data\\figmaTree_269.json...\n",
      "Processing ../json_data\\figmaTree_27.json...\n",
      "Processing ../json_data\\figmaTree_270.json...\n",
      "Processing ../json_data\\figmaTree_271.json...\n",
      "Processing ../json_data\\figmaTree_272.json...\n",
      "Processing ../json_data\\figmaTree_273.json...\n",
      "Processing ../json_data\\figmaTree_274.json...\n",
      "Processing ../json_data\\figmaTree_275.json...\n",
      "Processing ../json_data\\figmaTree_276.json...\n",
      "Processing ../json_data\\figmaTree_277.json...\n",
      "Processing ../json_data\\figmaTree_278.json...\n",
      "Processing ../json_data\\figmaTree_279.json...\n",
      "Processing ../json_data\\figmaTree_28.json...\n",
      "Processing ../json_data\\figmaTree_280.json...\n",
      "Processing ../json_data\\figmaTree_281.json...\n",
      "Processing ../json_data\\figmaTree_282.json...\n",
      "Processing ../json_data\\figmaTree_284.json...\n",
      "Processing ../json_data\\figmaTree_285.json...\n",
      "Processing ../json_data\\figmaTree_286.json...\n",
      "Processing ../json_data\\figmaTree_287.json...\n",
      "Processing ../json_data\\figmaTree_288.json...\n",
      "Processing ../json_data\\figmaTree_289.json...\n",
      "Processing ../json_data\\figmaTree_29.json...\n",
      "Processing ../json_data\\figmaTree_290.json...\n",
      "Processing ../json_data\\figmaTree_291.json...\n",
      "Processing ../json_data\\figmaTree_292.json...\n",
      "Processing ../json_data\\figmaTree_293.json...\n",
      "Processing ../json_data\\figmaTree_294.json...\n",
      "Processing ../json_data\\figmaTree_295.json...\n",
      "Processing ../json_data\\figmaTree_296.json...\n",
      "Processing ../json_data\\figmaTree_297.json...\n",
      "Processing ../json_data\\figmaTree_298.json...\n",
      "Processing ../json_data\\figmaTree_299.json...\n",
      "Processing ../json_data\\figmaTree_3.json...\n",
      "Processing ../json_data\\figmaTree_30.json...\n",
      "Processing ../json_data\\figmaTree_300.json...\n",
      "Processing ../json_data\\figmaTree_301.json...\n",
      "Processing ../json_data\\figmaTree_302.json...\n",
      "Processing ../json_data\\figmaTree_303.json...\n",
      "Processing ../json_data\\figmaTree_304.json...\n",
      "Processing ../json_data\\figmaTree_305.json...\n",
      "Processing ../json_data\\figmaTree_306.json...\n",
      "Processing ../json_data\\figmaTree_307.json...\n",
      "Processing ../json_data\\figmaTree_308.json...\n",
      "Processing ../json_data\\figmaTree_309.json...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Extract features using the recursive function starting at the root\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m features_list \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msibling_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_tag_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m features_list:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if no features extracted\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 338\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(node, depth, parent_tag, sibling_count, parent_tag_html, prev_sibling_tag, parent_height, parent_bg_color, text_nodes)\u001b[0m\n\u001b[0;32m    336\u001b[0m prev_sib_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[1;32m--> 338\u001b[0m     features\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43msibling_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_tag_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_sibling_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_sib_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_color\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_background_color\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_nodes\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    349\u001b[0m     prev_sib_tag \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "Cell \u001b[1;32mIn[3], line 338\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(node, depth, parent_tag, sibling_count, parent_tag_html, prev_sibling_tag, parent_height, parent_bg_color, text_nodes)\u001b[0m\n\u001b[0;32m    336\u001b[0m prev_sib_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[1;32m--> 338\u001b[0m     features\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43msibling_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_tag_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_sibling_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_sib_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_color\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_background_color\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_nodes\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    349\u001b[0m     prev_sib_tag \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "    \u001b[1;31m[... skipping similar frames: extract_features at line 338 (16 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 338\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(node, depth, parent_tag, sibling_count, parent_tag_html, prev_sibling_tag, parent_height, parent_bg_color, text_nodes)\u001b[0m\n\u001b[0;32m    336\u001b[0m prev_sib_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[1;32m--> 338\u001b[0m     features\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43msibling_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_tag_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_sibling_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_sib_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_color\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_background_color\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparent_bg_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_nodes\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    349\u001b[0m     prev_sib_tag \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "Cell \u001b[1;32mIn[3], line 323\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(node, depth, parent_tag, sibling_count, parent_tag_html, prev_sibling_tag, parent_height, parent_bg_color, text_nodes)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Get semantic features for the current node\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m semantic_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_semantic_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Calculate nearest text node distance\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# TODO: complete after prayer\u001b[39;00m\n\u001b[0;32m    327\u001b[0m nearest_text_distance \u001b[38;5;241m=\u001b[39m semantic_features\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest_text_semantic_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mextract_semantic_features\u001b[1;34m(node, text_nodes_with_content)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# If a nearest text node is found, get its semantic embedding\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nearest_text_node \u001b[38;5;129;01mand\u001b[39;00m nearest_text_node\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Generate semantic embedding for the text\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     semantic_vector \u001b[38;5;241m=\u001b[39m \u001b[43msemantic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnearest_text_node\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest_text_semantic_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m: semantic_vector\u001b[38;5;241m.\u001b[39mtolist(),  \u001b[38;5;66;03m# Convert to list for JSON serialization\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest_text_semantic_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m: min_distance\n\u001b[0;32m     45\u001b[0m     }\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Fallback if no meaningful text is found\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    437\u001b[0m     key: value\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    440\u001b[0m }\n\u001b[1;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    406\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Folder containing JSON files\n",
    "data_folder = \"../json_data\"\n",
    "output_csv_file = \"figma_dataset.csv\"\n",
    "\n",
    "normalize_columns = [\n",
    "    # \"area\",\n",
    "    # \"word_count\",\n",
    "    # \"text_length\",\n",
    "    # \"font_size\",\n",
    "    # \"sibling_count\",\n",
    "    # \"num_children\",\n",
    "    # \"height\",\n",
    "    # \"width\",\n",
    "    # \"depth\",\n",
    "    # \"nearest_text_node_dist\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# If the output CSV exists, remove it so we start fresh\n",
    "if os.path.exists(output_csv_file):\n",
    "    os.remove(output_csv_file)\n",
    "\n",
    "# Flag to write header only for the first batch\n",
    "first_batch = True\n",
    "\n",
    "# Iterate over all JSON files in the data folder\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract features using the recursive function starting at the root\n",
    "        features_list = extract_features(data, depth=0, parent_tag=None, sibling_count=0, parent_tag_html=None, parent_height= 0, parent_bg_color=None)\n",
    "        if not features_list:\n",
    "            continue  # Skip if no features extracted\n",
    "        \n",
    "        df = pd.DataFrame(features_list)\n",
    "\n",
    "        df = df.drop(columns=['x'])\n",
    "        df = df.drop(columns=['y'])\n",
    "        df = df.drop(columns=['characters'])\n",
    "        df = df.drop(columns=['font_size'])\n",
    "        df = df.drop(columns=['font_name'])\n",
    "        df = df.drop(columns=['color_r'])\n",
    "        df = df.drop(columns=['color_g'])\n",
    "        df = df.drop(columns=['color_b'])\n",
    "        df = df.drop(columns=['background_r'])\n",
    "        df = df.drop(columns=['background_g'])\n",
    "        df = df.drop(columns=['background_b'])\n",
    "        # df = df.drop(columns=['border_radius'])\n",
    "        df = df.drop(columns=['border_r'])\n",
    "        df = df.drop(columns=['border_g'])\n",
    "        df = df.drop(columns=['border_b'])\n",
    "        df = df.drop(columns=['border_opacity'])\n",
    "        df = df.drop(columns=['border_weight'])\n",
    "        df = df.drop(columns=['shadow_r'])\n",
    "        df = df.drop(columns=['shadow_g'])\n",
    "        df = df.drop(columns=['shadow_b'])\n",
    "        df = df.drop(columns=['shadow_radius'])\n",
    "        df = df.drop(columns=['word_count'])\n",
    "        df = df.drop(columns=['contains_special_chars'])\n",
    "        df = df.drop(columns=['contains_number'])\n",
    "        df = df.drop(columns=['has_shadow'])\n",
    "        df = df.drop(columns=['has_border'])\n",
    "        df = df.drop(columns=['has_text_color'])\n",
    "        # df = df.drop(columns=['height'])\n",
    "        df = df.drop(columns=['has_text'])\n",
    "        df = df.drop(columns=['depth'])\n",
    "        df = df.drop(columns=['has_font_size'])\n",
    "        df = df.drop(columns=['parent_tag'])\n",
    "        df = df.drop(columns=['parent_tag_html'])\n",
    "        df = df.drop(columns=['is_leaf'])\n",
    "        df = df.drop(columns=['center_of_weight_diff'])\n",
    "        df = df.drop(columns=['child_3_html_tag'])\n",
    "        df = df.drop(columns=['child_3_percentage_of_parent'])\n",
    "        df = df.drop(columns=['num_direct_children'])\n",
    "\n",
    "\n",
    "        df.to_csv(output_csv_file, mode='a', header=first_batch, index=False)\n",
    "        first_batch = False\n",
    "\n",
    "print(f\"Extracted features from all JSON files have been saved to {output_csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
