{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(node, depth=0, parent_tag=None, sibling_count=0):\n",
    "    features = []\n",
    "    \n",
    "    tag = node.get(\"tag\", \"\")\n",
    "    node_data = node.get(\"node\", {})\n",
    "    node_type = str(node_data.get(\"type\", \"\"))\n",
    "\n",
    "    text = node_data.get(\"characters\", \"\")\n",
    "    text_length = len(text)\n",
    "    word_count = len(text.split()) if text else 0\n",
    "    contains_number = any(ch.isdigit() for ch in text)\n",
    "    contains_special_chars = any(not ch.isalnum() and not ch.isspace() for ch in text)\n",
    "    \n",
    "    children = node.get(\"children\", [])\n",
    "    num_children = len(children)\n",
    "    is_leaf = 1 if num_children == 0 else 0\n",
    "    \n",
    "    feature = {\n",
    "        \"tag\": tag,\n",
    "        \"type\": node_type,\n",
    "        \"x\": node_data.get(\"x\", 0),\n",
    "        \"y\": node_data.get(\"y\", 0),\n",
    "        \"width\": node_data.get(\"width\", 0),\n",
    "        \"height\": node_data.get(\"height\", 0),\n",
    "        \"characters\": text,\n",
    "        \"has_text\": int(bool(text)),\n",
    "        \"depth\": depth,\n",
    "        \"num_children\": num_children,\n",
    "        \"parent_tag\": parent_tag if parent_tag else \"\",\n",
    "        \"sibling_count\": sibling_count,\n",
    "        \"is_leaf\": is_leaf,\n",
    "        \"font_size\": node_data.get(\"fontSize\", 16),\n",
    "        \"has_font_size\": int(\"fontSize\" in node_data),\n",
    "        \"font_name\": node_data.get(\"fontName\", {}).get(\"style\", \"\") if node_data.get(\"fontName\") else \"normal\",\n",
    "        \"has_text_color\": 0, \"color_r\": 0, \"color_g\": 0, \"color_b\": 0,\n",
    "        \"has_background_color\": 0, \"background_r\": 0, \"background_g\": 0, \"background_b\": 0,\n",
    "        \"border_radius\": 0,\n",
    "        \"border_r\": 0, \"border_g\": 0, \"border_b\": 0,\n",
    "        \"border_weight\": node_data.get(\"strokeWeight\", 0),\n",
    "        \"has_shadow\": 0, \"shadow_r\": 0, \"shadow_g\": 0, \"shadow_b\": 0,\n",
    "        \"shadow_radius\": 0, \n",
    "        \"text_length\": text_length,\n",
    "        \"word_count\": word_count,\n",
    "        \"contains_number\": int(contains_number),\n",
    "        \"contains_special_chars\": int(contains_special_chars),\n",
    "    }\n",
    "    \n",
    "    # Extract fills (background and text color)\n",
    "    fills = node_data.get(\"fills\", [])\n",
    "    for fill in fills:\n",
    "        if fill.get(\"type\") == \"SOLID\" and \"color\" in fill:\n",
    "            r, g, b = (\n",
    "                int(fill[\"color\"].get(\"r\", 0) * 255),\n",
    "                int(fill[\"color\"].get(\"g\", 0) * 255),\n",
    "                int(fill[\"color\"].get(\"b\", 0) * 255),\n",
    "            )\n",
    "            feature[\"color_r\"], feature[\"color_g\"], feature[\"color_b\"] = r, g, b\n",
    "            feature[\"has_text_color\"] = 1  # Flag indicating explicit text color is set\n",
    "            \n",
    "            feature[\"background_r\"], feature[\"background_g\"], feature[\"background_b\"] = r, g, b\n",
    "            feature[\"has_background_color\"] = 1  # Flag for explicit background color\n",
    "            break  \n",
    "    \n",
    "    # Extract strokes (borders)\n",
    "    strokes = node_data.get(\"strokes\", [])\n",
    "    if strokes:\n",
    "        stroke = strokes[0]\n",
    "        feature[\"has_border\"] = 1\n",
    "        if \"color\" in stroke:\n",
    "            feature[\"border_r\"], feature[\"border_g\"], feature[\"border_b\"] = (\n",
    "                int(stroke[\"color\"].get(\"r\", 0) * 255),\n",
    "                int(stroke[\"color\"].get(\"g\", 0) * 255),\n",
    "                int(stroke[\"color\"].get(\"b\", 0) * 255),\n",
    "            )\n",
    "        feature[\"border_opacity\"] = stroke.get(\"opacity\", 0)\n",
    "    \n",
    "    # Extract border radius\n",
    "    br_top_left = node_data.get(\"topLeftRadius\", 0)\n",
    "    br_top_right = node_data.get(\"topRightRadius\", 0)\n",
    "    br_bottom_left = node_data.get(\"bottomLeftRadius\", 0)\n",
    "    br_bottom_right = node_data.get(\"bottomRightRadius\", 0)\n",
    "    \n",
    "    if any([br_top_left, br_top_right, br_bottom_left, br_bottom_right]):\n",
    "        feature[\"border_radius\"] = (br_top_left + br_top_right + br_bottom_left + br_bottom_right) / 4\n",
    "    \n",
    "    # Extract shadow\n",
    "    effects = node_data.get(\"effects\", [])\n",
    "    for effect in effects:\n",
    "        if effect.get(\"type\") == \"DROP_SHADOW\":\n",
    "            feature[\"has_shadow\"] = 1\n",
    "            if \"color\" in effect:\n",
    "                feature[\"shadow_r\"], feature[\"shadow_g\"], feature[\"shadow_b\"] = (\n",
    "                    int(effect[\"color\"].get(\"r\", 0) * 255),\n",
    "                    int(effect[\"color\"].get(\"g\", 0) * 255),\n",
    "                    int(effect[\"color\"].get(\"b\", 0) * 255),\n",
    "                )\n",
    "            feature[\"shadow_radius\"] = effect.get(\"radius\", 0)\n",
    "            break  \n",
    "    \n",
    "    features.append(feature)\n",
    "    \n",
    "    for child in children:\n",
    "        features.extend(extract_features(child, depth=depth+1, parent_tag=node_type, sibling_count=len(children)-1))\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing json_data\\figmaTree_1.json...\n",
      "Processing json_data\\figmaTree_10.json...\n",
      "Processing json_data\\figmaTree_100.json...\n",
      "Processing json_data\\figmaTree_101.json...\n",
      "Processing json_data\\figmaTree_102.json...\n",
      "Processing json_data\\figmaTree_11.json...\n",
      "Processing json_data\\figmaTree_12.json...\n",
      "Processing json_data\\figmaTree_13.json...\n",
      "Processing json_data\\figmaTree_14.json...\n",
      "Processing json_data\\figmaTree_16.json...\n",
      "Processing json_data\\figmaTree_18.json...\n",
      "Processing json_data\\figmaTree_19.json...\n",
      "Processing json_data\\figmaTree_2.json...\n",
      "Processing json_data\\figmaTree_20.json...\n",
      "Processing json_data\\figmaTree_21.json...\n",
      "Processing json_data\\figmaTree_22.json...\n",
      "Processing json_data\\figmaTree_23.json...\n",
      "Processing json_data\\figmaTree_24.json...\n",
      "Processing json_data\\figmaTree_25.json...\n",
      "Processing json_data\\figmaTree_26.json...\n",
      "Processing json_data\\figmaTree_27.json...\n",
      "Processing json_data\\figmaTree_28.json...\n",
      "Processing json_data\\figmaTree_29.json...\n",
      "Processing json_data\\figmaTree_3.json...\n",
      "Processing json_data\\figmaTree_30.json...\n",
      "Processing json_data\\figmaTree_31.json...\n",
      "Processing json_data\\figmaTree_32.json...\n",
      "Processing json_data\\figmaTree_33.json...\n",
      "Processing json_data\\figmaTree_34.json...\n",
      "Processing json_data\\figmaTree_35.json...\n",
      "Processing json_data\\figmaTree_36.json...\n",
      "Processing json_data\\figmaTree_37.json...\n",
      "Processing json_data\\figmaTree_38.json...\n",
      "Processing json_data\\figmaTree_39.json...\n",
      "Processing json_data\\figmaTree_4.json...\n",
      "Processing json_data\\figmaTree_40.json...\n",
      "Processing json_data\\figmaTree_41.json...\n",
      "Processing json_data\\figmaTree_42.json...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 32\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Extract features using the recursive function starting at the root\u001b[39;00m\n\u001b[0;32m     35\u001b[0m features_list \u001b[38;5;241m=\u001b[39m extract_features(data, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, parent_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sibling_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Folder containing JSON files\n",
    "data_folder = \"json_data\"\n",
    "output_csv_file = \"figma_dataset.csv\"\n",
    "\n",
    "normalize_columns = [\n",
    "    \"area\",\n",
    "    \"word_count\",\n",
    "    \"text_length\",\n",
    "    \"font_size\",\n",
    "    \"sibling_count\",\n",
    "    \"num_children\",\n",
    "    \"height\",\n",
    "    \"width\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# If the output CSV exists, remove it so we start fresh\n",
    "if os.path.exists(output_csv_file):\n",
    "    os.remove(output_csv_file)\n",
    "\n",
    "# Flag to write header only for the first batch\n",
    "first_batch = True\n",
    "\n",
    "# Iterate over all JSON files in the data folder\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract features using the recursive function starting at the root\n",
    "        features_list = extract_features(data, depth=0, parent_tag=None, sibling_count=0)\n",
    "        if not features_list:\n",
    "            continue  # Skip if no features extracted\n",
    "        \n",
    "        df = pd.DataFrame(features_list)\n",
    "        \n",
    "        # Normalize positions per JSON file to avoid cross-file influence\n",
    "        min_x = df['x'].min() if df['x'].notnull().any() else 0\n",
    "        min_y = df['y'].min() if df['y'].notnull().any() else 0\n",
    "        df['x_normalized'] = df['x'] - min_x\n",
    "        df['y_normalized'] = df['y'] - min_y\n",
    "        \n",
    "        df['x_center'] = df['x'] + df['width'] / 2\n",
    "        df['y_center'] = df['y'] + df['height'] / 2\n",
    "        \n",
    "        # Attempt to compute total dimensions using a BODY tag if available\n",
    "        body_node = df[df['tag'] == 'BODY']\n",
    "        if not body_node.empty:\n",
    "            total_width = body_node.iloc[0]['width']\n",
    "            total_height = body_node.iloc[0]['height']\n",
    "        else:\n",
    "            total_width = (df['x'] + df['width']).max()\n",
    "            total_height = (df['y'] + df['height']).max()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if total_width and total_height:\n",
    "            df['x_quarter'] = df['x_center'] / total_width\n",
    "            df['y_quarter'] = df['y_center'] / total_height\n",
    "        else:\n",
    "            df['x_quarter'] = None\n",
    "            df['y_quarter'] = None\n",
    "        \n",
    "        df['aspect_ratio'] = df.apply(\n",
    "            lambda row: row['width'] / row['height'] if row['height'] and row['height'] != 0 else None, axis=1\n",
    "        )\n",
    "        df['area'] = df['width'] * df['height']\n",
    "        if total_width:\n",
    "            df['normalized_width'] = df['width'] / total_width\n",
    "        else:\n",
    "            df['normalized_width'] = None\n",
    "        if total_height:\n",
    "            df['normalized_height'] = df['height'] / total_height\n",
    "        else:\n",
    "            df['normalized_height'] = None\n",
    "\n",
    "\n",
    "        df = df.drop(columns=['x'])\n",
    "        df = df.drop(columns=['y'])\n",
    "        df = df.drop(columns=['x_normalized'])\n",
    "        df = df.drop(columns=['y_normalized'])\n",
    "        df = df.drop(columns=['x_center'])\n",
    "        df = df.drop(columns=['y_center'])\n",
    "\n",
    "\n",
    "        # Append this batch to the CSV file\n",
    "\n",
    "\n",
    "        # Compute min and max for each column\n",
    "        min_max_values = {col: (df[col].min(), df[col].max()) for col in normalize_columns}\n",
    "\n",
    "        # Apply Min-Max normalization (scaling between 0 and 1)\n",
    "        for col in normalize_columns:\n",
    "            min_val, max_val = min_max_values[col]\n",
    "            if max_val > min_val:  # Avoid division by zero\n",
    "                df[col] = (df[col] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                df[col] = 0  # If min and max are the same, set to 0\n",
    "\n",
    "        \n",
    "        df.to_csv(output_csv_file, mode='a', header=first_batch, index=False)\n",
    "        first_batch = False\n",
    "\n",
    "print(f\"Extracted features from all JSON files have been saved to {output_csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
