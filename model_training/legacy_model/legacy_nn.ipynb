{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib  # For saving encoders and scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset and remove rows with '-' in the tag column\n",
    "df = pd.read_csv(\"../../feature_extraction/figma_dataset.csv\")\n",
    "df = df[~df['tag'].str.contains('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate features and target\n",
    "y = df[\"tag\"]\n",
    "X = df.drop(columns=[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identify categorical and continuous columns\n",
    "categorical_cols = ['type','parent_tag','parent_tag_html']\n",
    "continuous_cols = [col for col in X.columns if col not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process categorical features with LabelEncoder\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    # If you need to save individual encoders, consider saving them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values in continuous columns and scale them\n",
    "X[continuous_cols] = X[continuous_cols].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X[continuous_cols])\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace continuous columns in X with their scaled values\n",
    "X_scaled = X.copy()\n",
    "X_scaled[continuous_cols] = X_continuous_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "# Note: X_train and X_test are DataFrames, so use .values to convert to NumPy arrays.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define the Neural Network Model with non-linear activations between linear layers\n",
    "class TagClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TagClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 128)           # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 256)            # Third hidden layer\n",
    "        self.fc4 = nn.Linear(256, 512)            # Fourth hidden layer\n",
    "        self.fc5 = nn.Linear(512, 512)            # Fifth hidden layer\n",
    "        self.fc6 = nn.Linear(512, 512)            # Sixth hidden layer\n",
    "        self.fc7 = nn.Linear(512, 256)            # Seventh hidden layer\n",
    "        self.fc8 = nn.Linear(256, 128)           # Eighth hidden layer\n",
    "        self.fc9 = nn.Linear(128, output_size)   # Output layer\n",
    "        self.relu = nn.ReLU()                  # Non-linear activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.relu(self.fc8(x))\n",
    "        logits = self.fc9(x)  # No activation here: CrossEntropyLoss expects raw logits.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = len(label_encoder.classes_)\n",
    "model = TagClassifier(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Internally applies softmax on logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [10/100], Loss: 4.3198\n",
      "Epoch [20/100], Loss: 4.1372\n",
      "Epoch [30/100], Loss: 3.6302\n",
      "Epoch [40/100], Loss: 3.3287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_train_tensor)\n\u001b[0;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_tensor)\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 9. Training loop\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"tag_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8356\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      1.00      0.99      9870\n",
      "     ADDRESS       0.00      0.00      0.00        12\n",
      "     ARTICLE       0.43      0.01      0.02       286\n",
      "       ASIDE       0.00      0.00      0.00        54\n",
      "           B       0.00      0.00      0.00        71\n",
      "        BODY       0.90      0.96      0.93        72\n",
      "      BUTTON       0.65      0.09      0.16       819\n",
      "      CANVAS       0.00      0.00      0.00         1\n",
      "        CITE       0.00      0.00      0.00         5\n",
      "         CNX       0.00      0.00      0.00         2\n",
      "          DD       0.00      0.00      0.00         3\n",
      "         DIV       0.83      0.98      0.90     29283\n",
      "          DL       0.00      0.00      0.00         1\n",
      "          DT       0.00      0.00      0.00         6\n",
      "          EM       0.00      0.00      0.00        25\n",
      "  FIGCAPTION       0.00      0.00      0.00       115\n",
      "      FIGURE       0.00      0.00      0.00       139\n",
      "      FOOTER       0.00      0.00      0.00       119\n",
      "        FORM       0.00      0.00      0.00        63\n",
      "       FRONT       1.00      1.00      1.00         1\n",
      "          H1       0.00      0.00      0.00        55\n",
      "          H2       0.52      0.31      0.39       868\n",
      "          H3       0.00      0.00      0.00       571\n",
      "          H4       0.00      0.00      0.00        65\n",
      "          H5       0.00      0.00      0.00        23\n",
      "          H6       0.00      0.00      0.00        16\n",
      "      HEADER       0.00      0.00      0.00       198\n",
      "          HR       0.99      0.90      0.94       164\n",
      "           I       0.00      0.00      0.00       282\n",
      "        ICON       0.00      0.00      0.00        12\n",
      "      IFRAME       0.00      0.00      0.00       221\n",
      "         IMG       0.66      0.94      0.78      1405\n",
      "       INPUT       1.00      0.53      0.69       186\n",
      "       LABEL       0.00      0.00      0.00       130\n",
      "          LI       0.98      0.97      0.97      5092\n",
      "        MAIN       0.00      0.00      0.00        25\n",
      "         NAV       0.00      0.00      0.00       242\n",
      "          OL       0.00      0.00      0.00         5\n",
      "           P       0.75      0.48      0.59      1645\n",
      "     PICTURE       0.86      0.55      0.67      1060\n",
      "           S       0.00      0.00      0.00         5\n",
      "     SECTION       0.00      0.00      0.00       597\n",
      "      SELECT       0.00      0.00      0.00         5\n",
      "       SMALL       0.00      0.00      0.00         4\n",
      "        SPAN       0.62      0.56      0.59      6601\n",
      "      STRONG       0.00      0.00      0.00       101\n",
      "         SUP       0.00      0.00      0.00         2\n",
      "         SVG       0.99      1.00      1.00      1648\n",
      "       TABLE       0.00      0.00      0.00         2\n",
      "       TBODY       0.00      0.00      0.00         7\n",
      "          TD       0.51      0.75      0.60       130\n",
      "          TH       0.00      0.00      0.00        27\n",
      "       THEAD       0.00      0.00      0.00         2\n",
      "        TIME       0.00      0.00      0.00       165\n",
      "          TR       0.86      0.74      0.79        42\n",
      "         TXT       0.82      0.85      0.84      8051\n",
      "           U       0.00      0.00      0.00        10\n",
      "          UL       0.71      0.57      0.63       930\n",
      "      VECTOR       0.00      0.00      0.00         5\n",
      "       VIDEO       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.84     71565\n",
      "   macro avg       0.25      0.22      0.22     71565\n",
      "weighted avg       0.79      0.84      0.80     71565\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            labels=np.unique(y_test),\n",
    "                            target_names=label_encoder.inverse_transform(np.unique(y_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
