{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cuDNN benchmarking for faster training\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_7320\\3723527011.py:5: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['tag'].str.contains(r'\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|A|HR|SEARCH|DETAILS|FIELDSET|SLOT|SVG|AD|ADSLOT|AUDIO|BLINK|BOLD|COL|COLGROUP|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MARK|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|OPTION|PATH|PROGRESS|STRIKE|SWAL|TEXT|TFOOT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1)\\b', regex=True)]\n",
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_7320\\3723527011.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col] = np.where(df[col].str.contains(pattern, regex=True, na=False), 'DIV', df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIV' 'BUTTON' 'INPUT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_7320\\3723527011.py:48: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['tag'].str.contains(r'\\b(P|IMG)\\b', regex=True)]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dataset and remove rows with '-' in the tag column\n",
    "df = pd.read_csv(\"figma_dataset.csv\")\n",
    "\n",
    "df = df[~df['tag'].str.contains(r'[-:]', regex=True)]\n",
    "df = df[~df['tag'].str.contains(r'\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|A|HR|SEARCH|DETAILS|FIELDSET|SLOT|SVG|AD|ADSLOT|AUDIO|BLINK|BOLD|COL|COLGROUP|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MARK|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|OPTION|PATH|PROGRESS|STRIKE|SWAL|TEXT|TFOOT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1)\\b', regex=True)]\n",
    "\n",
    "# Define the regex pattern for matching\n",
    "pattern = r'[-:]|\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|SEARCH|DETAILS|FIELDSET|SLOT|AD|ADSLOT|AUDIO|BLINK|BOLD|COL|COLGROUP|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MARK|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|OPTION|PATH|PROGRESS|STRIKE|SWAL|TEXT|TFOOT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1)\\b'\n",
    "\n",
    "# Apply the replacement conditionally\n",
    "for col in ['prev_sibling_html_tag', 'child_1_html_tag', 'child_2_html_tag']:\n",
    "    df[col] = np.where(df[col].str.contains(pattern, regex=True, na=False), 'DIV', df[col])\n",
    "\n",
    "# Define mapping for tag replacements\n",
    "tag_mapping = {\n",
    "    \"ARTICLE\": \"DIV\", \"DIV\": \"DIV\", \"FIGURE\": \"DIV\", \"FOOTER\": \"DIV\", \"HEADER\": \"DIV\", \"NAV\": \"DIV\", \"MAIN\": \"DIV\",\n",
    "    \"BODY\" : \"DIV\", \"FORM\" : \"DIV\", \"OL\" : \"DIV\", \"UL\" : \"DIV\", \"TABLE\": \"DIV\", \"THEAD\":\"DIV\" , \"TBODY\": \"DIV\", \"SECTION\" : \"DIV\",\n",
    "    \"H1\": \"P\", \"H2\": \"P\", \"H3\": \"P\", \"H4\": \"P\", \"H5\": \"P\", \"H6\": \"P\",\"SUP\": \"P\",\"SUB\": \"P\", \"BIG\": \"P\",\n",
    "    \"P\": \"P\", \"CAPTION\": \"P\", \"FIGCAPTION\": \"P\", \"B\": \"P\", \"EM\": \"P\", \"I\": \"P\", \"TD\": \"P\", \"TH\": \"P\", \"TR\": \"P\",\"PRE\":\"P\",\n",
    "    \"U\": \"P\", \"TIME\": \"P\", \"TXT\": \"P\", \"ABBR\": \"P\",\"SMALL\": \"P\",\"STRONG\": \"P\",\"SUMMARY\": \"P\",\"SPAN\": \"P\", \"LABEL\": \"P\",\"LI\":\"P\",\n",
    "    \"PICTURE\": \"IMG\" , \"VIDEO\": \"IMG\",\n",
    "    \"SELECT\": \"INPUT\",\"TEXTAREA\": \"INPUT\",\n",
    "    \"VECTOR\": \"SVG\"\n",
    "}\n",
    "\n",
    "# df.loc[(df[\"tag\"] == \"LABEL\") & ((df[\"type\"] == \"RECTANGLE\") | (df[\"type\"] == \"GROUP\")), \"tag\"] = \"DIV\"\n",
    "df.loc[(df[\"tag\"] == \"SPAN\") & ((df[\"type\"] == \"RECTANGLE\") | (df[\"type\"] == \"GROUP\")), \"tag\"] = \"DIV\"\n",
    "\n",
    "# Replace any value in children tag columns that contains '-' with 'DIV'\n",
    "children_cols = ['child_1_html_tag', 'child_2_html_tag']\n",
    "for col in children_cols:\n",
    "    df[col] = df[col].apply(lambda x: \"DIV\" if isinstance(x, str) and '-' in x else x)\n",
    "\n",
    "# Convert tag and parent_tag_html columns to uppercase\n",
    "df['tag'] = df['tag'].str.upper()\n",
    "df['prev_sibling_html_tag'] = df['prev_sibling_html_tag'].str.upper()\n",
    "df['child_1_html_tag'] = df['child_1_html_tag'].str.upper()\n",
    "df['child_2_html_tag'] = df['child_2_html_tag'].str.upper()\n",
    "\n",
    "# Apply mapping to 'tag' and 'parent_tag_html' columns\n",
    "df['tag'] = df['tag'].replace(tag_mapping)\n",
    "df['prev_sibling_html_tag'] = df['prev_sibling_html_tag'].replace(tag_mapping)\n",
    "df['child_1_html_tag'] = df['child_1_html_tag'].replace(tag_mapping)\n",
    "df['child_2_html_tag'] = df['child_2_html_tag'].replace(tag_mapping)\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df['tag'].str.contains(r'\\b(P|IMG)\\b', regex=True)]\n",
    "\n",
    "# Print remaining unique tags\n",
    "print(df['tag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate features and target\n",
    "y = df[\"tag\"]\n",
    "X = df.drop(columns=[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identify categorical and continuous columns\n",
    "categorical_cols = ['type','prev_sibling_html_tag','child_1_html_tag','child_2_html_tag']\n",
    "continuous_cols = [col for col in X.columns if col not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ohe_encoder.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process categorical features with OneHotEncoder instead of LabelEncoder\n",
    "X[categorical_cols] = X[categorical_cols].astype(str).fillna('unknown')\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_cat_encoded = ohe.fit_transform(X[categorical_cols])\n",
    "joblib.dump(ohe, \"ohe_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imputer.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better missing value handling with imputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_continuous_imputed = imputer.fit_transform(X[continuous_cols])\n",
    "joblib.dump(imputer, \"imputer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale continuous features\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X_continuous_imputed)\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine one-hot encoded categorical features with scaled continuous features\n",
    "X_processed = np.concatenate([X_cat_encoded, X_continuous_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(y_encoded)\n",
    "\n",
    "# Find classes with only 1 sample\n",
    "rare_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "\n",
    "# Duplicate rare class samples\n",
    "for cls in rare_classes:\n",
    "    idx = np.where(y_encoded == cls)[0][0]  # Get the index of the rare sample\n",
    "    original_class_name = label_encoder.inverse_transform([cls])[0]  # Convert back to original label\n",
    "    print(f\"Duplicating class '{original_class_name}' (only 1 sample present).\")\n",
    "\n",
    "    X_processed = np.vstack([X_processed, X_processed[idx]])  # Duplicate features\n",
    "    y_encoded = np.append(y_encoded, y_encoded[idx])  # Duplicate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train/test split - remove stratification if there are classes with too few samples\n",
    "unique_counts = np.unique(y_encoded, return_counts=True)\n",
    "min_samples = min(unique_counts[1])\n",
    "\n",
    "if min_samples < 2:\n",
    "    print(f\"Warning: The least populated class has only {min_samples} sample(s). Removing stratification.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Move GPU setup earlier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512,  # Larger batch size\n",
    "    shuffle=True, \n",
    "    num_workers=8,   # Parallel loading\n",
    "    pin_memory=True,  # Faster data transfer to GPU\n",
    "    prefetch_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights...\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "print(\"Computing class weights...\")\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define improved model architecture with proper input size\n",
    "class ImprovedTagClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.05):\n",
    "        super(ImprovedTagClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(self.relu(self.bn3(self.fc3(x))))\n",
    "        logits = self.fc4(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(label_encoder.classes_)\n",
    "model = ImprovedTagClassifier(input_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Setup mixed precision training\n",
    "scaler = GradScaler(device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [5/200], Loss: 0.0861, Time: 5.03s\n",
      "Epoch [10/200], Loss: 0.0644, Time: 5.30s\n",
      "Epoch [15/200], Loss: 0.0574, Time: 6.75s\n",
      "Epoch [20/200], Loss: 0.0483, Time: 4.90s\n",
      "Epoch [25/200], Loss: 0.0419, Time: 6.85s\n",
      "Epoch [30/200], Loss: 0.0343, Time: 4.91s\n",
      "Epoch [35/200], Loss: 0.0316, Time: 4.82s\n",
      "Epoch [40/200], Loss: 0.0239, Time: 4.91s\n",
      "Epoch [45/200], Loss: 0.0207, Time: 6.01s\n",
      "Epoch [50/200], Loss: 0.0218, Time: 4.83s\n",
      "Epoch [55/200], Loss: 0.0198, Time: 6.13s\n",
      "Epoch [60/200], Loss: 0.0235, Time: 5.37s\n",
      "Epoch [65/200], Loss: 0.0256, Time: 5.21s\n",
      "Early stopping at epoch 65\n"
     ]
    }
   ],
   "source": [
    "# 9. Training loop with timing and early stopping\n",
    "print(\"Starting training...\")\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "early_stop = False\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move batch to device\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use mixed precision for faster training\n",
    "        with torch.amp.autocast('cuda', enabled=device.type=='cuda'):\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Scale gradients and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_tag_classifier.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "    \n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 339.48 seconds\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"tag_classifier.pth\")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_7320\\762082746.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_tag_classifier.pth\"))  # Load best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluation on the test set\n",
    "print(\"Evaluating model...\")\n",
    "model.load_state_dict(torch.load(\"best_tag_classifier.pth\"))  # Load best model\n",
    "model.eval()\n",
    "\n",
    "# Process test data in batches for memory efficiency\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "y_pred = np.array(all_predictions)\n",
    "y_test_np = np.array(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BUTTON       0.84      0.98      0.90       569\n",
      "         DIV       1.00      0.99      1.00     23019\n",
      "       INPUT       0.77      0.97      0.86       148\n",
      "\n",
      "    accuracy                           0.99     23736\n",
      "   macro avg       0.87      0.98      0.92     23736\n",
      "weighted avg       0.99      0.99      0.99     23736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test_np, \n",
    "    y_pred,\n",
    "    labels=np.unique(y_test_np),\n",
    "    target_names=label_encoder.inverse_transform(np.unique(y_test_np))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing feature importance...\n",
      "\n",
      "Top 100 most important features:\n",
      "width: 0.1449\n",
      "aspect_ratio: 0.1228\n",
      "prev_sibling_html_tag_BUTTON: 0.1227\n",
      "child_2_html_tag_SVG: 0.1197\n",
      "child_1_html_tag_SVG: 0.1175\n",
      "height: 0.1169\n",
      "prev_sibling_html_tag_A: 0.1165\n",
      "sibling_count: 0.1152\n",
      "child_2_html_tag_INPUT: 0.1117\n",
      "num_children_to_end: 0.1089\n",
      "child_2_html_tag_P: 0.1083\n",
      "type_ELLIPSE: 0.1080\n",
      "child_1_html_tag_IMG: 0.1071\n",
      "chars_count_to_end: 0.1069\n",
      "prev_sibling_html_tag_INPUT: 0.1041\n",
      "prev_sibling_html_tag_IMG: 0.1039\n",
      "prev_sibling_html_tag_DIV: 0.1011\n",
      "child_2_html_tag_DIV: 0.1009\n",
      "child_1_html_tag_A: 0.1005\n",
      "child_1_html_tag_INPUT: 0.1003\n",
      "prev_sibling_html_tag_P: 0.1003\n",
      "child_2_html_tag_A: 0.1000\n",
      "child_1_html_tag_BUTTON: 0.0993\n",
      "child_2_html_tag_nan: 0.0978\n",
      "child_2_html_tag_IMG: 0.0968\n",
      "child_1_html_tag_HR: 0.0956\n",
      "child_1_html_tag_nan: 0.0934\n",
      "child_1_html_tag_P: 0.0923\n",
      "prev_sibling_html_tag_SVG: 0.0919\n",
      "child_2_html_tag_BUTTON: 0.0911\n",
      "child_1_html_tag_DIV: 0.0907\n",
      "prev_sibling_html_tag_nan: 0.0867\n",
      "type_FRAME: 0.0825\n",
      "child_2_html_tag_HR: 0.0806\n",
      "type_RECTANGLE: 0.0787\n",
      "prev_sibling_html_tag_HR: 0.0783\n",
      "type_GROUP: 0.0775\n",
      "text_length: 0.0766\n",
      "border_radius: 0.0730\n",
      "has_background_color: 0.0693\n",
      "child_1_percentage_of_parent: 0.0609\n",
      "child_2_percentage_of_parent: 0.0598\n",
      "nearest_text_node_dist: 0.0548\n",
      "distinct_background: 0.0454\n"
     ]
    }
   ],
   "source": [
    "# Print feature importances from the model\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "with torch.no_grad():\n",
    "    weights = model.fc1.weight.cpu().numpy()\n",
    "    importance = np.abs(weights).mean(axis=0)\n",
    "    \n",
    "    # Get feature names (both categorical encoded and continuous)\n",
    "    cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "    all_feature_names = np.concatenate([cat_feature_names, np.array(continuous_cols)])\n",
    "    \n",
    "    feature_importance = list(zip(all_feature_names, importance))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 100 most important features:\")\n",
    "    for feature, imp in feature_importance[:100]:\n",
    "        print(f\"{feature}: {imp:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
