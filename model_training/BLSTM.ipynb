{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba31bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FigmaDataset(Dataset):\n",
    "    \"\"\"Dataset for Figma node sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels, sequence_ids):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            features (dict): Dictionary mapping sequence IDs to feature tensors\n",
    "            labels (dict): Dictionary mapping sequence IDs to label tensors\n",
    "            sequence_ids (list): List of unique sequence IDs\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.sequence_ids = sequence_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_id = self.sequence_ids[idx]\n",
    "        return {\n",
    "            'features': self.features[seq_id],\n",
    "            'labels': self.labels[seq_id],\n",
    "            'seq_id': seq_id\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable length sequences.\"\"\"\n",
    "    features = [item['features'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    seq_ids = [item['seq_id'] for item in batch]\n",
    "    \n",
    "    # Get sequence lengths\n",
    "    lengths = torch.tensor([len(f) for f in features])\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # Padding for features (get feature dimension from first item)\n",
    "    feature_dim = features[0].shape[1]\n",
    "    padded_features = torch.zeros((len(batch), max_len, feature_dim))\n",
    "    \n",
    "    # Padding for labels\n",
    "    padded_labels = torch.ones((len(batch), max_len), dtype=torch.long) * -100  # Use -100 for ignore_index in CrossEntropyLoss\n",
    "    \n",
    "    # Fill padded tensors\n",
    "    for i, (f, l) in enumerate(zip(features, labels)):\n",
    "        seq_len = f.shape[0]\n",
    "        padded_features[i, :seq_len] = f\n",
    "        padded_labels[i, :seq_len] = l\n",
    "    \n",
    "    return {\n",
    "        'features': padded_features,\n",
    "        'labels': padded_labels,\n",
    "        'lengths': lengths,\n",
    "        'seq_ids': seq_ids\n",
    "    }\n",
    "\n",
    "class FigmaBLSTM(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM model for Figma tag prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the BLSTM model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features\n",
    "            hidden_dim (int): Dimension of hidden layers\n",
    "            output_dim (int): Number of output classes (HTML tags)\n",
    "            num_layers (int): Number of LSTM layers\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(FigmaBLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "            lengths (torch.Tensor): Lengths of each sequence in the batch\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions of shape (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack output\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Linear layer to get logits\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class FigmaHTMLPredictor:\n",
    "    \"\"\"Class for training and predicting HTML tags from Figma features.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config=None):\n",
    "        \"\"\"\n",
    "        Initialize the predictor.\n",
    "        \n",
    "        Args:\n",
    "            model_config (dict, optional): Configuration for the model\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model_config = model_config if model_config else {\n",
    "            'hidden_dim': 256,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 16,\n",
    "            'epochs': 20\n",
    "        }\n",
    "    \n",
    "    def load_data(self, data_path, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Load and preprocess data.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the data file (CSV or HDF5)\n",
    "            test_size (float): Proportion of data to use for testing\n",
    "            random_state (int): Random seed for splitting\n",
    "            \n",
    "        Returns:\n",
    "            tuple: train_loader, val_loader, num_classes, input_dim\n",
    "        \"\"\"\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "        \n",
    "        # Load based on file extension\n",
    "        if data_path.endswith('.csv'):\n",
    "            df = pd.read_csv(data_path)\n",
    "            # Convert string representation of feature vectors to numpy arrays\n",
    "            df['feature_vector'] = df['feature_vector'].apply(lambda x: np.array(eval(x)))\n",
    "        elif data_path.endswith('.hdf5') or data_path.endswith('.h5'):\n",
    "            with h5py.File(data_path, 'r') as f:\n",
    "                # Convert HDF5 to pandas DataFrame\n",
    "                sequence_ids = [s.decode('utf-8') for s in f['sequence_id'][:]]\n",
    "                node_ids = [s.decode('utf-8') for s in f['node_id'][:]]\n",
    "                feature_vectors = f['feature_vector'][:]\n",
    "                tags = [s.decode('utf-8') for s in f['tag'][:]]\n",
    "                raw_tags = [s.decode('utf-8') for s in f['raw_tag'][:]]\n",
    "                node_types = [s.decode('utf-8') for s in f['node_type'][:]]\n",
    "                depths = f['depth'][:]\n",
    "                positions = f['position'][:]\n",
    "                parent_tags = [s.decode('utf-8') for s in f['parent_tag'][:]]\n",
    "                \n",
    "                df = pd.DataFrame({\n",
    "                    'sequence_id': sequence_ids,\n",
    "                    'node_id': node_ids,\n",
    "                    'feature_vector': list(feature_vectors),\n",
    "                    'tag': tags,\n",
    "                    'raw_tag': raw_tags,\n",
    "                    'node_type': node_types,\n",
    "                    'depth': depths,\n",
    "                    'position': positions,\n",
    "                    'parent_tag': parent_tags\n",
    "                })\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {data_path}\")\n",
    "        \n",
    "        print(f\"Loaded {len(df)} records with {df['sequence_id'].nunique()} unique sequences\")\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder.fit(df['tag'].unique())\n",
    "        print(f\"Found {len(self.label_encoder.classes_)} unique tags: {self.label_encoder.classes_}\")\n",
    "        \n",
    "        # Get input dimension from first feature vector\n",
    "        input_dim = len(df['feature_vector'].iloc[0])\n",
    "        print(f\"Input feature dimension: {input_dim}\")\n",
    "        \n",
    "        # Group by sequence_id to create sequences\n",
    "        sequences = {}\n",
    "        for seq_id, group in df.groupby('sequence_id'):\n",
    "            feature_vectors = np.stack(group['feature_vector'].values)\n",
    "            labels = self.label_encoder.transform(group['tag'].values)\n",
    "            sequences[seq_id] = {\n",
    "                'features': torch.FloatTensor(feature_vectors),\n",
    "                'labels': torch.LongTensor(labels)\n",
    "            }\n",
    "        \n",
    "        # Split into train and validation sets\n",
    "        train_seq_ids, val_seq_ids = train_test_split(\n",
    "            list(sequences.keys()), \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"Training on {len(train_seq_ids)} sequences, validating on {len(val_seq_ids)} sequences\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_features = {seq_id: sequences[seq_id]['features'] for seq_id in train_seq_ids}\n",
    "        train_labels = {seq_id: sequences[seq_id]['labels'] for seq_id in train_seq_ids}\n",
    "        val_features = {seq_id: sequences[seq_id]['features'] for seq_id in val_seq_ids}\n",
    "        val_labels = {seq_id: sequences[seq_id]['labels'] for seq_id in val_seq_ids}\n",
    "        \n",
    "        train_dataset = FigmaDataset(train_features, train_labels, train_seq_ids)\n",
    "        val_dataset = FigmaDataset(val_features, val_labels, val_seq_ids)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.model_config['batch_size'], \n",
    "            shuffle=True, \n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.model_config['batch_size'], \n",
    "            shuffle=False, \n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, len(self.label_encoder.classes_), input_dim\n",
    "    \n",
    "    def build_model(self, input_dim, num_classes):\n",
    "        \"\"\"\n",
    "        Build the BLSTM model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features\n",
    "            num_classes (int): Number of output classes\n",
    "            \n",
    "        Returns:\n",
    "            FigmaBLSTM: The model\n",
    "        \"\"\"\n",
    "        model = FigmaBLSTM(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=self.model_config['hidden_dim'],\n",
    "            output_dim=num_classes,\n",
    "            num_layers=self.model_config['num_layers'],\n",
    "            dropout=self.model_config['dropout']\n",
    "        )\n",
    "        model = model.to(self.device)\n",
    "        print(f\"Model built with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "        return model\n",
    "    \n",
    "    def train(self, data_path, output_dir='./models', model_name='figma_blstm_model.pt'):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the data file\n",
    "            output_dir (str): Directory to save the model\n",
    "            model_name (str): Name of the model file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history\n",
    "        \"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, val_loader, num_classes, input_dim = self.load_data(data_path)\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(input_dim, num_classes)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.model_config['learning_rate'])\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(self.model_config['epochs']):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.model_config['epochs']} [Train]\")\n",
    "            for batch in progress_bar:\n",
    "                # Get batch data\n",
    "                features = batch['features'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                lengths = batch['lengths']\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(features, lengths)\n",
    "                batch_size, seq_len, num_classes = outputs.size()\n",
    "                \n",
    "                # Reshape outputs and labels for loss calculation\n",
    "                outputs = outputs.reshape(-1, num_classes)\n",
    "                labels = labels.reshape(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update statistics\n",
    "                train_loss += loss.item()\n",
    "                train_batches += 1\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / train_batches\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{self.model_config['epochs']} [Val]\")\n",
    "                for batch in progress_bar:\n",
    "                    # Get batch data\n",
    "                    features = batch['features'].to(self.device)\n",
    "                    labels = batch['labels'].to(self.device)\n",
    "                    lengths = batch['lengths']\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self.model(features, lengths)\n",
    "                    batch_size, seq_len, num_classes = outputs.size()\n",
    "                    \n",
    "                    # Reshape outputs and labels for loss calculation\n",
    "                    outputs_flat = outputs.reshape(-1, num_classes)\n",
    "                    labels_flat = labels.reshape(-1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(outputs_flat, labels_flat)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    mask = (labels_flat != -100)\n",
    "                    if mask.sum() > 0:\n",
    "                        predicted = torch.argmax(outputs_flat[mask], dim=1)\n",
    "                        correct = (predicted == labels_flat[mask]).sum().item()\n",
    "                        total_correct += correct\n",
    "                        total_samples += mask.sum().item()\n",
    "                    \n",
    "                    progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            val_accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "            \n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.model_config['epochs']}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                model_path = os.path.join(output_dir, model_name)\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': best_val_loss,\n",
    "                    'label_encoder': self.label_encoder,\n",
    "                    'model_config': self.model_config\n",
    "                }, model_path)\n",
    "                print(f\"  Model saved to {model_path}\")\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Load a trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the model file\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Load model configuration\n",
    "        self.model_config = checkpoint.get('model_config', self.model_config)\n",
    "        \n",
    "        # Load label encoder\n",
    "        self.label_encoder = checkpoint['label_encoder']\n",
    "        \n",
    "        # Build model\n",
    "        input_dim = checkpoint['model_state_dict']['lstm.weight_ih_l0'].size(1)\n",
    "        num_classes = checkpoint['model_state_dict']['fc.weight'].size(0)\n",
    "        \n",
    "        self.model = FigmaBLSTM(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=self.model_config['hidden_dim'],\n",
    "            output_dim=num_classes,\n",
    "            num_layers=self.model_config['num_layers'],\n",
    "            dropout=self.model_config['dropout']\n",
    "        )\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    def predict(self, features, sequence_id='test_sequence'):\n",
    "        \"\"\"\n",
    "        Predict HTML tags for a sequence of features.\n",
    "        \n",
    "        Args:\n",
    "            features (np.ndarray): Feature vectors of shape (seq_len, feature_dim)\n",
    "            sequence_id (str): Identifier for the sequence\n",
    "            \n",
    "        Returns:\n",
    "            list: Predicted HTML tags\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        # Convert to tensor\n",
    "        features_tensor = torch.FloatTensor(features).unsqueeze(0)  # Add batch dimension\n",
    "        lengths = torch.tensor([features_tensor.size(1)])\n",
    "        \n",
    "        # Move to device\n",
    "        features_tensor = features_tensor.to(self.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(features_tensor, lengths)\n",
    "            predictions = torch.argmax(outputs, dim=2).squeeze(0)\n",
    "        \n",
    "        # Convert to class labels\n",
    "        predicted_labels = self.label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "        \n",
    "        return predicted_labels\n",
    "    \n",
    "    def predict_batch(self, data_loader):\n",
    "        \"\"\"\n",
    "        Predict HTML tags for a batch of sequences.\n",
    "        \n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader containing the sequences\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary mapping sequence IDs to predicted tags and true tags\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        results = {}\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "                # Get batch data\n",
    "                features = batch['features'].to(self.device)\n",
    "                labels = batch['labels']\n",
    "                lengths = batch['lengths']\n",
    "                seq_ids = batch['seq_ids']\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(features, lengths)\n",
    "                predictions = torch.argmax(outputs, dim=2)\n",
    "                \n",
    "                # Process each sequence in the batch\n",
    "                for i, seq_id in enumerate(seq_ids):\n",
    "                    seq_len = lengths[i].item()\n",
    "                    pred_indices = predictions[i, :seq_len].cpu().numpy()\n",
    "                    true_indices = labels[i, :seq_len].cpu().numpy()\n",
    "                    \n",
    "                    # Convert indices to tags\n",
    "                    pred_tags = self.label_encoder.inverse_transform(pred_indices)\n",
    "                    true_tags = [self.label_encoder.classes_[idx] if idx != -100 else \"UNKNOWN\" for idx in true_indices]\n",
    "                    \n",
    "                    results[seq_id] = {\n",
    "                        'predicted_tags': pred_tags,\n",
    "                        'true_tags': true_tags\n",
    "                    }\n",
    "        \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_config = {\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 16,\n",
    "        'epochs': 20\n",
    "    }\n",
    "    \n",
    "    # Initialize predictor\n",
    "    predictor = FigmaHTMLPredictor(model_config)\n",
    "    \n",
    "    # Train model\n",
    "    data_path = \"figma_dataset_custom.csv\"  # Path to the output from FigmaHTMLFeatureExtractor\n",
    "    output_dir = \"./models\"\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training BLSTM model...\")\n",
    "    history = predictor.train(data_path, output_dir)\n",
    "    \n",
    "    # Example of loading and using the model\n",
    "    print(\"\\nLoading trained model...\")\n",
    "    predictor.load_model(os.path.join(output_dir, \"figma_blstm_model.pt\"))\n",
    "    \n",
    "    # Load test data for prediction demonstration\n",
    "    _, test_loader, _, _ = predictor.load_data(data_path, test_size=0.2)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on test data...\")\n",
    "    results = predictor.predict_batch(test_loader)\n",
    "    \n",
    "    # Print sample predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i, (seq_id, seq_results) in enumerate(list(results.items())[:3]):\n",
    "        print(f\"\\nSequence {i+1} (ID: {seq_id}):\")\n",
    "        \n",
    "        # Get the true and predicted tags\n",
    "        true_tags = seq_results['true_tags']\n",
    "        pred_tags = seq_results['predicted_tags']\n",
    "        \n",
    "        # Print first 10 predictions\n",
    "        for j in range(min(10, len(true_tags))):\n",
    "            print(f\"  Node {j+1}: True={true_tags[j]}, Predicted={pred_tags[j]}\")\n",
    "    \n",
    "    print(\"\\nModel training and evaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
