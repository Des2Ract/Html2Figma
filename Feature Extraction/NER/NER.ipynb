{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc60576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import h5py\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 1370 JSON files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1370/1370 [45:17<00:00,  1.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Statistics ---\n",
      "Files processed: 1370\n",
      "Nodes processed: 1337752\n",
      "JSON errors: 0\n",
      "Unique node types: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FigmaHTMLFeatureExtractor:\n",
    "    def __init__(self, semantic_model_name='all-MiniLM-L6-v2', node_type_embedding_dim=50, output_format='hdf5'):\n",
    "        # Using a pre-trained model to turn text into embeddings\n",
    "        self.semantic_model = SentenceTransformer(semantic_model_name)\n",
    "        self.text_embedding_dim = 384  # Size of text embeddings\n",
    "        self.node_name_embedding_dim = 384  # Size for node name embeddings\n",
    "        \n",
    "        # Set up node type embeddings (to represent different Figma node types)\n",
    "        self.node_types = self._get_node_types()\n",
    "        self.node_type_to_idx = {node_type: idx for idx, node_type in enumerate(self.node_types)}\n",
    "        self.node_type_embedding_dim = node_type_embedding_dim\n",
    "        self.node_type_embedding_layer = nn.Embedding(len(self.node_types), self.node_type_embedding_dim)\n",
    "        \n",
    "        # Set up tag mapping and cleaning stuff\n",
    "        self.tag_mapping = self._get_tag_mapping()\n",
    "        self.custom_tag_removal_pattern = self._get_custom_tag_removal_pattern()\n",
    "        self.default_tag = \"DIV\"  # Fallback tag\n",
    "        self.icon_like_node_types = {\"VECTOR\", \"INSTANCE\", \"COMPONENT\", \"SHAPE\", \"SVG_ICON\"}\n",
    "        \n",
    "        # Output format (csv or hdf5 for saving features)\n",
    "        self.output_format = output_format\n",
    "        \n",
    "        # Keep track of stats while processing\n",
    "        self.stats = {\n",
    "            \"files_processed\": 0,\n",
    "            \"nodes_processed\": 0,\n",
    "            \"json_errors\": 0,\n",
    "            \"tag_mappings\": {},\n",
    "            \"unique_node_types\": set()\n",
    "        }\n",
    "\n",
    "    def _get_node_types(self):\n",
    "        # This makes a list of all possible Figma node types\n",
    "        return [\n",
    "            \"TEXT\", \"RECTANGLE\", \"GROUP\", \"ELLIPSE\", \"FRAME\", \"VECTOR\", \"STAR\", \"LINE\", \n",
    "            \"POLYGON\", \"BOOLEAN_OPERATION\", \"SLICE\", \"COMPONENT\", \"INSTANCE\", \"COMPONENT_SET\", \n",
    "            \"DOCUMENT\", \"CANVAS\", \"SECTION\", \"SHAPE_WITH_TEXT\", \"STICKY\", \"TABLE\", \"WASHI_TAPE\", \n",
    "            \"CONNECTOR\", \"HIGHLIGHT\", \"WIDGET\", \"EMBED\", \"LINK\", \"LINK_UNFURL\", \"MEDIA\", \"CODE_BLOCK\", \n",
    "            \"STAMP\", \"COMMENT\", \"FREEFORM\", \"TIMELINE\", \"STICKER\", \"SHAPE\", \"ARROW\", \"CALL_OUT\", \n",
    "            \"FLOW\", \"TEXT_AREA\", \"TEXT_FIELD\", \"BUTTON\", \"CHECKBOX\", \"RADIO\", \"TOGGLE\", \"SLIDER\", \n",
    "            \"DROPDOWN\", \"COMBOBOX\", \"LIST\", \"TABLE_CELL\", \"TABLE_ROW\", \"TABLE_COLUMN\", \"TABLE_SECTION\", \n",
    "            \"TABLE_HEADER\", \"TABLE_FOOTER\", \"TABLE_BODY\", \"TABLE_CAPTION\", \"TABLE_COLGROUP\", \"TABLE_COL\", \n",
    "            \"TABLE_THEAD\", \"TABLE_TBODY\", \"TABLE_TFOOT\", \"TABLE_TR\", \"TABLE_TH\", \"TABLE_TD\", \n",
    "            \"UNKNOWN_TYPE\"\n",
    "        ]\n",
    "\n",
    "    def _get_tag_mapping(self):\n",
    "        # Cluster HTML tags\n",
    "        return {\n",
    "            \"ARTICLE\": \"DIV\", \"DIV\": \"DIV\", \"FIGURE\": \"DIV\", \"FOOTER\": \"DIV\", \"HEADER\": \"DIV\", \n",
    "            \"NAV\": \"DIV\", \"MAIN\": \"DIV\", \"IFRAME\": \"DIV\", \"BODY\": \"DIV\", \"FORM\": \"DIV\", \n",
    "            \"TABLE\": \"DIV\", \"THEAD\": \"DIV\", \"TBODY\": \"DIV\", \"SECTION\": \"DIV\", \"ASIDE\": \"DIV\",\n",
    "            \"UL\": \"LIST\", \"OL\": \"LIST\", \"DL\": \"LIST\",\n",
    "            \"H1\": \"P\", \"H2\": \"P\", \"H3\": \"P\", \"H4\": \"P\", \"H5\": \"P\", \"H6\": \"P\", \"SUP\": \"P\", \n",
    "            \"SUB\": \"P\", \"BIG\": \"P\", \"P\": \"P\", \"CAPTION\": \"P\", \"FIGCAPTION\": \"P\", \"B\": \"P\", \n",
    "            \"EM\": \"P\", \"I\": \"P\", \"TD\": \"P\", \"TH\": \"P\", \"TR\": \"P\", \"PRE\": \"P\", \"U\": \"P\", \n",
    "            \"TIME\": \"P\", \"TXT\": \"P\", \"ABBR\": \"P\", \"SMALL\": \"P\", \"STRONG\": \"P\", \"SUMMARY\": \"P\", \n",
    "            \"SPAN\": \"P\", \"LABEL\": \"P\", \"LI\": \"P\", \"DD\": \"P\", \"A\": \"P\", \"BLOCKQUOTE\": \"P\", \n",
    "            \"CODE\": \"P\", \"PICTURE\": \"IMG\", \"VIDEO\": \"IMG\", \"SELECT\": \"INPUT\", \"TEXTAREA\": \"INPUT\",\n",
    "            \"VECTOR\": \"SVG\", \"ICON\": \"SVG\", \"UNK\": \"CONTAINER\"\n",
    "        }\n",
    "\n",
    "    def _get_custom_tag_removal_pattern(self):\n",
    "        # remove weird tags\n",
    "        return r'[-:]|\\b(DETAILS|CANVAS|FIELDSET|COLGROUP|COL|CNX|ADDRESS|CITE|S|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|FRONT|Q|SEARCH|SLOT|AD|ADSLOT|BLINK|BOLD|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|PATH|PROGRESS|STRIKE|SWAL|TEXT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1)\\b'\n",
    "\n",
    "    def clean_and_map_tag(self, raw_tag):\n",
    "        # Apply tag cleaning\n",
    "        if not raw_tag:\n",
    "            return self.default_tag\n",
    "        raw_tag = raw_tag.upper()  # Make it all uppercase\n",
    "        cleaned_tag = self.tag_mapping.get(raw_tag, raw_tag)  # Map to a clean tag\n",
    "        if re.search(self.custom_tag_removal_pattern, cleaned_tag, re.IGNORECASE):\n",
    "            cleaned_tag = self.default_tag  # If it is weird use default\n",
    "        final_tag = self.tag_mapping.get(cleaned_tag, cleaned_tag)\n",
    "        if final_tag != raw_tag:\n",
    "            self.stats[\"tag_mappings\"][raw_tag] = self.stats[\"tag_mappings\"].get(raw_tag, 0) + 1\n",
    "        return final_tag\n",
    "\n",
    "    def determine_bioes_label(self, base_tag):\n",
    "        # Decides if a tag should be a \"beginning\" container or just itself\n",
    "        bioes_label = \"\"\n",
    "        if base_tag == \"CONTAINER\":\n",
    "            bioes_label = \"B_CONTAINER\"  # Mark the start of a container\n",
    "        else:\n",
    "            bioes_label = base_tag\n",
    "        return bioes_label\n",
    "\n",
    "    def extract_features(self, node_data_item, current_body_width, sequence_id, \n",
    "                       parent_node_height=None, parent_base_tag=None, depth=0, \n",
    "                       position_in_siblings=0, total_siblings=1):\n",
    "        features_and_labels_list = []\n",
    "        node_dict = node_data_item.get(\"node\", {})\n",
    "        raw_tag = node_data_item.get(\"tag\", \"UNK\").upper()\n",
    "\n",
    "        # get cleaned tag\n",
    "        has_children = bool(node_data_item.get(\"children\"))\n",
    "        base_tag = self.clean_and_map_tag(raw_tag)\n",
    "        \n",
    "        # Get the label (B_CONTAINER or just the tag)\n",
    "        bioes_label = self.determine_bioes_label(base_tag)\n",
    "        \n",
    "        # Get node type embedding\n",
    "        node_type_str = node_dict.get(\"type\", \"UNKNOWN_TYPE\")\n",
    "        self.stats[\"unique_node_types\"].add(node_type_str)\n",
    "        node_type_idx = self.node_type_to_idx.get(node_type_str, self.node_type_to_idx.get(\"UNKNOWN_TYPE\", 0))\n",
    "        node_type_emb = self.node_type_embedding_layer(torch.tensor(node_type_idx)).detach().numpy()\n",
    "        \n",
    "        # Get text embedding\n",
    "        text_content = node_dict.get(\"characters\", \"\").strip()\n",
    "        text_emb = self.semantic_model.encode(text_content) if node_type_str == \"TEXT\" and text_content else np.zeros(self.text_embedding_dim)\n",
    "        \n",
    "        # Get node name embedding\n",
    "        node_name = node_data_item.get(\"name\", \"\").strip()\n",
    "        node_name_emb = self.semantic_model.encode(node_name) if node_name and (node_type_str in self.icon_like_node_types or \"icon\" in node_name.lower()) else np.zeros(self.node_name_embedding_dim)\n",
    "        \n",
    "        # Get numerical features\n",
    "        eps = 1e-6  # avoid dividing by zero\n",
    "        node_width = float(node_dict.get(\"width\", 0))\n",
    "        node_height = float(node_dict.get(\"height\", 0))\n",
    "        aspect_ratio = node_width / (node_height + eps) if node_height > 0 else 0\n",
    "        normalized_width = node_width / (current_body_width + eps) if current_body_width > 0 else 0\n",
    "        normalized_height = node_height / (parent_node_height + eps) if parent_node_height and parent_node_height > 0 else 0\n",
    "        \n",
    "        x_position = float(node_dict.get(\"x\", 0))\n",
    "        y_position = float(node_dict.get(\"y\", 0))\n",
    "        normalized_x = x_position / (current_body_width + eps) if current_body_width > 0 else 0\n",
    "        normalized_y = y_position / (parent_node_height + eps) if parent_node_height and parent_node_height > 0 else 0\n",
    "        \n",
    "        normalized_depth = min(depth / 20.0, 1.0)  # max depth at 20\n",
    "        normalized_position = position_in_siblings / (total_siblings + eps)\n",
    "        \n",
    "        # Get background color\n",
    "        bg_color = [0, 0, 0, 0]\n",
    "        fills = node_dict.get(\"fills\", [])\n",
    "        if fills and isinstance(fills, list) and len(fills) > 0:\n",
    "            color = fills[0][\"color\"]\n",
    "            if color:\n",
    "                bg_color = [color.get(k, 0) for k in (\"r\", \"g\", \"b\", \"a\")]\n",
    "        \n",
    "        font_size = float(node_dict.get(\"fontSize\", 0)) / 100.0\n",
    "        flex_direction = 1 if node_dict.get(\"flexDirection\", \"\") == \"column\" else 0\n",
    "        \n",
    "        # Combine all features\n",
    "        feature_vector = np.concatenate([\n",
    "            node_type_emb,\n",
    "            text_emb,\n",
    "            node_name_emb,\n",
    "            [normalized_width, normalized_height, aspect_ratio,\n",
    "             normalized_x, normalized_y,\n",
    "             normalized_depth, normalized_position,\n",
    "             *bg_color, font_size, flex_direction]\n",
    "        ])\n",
    "        \n",
    "        # Append features to the list\n",
    "        features_and_labels_list.append({\n",
    "            \"feature_vector\": feature_vector,\n",
    "            \"tag\": bioes_label\n",
    "        })\n",
    "        \n",
    "        self.stats[\"nodes_processed\"] += 1\n",
    "        \n",
    "        # Recursively process children\n",
    "        if has_children:\n",
    "            children = node_data_item[\"children\"]\n",
    "            total_children = len(children)\n",
    "            for child_idx, child_node in enumerate(children):\n",
    "                child_features = self.extract_features(\n",
    "                    node_data_item=child_node,\n",
    "                    current_body_width=current_body_width,\n",
    "                    sequence_id=sequence_id,\n",
    "                    parent_node_height=node_height,\n",
    "                    parent_base_tag=base_tag,\n",
    "                    depth=depth + 1,\n",
    "                    position_in_siblings=child_idx,\n",
    "                    total_siblings=total_children\n",
    "                )\n",
    "                features_and_labels_list.extend(child_features)\n",
    "            \n",
    "            # If this is a container, add an end tag => (used to end context in training)\n",
    "            if bioes_label == \"B_CONTAINER\":\n",
    "                e_container_feature = {\n",
    "                    \"feature_vector\": np.zeros_like(feature_vector),\n",
    "                    \"tag\": \"E_CONTAINER\"\n",
    "                }\n",
    "                features_and_labels_list.append(e_container_feature)\n",
    "                \n",
    "        return features_and_labels_list\n",
    "\n",
    "    def process_file(self, file_path):\n",
    "        # Process JSON file and extract its features\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "            sequence_id = os.path.basename(file_path).replace(\".json\", \"\")\n",
    "            root_node_info = json_data.get(\"node\", {})\n",
    "            body_width = float(root_node_info.get(\"width\", 1000.0)) or 1000.0\n",
    "            features = self.extract_features(\n",
    "                node_data_item=json_data,\n",
    "                current_body_width=body_width,\n",
    "                sequence_id=sequence_id\n",
    "            )\n",
    "            \n",
    "            # Add an end-of-website tag => (used to end context when training)\n",
    "            e_website_feature = {\n",
    "                \"feature_vector\": np.ones_like(features[0][\"feature_vector\"]),\n",
    "                \"tag\": \"E_WEBSITE\"\n",
    "            }\n",
    "            features.append(e_website_feature)\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Oops, something broke with {file_path}: {str(e)}\")\n",
    "            self.stats[\"json_errors\"] += 1\n",
    "            return None\n",
    "\n",
    "    def process_directory(self, input_dir, output_path):\n",
    "        # Process JSON files in a folder and save their features\n",
    "        if not os.path.exists(input_dir):\n",
    "            print(f\"Uh-oh, {input_dir} doesn’t exist!\")\n",
    "            return\n",
    "        json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "        if not json_files:\n",
    "            print(f\"No JSON files in {input_dir}. Bummer!\")\n",
    "            return\n",
    "        print(f\"Found {len(json_files)} JSON files to process. Let’s go!\")\n",
    "\n",
    "        # bool used to write headers first time only\n",
    "        first_file = True\n",
    "\n",
    "        for file_name in tqdm(json_files, desc=\"Processing files\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            features = self.process_file(file_path)\n",
    "            if features:\n",
    "                self._save_features(features, output_path, append=not first_file)\n",
    "                self.stats[\"files_processed\"] += 1\n",
    "                first_file = False\n",
    "\n",
    "        self._print_stats()\n",
    "\n",
    "    def _save_features(self, features, output_path, append=False):\n",
    "        # Save the features\n",
    "        if not features:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"feature_vector\": [f[\"feature_vector\"].tolist() for f in features],\n",
    "            \"tag\": [f[\"tag\"] for f in features]\n",
    "        })\n",
    "\n",
    "        if self.output_format == 'csv':\n",
    "            df.to_csv(output_path, index=False, header=not append, mode='a' if append else 'w')\n",
    "        elif self.output_format == 'parquet':\n",
    "            df.to_parquet(output_path, index=False, append=append)\n",
    "        elif self.output_format == 'hdf5':\n",
    "            with h5py.File(output_path, 'a' if append else 'w') as f:\n",
    "                for column in df.columns:\n",
    "                    data = df[column].apply(lambda x: x if not isinstance(x, list) else np.array(x)).values\n",
    "                    if column == \"feature_vector\":\n",
    "                        data = np.vstack(data)\n",
    "                        dtype = np.float32\n",
    "                    elif df[column].dtype == object:\n",
    "                        dtype = 'S100'\n",
    "                        data = np.array(data, dtype=dtype)\n",
    "                    else:\n",
    "                        dtype = df[column].dtype\n",
    "                        data = np.array(data)\n",
    "\n",
    "                    if column in f:\n",
    "                        old_size = f[column].shape[0]\n",
    "                        new_size = old_size + data.shape[0]\n",
    "                        f[column].resize((new_size,) + f[column].shape[1:])\n",
    "                        f[column][old_size:] = data\n",
    "                    else:\n",
    "                        maxshape = (None,) + data.shape[1:] if len(data.shape) > 1 else (None,)\n",
    "                        f.create_dataset(column, data=data, maxshape=maxshape, chunks=True)\n",
    "\n",
    "                    f.attrs['num_samples'] = f[column].shape[0]\n",
    "                    f.attrs['feature_dim'] = data.shape[1] if len(data.shape) > 1 else 1\n",
    "\n",
    "    def _print_stats(self):\n",
    "        print(\"\\n--- Processing Stats ---\")\n",
    "        print(f\"Files processed: {self.stats['files_processed']}\")\n",
    "        print(f\"Nodes processed: {self.stats['nodes_processed']}\")\n",
    "        print(f\"JSON errors: {self.stats['json_errors']}\")\n",
    "        print(f\"Unique node types: {len(self.stats['unique_node_types'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4061601",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"../../Data/new_json_data6\"\n",
    "    output_path = \"../Output/figma_dataset_custom.h5\"\n",
    "    extractor = FigmaHTMLFeatureExtractor(\n",
    "        semantic_model_name='all-MiniLM-L6-v2',\n",
    "        node_type_embedding_dim=50,\n",
    "        output_format='hdf5'\n",
    "    )\n",
    "    extractor.process_directory(input_dir, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
