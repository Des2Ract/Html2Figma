{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import warnings\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory monitoring function\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_gb = memory_info.rss / 1024**3\n",
    "    print(f\"Current memory usage: {memory_gb:.2f} GB\")\n",
    "    return memory_gb\n",
    "\n",
    "# Set memory limit and device\n",
    "MEMORY_LIMIT_GB = 7.5  # Leave some buffer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print_memory_usage()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Initialize GradScaler for mixed precision\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientFigmaDataset(Dataset):\n",
    "    \"\"\"Memory-efficient dataset for Figma node sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, sequence_ids, label_encoder, load_in_memory=True):\n",
    "        self.data_path = data_path\n",
    "        self.sequence_ids = sequence_ids\n",
    "        self.label_encoder = label_encoder\n",
    "        self.load_in_memory = load_in_memory\n",
    "        \n",
    "        if load_in_memory:\n",
    "            self._load_sequences()\n",
    "        else:\n",
    "            self.sequences = None\n",
    "    \n",
    "    def _load_sequences(self):\n",
    "        \"\"\"Load sequences into memory.\"\"\"\n",
    "        print(\"Loading sequences into memory...\")\n",
    "        \n",
    "        if self.data_path.endswith('.csv'):\n",
    "            df = pd.read_csv(self.data_path)\n",
    "            df['feature_vector'] = df['feature_vector'].apply(ast.literal_eval).apply(np.array)\n",
    "        elif self.data_path.endswith(('.hdf5', '.h5')):\n",
    "            with h5py.File(self.data_path, 'r') as f:\n",
    "                feature_vectors = f['feature_vector'][:]\n",
    "                tags = [s.decode('utf-8') for s in f['tag'][:]]\n",
    "                df = pd.DataFrame({\n",
    "                    'feature_vector': list(feature_vectors),\n",
    "                    'tag': tags\n",
    "                })\n",
    "        \n",
    "        sequences = []\n",
    "        current_sequence = []\n",
    "        for _, row in df.iterrows():\n",
    "            current_sequence.append(row)\n",
    "            if row['tag'] == 'E_WEBSITE':\n",
    "                sequences.append(current_sequence)\n",
    "                current_sequence = []\n",
    "        \n",
    "        if current_sequence:\n",
    "            sequences.append(current_sequence)\n",
    "        \n",
    "        self.sequences = {}\n",
    "        for i, seq_id in enumerate(self.sequence_ids):\n",
    "            if i < len(sequences):\n",
    "                seq = sequences[i]\n",
    "                feature_vectors = np.stack([row['feature_vector'] for row in seq])\n",
    "                tags = [row['tag'] for row in seq]\n",
    "                labels = self.label_encoder.transform(tags)\n",
    "                \n",
    "                self.sequences[seq_id] = {\n",
    "                    'features': torch.FloatTensor(feature_vectors),\n",
    "                    'labels': torch.LongTensor(labels)\n",
    "                }\n",
    "        \n",
    "        del df, sequences\n",
    "        gc.collect()\n",
    "        print(f\"Loaded {len(self.sequences)} sequences\")\n",
    "        print_memory_usage()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_id = self.sequence_ids[idx]\n",
    "        \n",
    "        if self.load_in_memory:\n",
    "            return {\n",
    "                'features': self.sequences[seq_id]['features'],\n",
    "                'labels': self.sequences[seq_id]['labels'],\n",
    "                'seq_id': seq_id\n",
    "            }\n",
    "        else:\n",
    "            raise NotImplementedError(\"Lazy loading not implemented in this example\")\n",
    "\n",
    "def memory_efficient_collate_fn(batch):\n",
    "    \"\"\"Memory-efficient collate function.\"\"\"\n",
    "    features = [item['features'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    seq_ids = [item['seq_id'] for item in batch]\n",
    "    \n",
    "    lengths = torch.tensor([len(f) for f in features])\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    MAX_SEQ_LENGTH = 500  # Adjust based on sequence length analysis\n",
    "    if max_len > MAX_SEQ_LENGTH:\n",
    "        max_len = MAX_SEQ_LENGTH\n",
    "        print(f\"Warning: Truncating sequences to {MAX_SEQ_LENGTH} tokens\")\n",
    "    \n",
    "    feature_dim = features[0].shape[1]\n",
    "    \n",
    "    padded_features = torch.zeros((len(batch), max_len, feature_dim), dtype=torch.float32)\n",
    "    padded_labels = torch.full((len(batch), max_len), -100, dtype=torch.long)\n",
    "    \n",
    "    for i, (f, l) in enumerate(zip(features, labels)):\n",
    "        seq_len = min(f.shape[0], max_len)\n",
    "        padded_features[i, :seq_len] = f[:seq_len]\n",
    "        padded_labels[i, :seq_len] = l[:seq_len]\n",
    "        lengths[i] = seq_len\n",
    "    \n",
    "    return {\n",
    "        'features': padded_features,\n",
    "        'labels': padded_labels,\n",
    "        'lengths': lengths,\n",
    "        'seq_ids': seq_ids\n",
    "    }\n",
    "\n",
    "print(\"Memory-efficient dataset classes defined\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedFigmaBLSTM(nn.Module):\n",
    "    \"\"\"Memory-optimized Bidirectional LSTM model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n",
    "        super(OptimizedFigmaBLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.attention = None  # Disabled for memory efficiency\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        if output.size(0) > 1:\n",
    "            output_reshaped = output.reshape(-1, output.size(-1))\n",
    "            output_reshaped = self.batch_norm(output_reshaped)\n",
    "            output = output_reshaped.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"Optimized model architecture defined\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(data_path, test_size=0.2, random_state=42, max_sequences=None):\n",
    "    \"\"\"Load and split data with memory optimization.\"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    \n",
    "    if data_path.endswith('.csv'):\n",
    "        chunk_size = 10000\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(data_path, chunksize=chunk_size):\n",
    "            chunk['feature_vector'] = chunk['feature_vector'].apply(ast.literal_eval).apply(np.array)\n",
    "            chunks.append(chunk)\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        del chunks\n",
    "        gc.collect()\n",
    "    elif data_path.endswith(('.hdf5', '.h5')):\n",
    "        with h5py.File(data_path, 'r') as f:\n",
    "            feature_vectors = f['feature_vector'][:]\n",
    "            tags = [s.decode('utf-8') for s in f['tag'][:]]\n",
    "            df = pd.DataFrame({\n",
    "                'feature_vector': list(feature_vectors),\n",
    "                'tag': tags\n",
    "            })\n",
    "    \n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    sequences = []\n",
    "    current_sequence = []\n",
    "    for _, row in df.iterrows():\n",
    "        current_sequence.append(row)\n",
    "        if row['tag'] == 'E_WEBSITE':\n",
    "            sequences.append(current_sequence)\n",
    "            current_sequence = []\n",
    "            if max_sequences and len(sequences) >= max_sequences:\n",
    "                break\n",
    "    \n",
    "    if current_sequence:\n",
    "        sequences.append(current_sequence)\n",
    "    \n",
    "    print(f\"Found {len(sequences)} sequences\")\n",
    "    \n",
    "    if max_sequences:\n",
    "        sequences = sequences[:max_sequences]\n",
    "        print(f\"Limited to {len(sequences)} sequences for memory efficiency\")\n",
    "    \n",
    "    all_tags = df['tag'].unique()\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_tags)\n",
    "    print(f\"Found {len(label_encoder.classes_)} unique tags\")\n",
    "    \n",
    "    sequence_ids = [f'seq_{i}' for i in range(len(sequences))]\n",
    "    \n",
    "    train_seq_ids, val_seq_ids = train_test_split(\n",
    "        sequence_ids, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Split: {len(train_seq_ids)} train, {len(val_seq_ids)} validation sequences\")\n",
    "    \n",
    "    first_seq = sequences[0]\n",
    "    input_dim = first_seq[0]['feature_vector'].shape[0]\n",
    "    print(f\"Input feature dimension: {input_dim}\")\n",
    "    \n",
    "    del df, sequences\n",
    "    gc.collect()\n",
    "    print_memory_usage()\n",
    "    \n",
    "    return train_seq_ids, val_seq_ids, label_encoder, len(label_encoder.classes_), input_dim\n",
    "\n",
    "DATA_PATH = \"figma_dataset_custom.h5\"  # Update this path\n",
    "MAX_SEQUENCES = 1000  # Adjust as needed\n",
    "\n",
    "train_seq_ids, val_seq_ids, label_encoder, num_classes, input_dim = load_and_split_data(\n",
    "    DATA_PATH, max_sequences=MAX_SEQUENCES\n",
    ")\n",
    "\n",
    "print(f\"Data loading completed. Memory usage:\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 10\n",
    "}\n",
    "\n",
    "print(\"Creating datasets and data loaders...\")\n",
    "\n",
    "train_dataset = MemoryEfficientFigmaDataset(DATA_PATH, train_seq_ids, label_encoder)\n",
    "val_dataset = MemoryEfficientFigmaDataset(DATA_PATH, val_seq_ids, label_encoder)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=memory_efficient_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=memory_efficient_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTesting data loader...\")\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    print(f\"Batch features shape: {test_batch['features'].shape}\")\n",
    "    print(f\"Batch labels shape: {test_batch['labels'].shape}\")\n",
    "    print(\"Data loader test successful!\")\n",
    "    del test_batch\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Data loader test failed: {e}\")\n",
    "    \n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building model...\")\n",
    "model = OptimizedFigmaBLSTM(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=model_config['hidden_dim'],\n",
    "    output_dim=num_classes,\n",
    "    num_layers=model_config['num_layers'],\n",
    "    dropout=model_config['dropout']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model built:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=model_config['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "output_dir = './models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Training setup completed\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_training_step(model, batch, criterion, optimizer, device):\n",
    "    \"\"\"Safe training step with mixed precision.\"\"\"\n",
    "    try:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        lengths = batch['lengths']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(features, lengths)\n",
    "            batch_size, seq_len, num_classes = outputs.size()\n",
    "            outputs = outputs.reshape(-1, num_classes)\n",
    "            labels = labels.reshape(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        del features, labels, outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"WARNING: Out of memory error occurred: {e}\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(model_config['epochs']):\n",
    "    mem_usage = print_memory_usage()\n",
    "    if mem_usage > MEMORY_LIMIT_GB:\n",
    "        print(f\"WARNING: Memory usage ({mem_usage:.2f} GB) approaching limit!\")\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    successful_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{model_config['epochs']} [Train]\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        loss = safe_training_step(model, batch, criterion, optimizer, device)\n",
    "        \n",
    "        if loss is not None:\n",
    "            train_loss += loss\n",
    "            successful_batches += 1\n",
    "            progress_bar.set_postfix({'loss': loss, 'mem': f'{print_memory_usage():.1f}GB'})\n",
    "        else:\n",
    "            print(f\"Skipped batch {batch_idx} due to memory error\")\n",
    "        \n",
    "        train_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    if successful_batches > 0:\n",
    "        avg_train_loss = train_loss / successful_batches\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} ({successful_batches}/{train_batches} successful batches)\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} - No successful training batches!\")\n",
    "        break\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def safe_validation_step(model, batch, criterion, device):\n",
    "    \"\"\"Safe validation step with mixed precision.\"\"\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(features, lengths)\n",
    "                batch_size, seq_len, num_classes = outputs.size()\n",
    "                outputs_flat = outputs.reshape(-1, num_classes)\n",
    "                labels_flat = labels.reshape(-1)\n",
    "                loss = criterion(outputs_flat, labels_flat)\n",
    "            \n",
    "            mask = (labels_flat != -100)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                predicted = torch.argmax(outputs_flat[mask], dim=1)\n",
    "                correct = (predicted == labels_flat[mask]).sum().item()\n",
    "                total = mask.sum().item()\n",
    "            \n",
    "            del features, labels, outputs, outputs_flat, labels_flat\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return loss.item(), correct, total\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"WARNING: Validation OOM error: {e}\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            return None, 0, 0\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    successful_val_batches = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{model_config['epochs']} [Val]\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        loss, correct, samples = safe_validation_step(model, batch, criterion, device)\n",
    "        \n",
    "        if loss is not None:\n",
    "            val_loss += loss\n",
    "            total_correct += correct\n",
    "            total_samples += samples\n",
    "            successful_val_batches += 1\n",
    "            \n",
    "            current_acc = (total_correct / total_samples * 100) if total_samples > 0 else 0\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss, \n",
    "                'acc': f'{current_acc:.1f}%',\n",
    "                'mem': f'{print_memory_usage():.1f}GB'\n",
    "            })\n",
    "        \n",
    "        val_batches += 1\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    if successful_val_batches > 0:\n",
    "        avg_val_loss = val_loss / successful_val_batches\n",
    "        val_accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_path = os.path.join(output_dir, \"figma_blstm_model.pt\")\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "                'label_encoder': label_encoder,\n",
    "                'model_config': model_config,\n",
    "                'input_dim': input_dim,\n",
    "                'num_classes': num_classes\n",
    "            }, model_path)\n",
    "            print(f\"  Best model saved to {model_path}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "    \n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} - No successful validation batches!\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 50)\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03848041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"Load the trained model safely.\"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    model_config_loaded = checkpoint['model_config']\n",
    "    input_dim = checkpoint['input_dim']\n",
    "    num_classes = checkpoint['num_classes']\n",
    "    label_encoder = checkpoint['label_encoder']\n",
    "    \n",
    "    model = OptimizedFigmaBLSTM(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=model_config_loaded['hidden_dim'],\n",
    "        output_dim=num_classes,\n",
    "        num_layers=model_config_loaded['num_layers'],\n",
    "        dropout=model_config_loaded['dropout']\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, label_encoder\n",
    "\n",
    "def predict_batch_safe(model, data_loader, label_encoder, device, max_batches=None):\n",
    "    \"\"\"Safe batch prediction with robust handling.\"\"\"\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    processed_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Predicting\")):\n",
    "            try:\n",
    "                features = batch['features'].to(device)\n",
    "                labels = batch['labels']\n",
    "                lengths = batch['lengths']\n",
    "                seq_ids = batch['seq_ids']\n",
    "                \n",
    "                outputs = model(features, lengths)\n",
    "                predictions = torch.argmax(outputs, dim=2)\n",
    "                \n",
    "                for i, seq_id in enumerate(seq_ids):\n",
    "                    seq_len = lengths[i].item()\n",
    "                    pred_indices = predictions[i, :seq_len].cpu().numpy()\n",
    "                    true_indices = labels[i, :seq_len].cpu().numpy()\n",
    "                    \n",
    "                    pred_tags = []\n",
    "                    for idx in pred_indices:\n",
    "                        if 0 <= idx < len(label_encoder.classes_):\n",
    "                            pred_tags.append(label_encoder.classes_[idx])\n",
    "                        else:\n",
    "                            pred_tags.append(\"UNKNOWN\")\n",
    "                    \n",
    "                    true_tags = [label_encoder.classes_[idx] if idx != -100 and 0 <= idx < len(label_encoder.classes_) else \"UNKNOWN\" for idx in true_indices]\n",
    "                    \n",
    "                    results[seq_id] = {\n",
    "                        'predicted_tags': pred_tags,\n",
    "                        'true_tags': true_tags\n",
    "                    }\n",
    "                \n",
    "                del features, outputs, predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                processed_batches += 1\n",
    "                if max_batches and processed_batches >= max_batches:\n",
    "                    break\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"Skipping batch {batch_idx} due to memory error\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            if batch_idx % 5 == 0:\n",
    "                gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "try:\n",
    "    model_path = os.path.join(output_dir, \"figma_blstm_model.pt\")\n",
    "    trained_model, trained_label_encoder = load_trained_model(model_path, device)\n",
    "    \n",
    "    print(\"Making predictions on validation data...\")\n",
    "    results = predict_batch_safe(\n",
    "        trained_model, val_loader, trained_label_encoder, device, max_batches=50\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated predictions for {len(results)} sequences\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"No trained model found. Please run the training cells first.\")\n",
    "    results = {}\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    results = {}\n",
    "\n",
    "def analyze_results(results, max_sequences_to_show=3):\n",
    "    \"\"\"Analyze and display results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    all_true_tags = []\n",
    "    all_pred_tags = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    sequence_accuracies = []\n",
    "    \n",
    "    for seq_id, seq_results in results.items():\n",
    "        true_tags = seq_results['true_tags']\n",
    "        pred_tags = seq_results['predicted_tags']\n",
    "        \n",
    "        all_true_tags.extend(true_tags)\n",
    "        all_pred_tags.extend(pred_tags)\n",
    "        \n",
    "        seq_correct = sum(1 for t, p in zip(true_tags, pred_tags) if t == p)\n",
    "        seq_total = len(true_tags)\n",
    "        seq_accuracy = seq_correct / seq_total if seq_total > 0 else 0\n",
    "        sequence_accuracies.append(seq_accuracy)\n",
    "        \n",
    "        correct_predictions += seq_correct\n",
    "        total_predictions += seq_total\n",
    "    \n",
    "    overall_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    avg_sequence_accuracy = sum(sequence_accuracies) / len(sequence_accuracies) if sequence_accuracies else 0\n",
    "    \n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "    print(f\"Average Sequence Accuracy: {avg_sequence_accuracy:.4f} ({avg_sequence_accuracy*100:.2f}%)\")\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Number of Sequences: {len(results)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"PER-CLASS ACCURACY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    class_counts = {}\n",
    "    class_correct = {}\n",
    "    \n",
    "    for true_tag, pred_tag in zip(all_true_tags, all_pred_tags):\n",
    "        class_counts[true_tag] = class_counts.get(true_tag, 0) + 1\n",
    "        if true_tag == pred_tag:\n",
    "            class_correct[true_tag] = class_correct.get(true_tag, 0) + 1\n",
    "    \n",
    "    for class_name in sorted(class_counts.keys()):\n",
    "        correct = class_correct.get(class_name, 0)\n",
    "        total = class_counts[class_name]\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"{class_name:>15}: {accuracy:.4f} ({accuracy*100:.2f}%) - {correct}/{total}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SAMPLE PREDICTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sample_count = 0\n",
    "    for seq_id, seq_results in results.items():\n",
    "        if sample_count >= max_sequences_to_show:\n",
    "            break\n",
    "            \n",
    "        true_tags = seq_results['true_tags']\n",
    "        pred_tags = seq_results['predicted_tags']\n",
    "        \n",
    "        seq_correct = sum(1 for t, p in zip(true_tags, pred_tags) if t == p)\n",
    "        seq_accuracy = seq_correct / len(true_tags) if true_tags else 0\n",
    "        \n",
    "        print(f\"\\nSequence {sample_count + 1} (ID: {seq_id}):\")\n",
    "        print(f\"Sequence Accuracy: {seq_accuracy:.4f} ({seq_accuracy*100:.2f}%)\")\n",
    "        print(f\"Correct/Total: {seq_correct}/{len(true_tags)}\")\n",
    "        \n",
    "        print(\"Predictions:\")\n",
    "        for j in range(min(10, len(true_tags))):\n",
    "            correct_indicator = \"✓\" if true_tags[j] == pred_tags[j] else \"✗\"\n",
    "            print(f\"  {correct_indicator} Node {j+1}: True={true_tags[j]}, Predicted={pred_tags[j]}\")\n",
    "        \n",
    "        if len(true_tags) > 10:\n",
    "            print(f\"  ... and {len(true_tags) - 10} more predictions\")\n",
    "            \n",
    "        sample_count += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL TRAINING AND EVALUATION COMPLETED!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "analyze_results(results, max_sequences_to_show=3)\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ec040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sequence_lengths(data_path, max_sequences=None):\n",
    "    \"\"\"Analyze sequence lengths to optimize MAX_SEQ_LENGTH.\"\"\"\n",
    "    print(f\"Analyzing sequence lengths from {data_path}...\")\n",
    "    \n",
    "    if data_path.endswith('.csv'):\n",
    "        df = pd.read_csv(data_path)\n",
    "        df['feature_vector'] = df['feature_vector'].apply(ast.literal_eval).apply(np.array)\n",
    "    elif data_path.endswith(('.hdf5', '.h5')):\n",
    "        with h5py.File(data_path, 'r') as f:\n",
    "            feature_vectors = f['feature_vector'][:]\n",
    "            tags = [s.decode('utf-8') for s in f['tag'][:]]\n",
    "            df = pd.DataFrame({\n",
    "                'feature_vector': list(feature_vectors),\n",
    "                'tag': tags\n",
    "            })\n",
    "    \n",
    "    sequences = []\n",
    "    current_sequence = []\n",
    "    for _, row in df.iterrows():\n",
    "        current_sequence.append(row)\n",
    "        if row['tag'] == 'E_WEBSITE':\n",
    "            sequences.append(current_sequence)\n",
    "            current_sequence = []\n",
    "            if max_sequences and len(sequences) >= max_sequences:\n",
    "                break\n",
    "    \n",
    "    if current_sequence:\n",
    "        sequences.append(current_sequence)\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in sequences]\n",
    "    \n",
    "    print(f\"Number of sequences: {len(sequence_lengths)}\")\n",
    "    print(f\"Min sequence length: {min(sequence_lengths)}\")\n",
    "    print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
    "    print(f\"Mean sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "    print(f\"Median sequence length: {np.median(sequence_lengths)}\")\n",
    "    print(f\"95th percentile: {np.percentile(sequence_lengths, 95)}\")\n",
    "    print(f\"99th percentile: {np.percentile(sequence_lengths, 99)}\")\n",
    "    \n",
    "    del df, sequences\n",
    "    gc.collect()\n",
    "    \n",
    "# Uncomment to analyze sequence lengths and adjust MAX_SEQ_LENGTH\n",
    "analyze_sequence_lengths(DATA_PATH, max_sequences=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
