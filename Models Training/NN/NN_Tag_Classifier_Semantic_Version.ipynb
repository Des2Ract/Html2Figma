{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cuDNN benchmarking for faster training\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_3748\\2337983856.py:5: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['tag'].str.contains(r'\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|A|HR|SEARCH|DETAILS|FIELDSET|SLOT|SVG)\\b', regex=True)]\n",
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_3748\\2337983856.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[col] = np.where(df[col].str.contains(pattern, regex=True, na=False), 'DIV', df[col])\n",
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_3748\\2337983856.py:46: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df['tag'].str.contains(r'\\b(P|IMG)\\b', regex=True)]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dataset and remove rows with '-' in the tag column\n",
    "df = pd.read_csv(\"figma_dataset.csv\")\n",
    "\n",
    "df = df[~df['tag'].str.contains(r'-', regex=True)]\n",
    "df = df[~df['tag'].str.contains(r'\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|A|HR|SEARCH|DETAILS|FIELDSET|SLOT|SVG)\\b', regex=True)]\n",
    "\n",
    "# Define the regex pattern for matching\n",
    "pattern = r'-|\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|SEARCH|DETAILS|FIELDSET|SLOT)\\b'\n",
    "\n",
    "# Apply the replacement conditionally\n",
    "for col in ['prev_sibling_html_tag', 'child_1_html_tag', 'child_2_html_tag']:\n",
    "    df[col] = np.where(df[col].str.contains(pattern, regex=True, na=False), 'DIV', df[col])\n",
    "\n",
    "# Define mapping for tag replacements\n",
    "tag_mapping = {\n",
    "    \"ARTICLE\": \"DIV\", \"DIV\": \"DIV\", \"FIGURE\": \"DIV\", \"FOOTER\": \"DIV\", \"HEADER\": \"DIV\", \"NAV\": \"DIV\", \"MAIN\": \"DIV\",\n",
    "    \"BODY\" : \"DIV\", \"FORM\" : \"DIV\", \"OL\" : \"DIV\", \"UL\" : \"DIV\", \"TABLE\": \"DIV\", \"THEAD\":\"DIV\" , \"TBODY\": \"DIV\", \"SECTION\" : \"DIV\",\n",
    "    \"H1\": \"P\", \"H2\": \"P\", \"H3\": \"P\", \"H4\": \"P\", \"H5\": \"P\", \"H6\": \"P\",\"SUP\": \"P\",\n",
    "    \"P\": \"P\", \"CAPTION\": \"P\", \"FIGCAPTION\": \"P\", \"B\": \"P\", \"EM\": \"P\", \"I\": \"P\", \"TD\": \"P\", \"TH\": \"P\", \"TR\": \"P\",\"PRE\":\"P\",\n",
    "    \"U\": \"P\", \"TIME\": \"P\", \"TXT\": \"P\", \"ABBR\": \"P\",\"SMALL\": \"P\",\"STRONG\": \"P\",\"SUMMARY\": \"P\",\"SPAN\": \"P\", \"LABEL\": \"P\",\"LI\":\"P\",\n",
    "    \"PICTURE\": \"IMG\" , \"VIDEO\": \"IMG\",\n",
    "    \"SELECT\": \"INPUT\",\"TEXTAREA\": \"INPUT\",\n",
    "    \"VECTOR\": \"SVG\"\n",
    "}\n",
    "\n",
    "# df.loc[(df[\"tag\"] == \"LABEL\") & ((df[\"type\"] == \"RECTANGLE\") | (df[\"type\"] == \"GROUP\")), \"tag\"] = \"DIV\"\n",
    "df.loc[(df[\"tag\"] == \"SPAN\") & ((df[\"type\"] == \"RECTANGLE\") | (df[\"type\"] == \"GROUP\")), \"tag\"] = \"DIV\"\n",
    "\n",
    "# Replace any value in children tag columns that contains '-' with 'DIV'\n",
    "children_cols = ['child_1_html_tag', 'child_2_html_tag']\n",
    "for col in children_cols:\n",
    "    df[col] = df[col].apply(lambda x: \"DIV\" if isinstance(x, str) and '-' in x else x)\n",
    "\n",
    "# Convert tag and parent_tag_html columns to uppercase\n",
    "df['tag'] = df['tag'].str.upper()\n",
    "df['prev_sibling_html_tag'] = df['prev_sibling_html_tag'].str.upper()\n",
    "df['child_1_html_tag'] = df['child_1_html_tag'].str.upper()\n",
    "df['child_2_html_tag'] = df['child_2_html_tag'].str.upper()\n",
    "\n",
    "# Apply mapping to 'tag' and 'parent_tag_html' columns\n",
    "df['tag'] = df['tag'].replace(tag_mapping)\n",
    "df['prev_sibling_html_tag'] = df['prev_sibling_html_tag'].replace(tag_mapping)\n",
    "df['child_1_html_tag'] = df['child_1_html_tag'].replace(tag_mapping)\n",
    "df['child_2_html_tag'] = df['child_2_html_tag'].replace(tag_mapping)\n",
    "\n",
    "df = df[~df['tag'].str.contains(r'\\b(P|IMG)\\b', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate features and target\n",
    "y = df[\"tag\"]\n",
    "X = df.drop(columns=[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimensions - Categorical: 33, Continuous: 13, Embedding: 384\n"
     ]
    }
   ],
   "source": [
    "# 3. Identify categorical and continuous columns\n",
    "categorical_cols = ['type','prev_sibling_html_tag','child_1_html_tag','child_2_html_tag']\n",
    "continuous_cols = [col for col in X.columns if col not in categorical_cols and col != 'nearest_text_semantic']\n",
    "\n",
    "# Handle nearest_text_semantic column safely\n",
    "if 'nearest_text_semantic' in X.columns and X['nearest_text_semantic'].notna().all():\n",
    "    X['nearest_text_semantic'] = X['nearest_text_semantic'].apply(eval)  # Ensure list format\n",
    "    embedding_dim = len(X['nearest_text_semantic'].iloc[0])\n",
    "    nearest_text_semantic_expanded = np.vstack(X['nearest_text_semantic'].values)\n",
    "else:\n",
    "    embedding_dim = 384\n",
    "    nearest_text_semantic_expanded = np.zeros((len(X), embedding_dim))  # Correct dimension for embeddings\n",
    "\n",
    "# Process categorical features with OneHotEncoder instead of LabelEncoder\n",
    "X[categorical_cols] = X[categorical_cols].astype(str).fillna('unknown')\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_cat_encoded = ohe.fit_transform(X[categorical_cols])\n",
    "joblib.dump(ohe, \"ohe_encoder.pkl\")\n",
    "\n",
    "# Better missing value handling with imputer\n",
    "imputer = SimpleImputer(strategy='mean')  # Changed to mean for numerical features\n",
    "X_continuous_imputed = imputer.fit_transform(X[continuous_cols])\n",
    "joblib.dump(imputer, \"imputer.pkl\")\n",
    "\n",
    "# Scale continuous features\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X_continuous_imputed)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Normalize embeddings separately to maintain consistency\n",
    "scaler_emb = StandardScaler()\n",
    "nearest_text_semantic_scaled = scaler_emb.fit_transform(nearest_text_semantic_expanded) if embedding_dim > 0 else nearest_text_semantic_expanded\n",
    "joblib.dump(scaler_emb, \"scaler_emb.pkl\")\n",
    "\n",
    "# Get dimensionality information for model architecture\n",
    "cat_dim = X_cat_encoded.shape[1]\n",
    "cont_dim = X_continuous_scaled.shape[1]\n",
    "emb_dim = nearest_text_semantic_scaled.shape[1]\n",
    "\n",
    "print(f\"Feature dimensions - Categorical: {cat_dim}, Continuous: {cont_dim}, Embedding: {emb_dim}\")\n",
    "\n",
    "# Combine one-hot encoded categorical features with scaled continuous features\n",
    "X_processed = np.concatenate([X_cat_encoded, X_continuous_scaled, nearest_text_semantic_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({2: 595664, 0: 24662, 3: 1815, 1: 2, 4: 2})\n"
     ]
    }
   ],
   "source": [
    "# 4. Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(y_encoded)\n",
    "print(f\"Class distribution: {class_counts}\")\n",
    "\n",
    "# Find classes with only 1 sample\n",
    "rare_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "\n",
    "# Duplicate rare class samples\n",
    "for cls in rare_classes:\n",
    "    idx = np.where(y_encoded == cls)[0][0]  # Get the index of the rare sample\n",
    "    original_class_name = label_encoder.inverse_transform([cls])[0]  # Convert back to original label\n",
    "    print(f\"Duplicating class '{original_class_name}' (only 1 sample present).\")\n",
    "\n",
    "    X_processed = np.vstack([X_processed, X_processed[idx]])  # Duplicate features\n",
    "    y_encoded = np.append(y_encoded, y_encoded[idx])  # Duplicate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Computing class weights...\n"
     ]
    }
   ],
   "source": [
    "# 5. Train/test split - remove stratification if there are classes with too few samples\n",
    "unique_counts = np.unique(y_encoded, return_counts=True)\n",
    "min_samples = min(unique_counts[1])\n",
    "\n",
    "if min_samples < 2:\n",
    "    print(f\"Warning: The least populated class has only {min_samples} sample(s). Removing stratification.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "# Move GPU setup earlier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloaders with more efficient settings\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512,  # Larger batch size\n",
    "    shuffle=True, \n",
    "    num_workers=8,   # Parallel loading\n",
    "    pin_memory=True,  # Faster data transfer to GPU\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "# Compute class weights\n",
    "print(\"Computing class weights...\")\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Total input size: 430, Output size (classes): 5\n",
      "Model created with 171,653 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 6. Define improved model architecture with separate pathways for different feature types\n",
    "class ImprovedTagClassifier(nn.Module):\n",
    "    def __init__(self, cat_dim, cont_dim, emb_dim, output_size, dropout_rate=0.3):\n",
    "        super(ImprovedTagClassifier, self).__init__()\n",
    "        # Embeddings pathway (special handling for semantic embeddings)\n",
    "        self.emb_fc = nn.Linear(emb_dim, 128)\n",
    "        self.emb_bn = nn.BatchNorm1d(128)\n",
    "        self.emb_dropout = nn.Dropout(dropout_rate * 0.5)  # Lower dropout for embeddings\n",
    "        \n",
    "        # Categorical features pathway\n",
    "        self.cat_fc = nn.Linear(cat_dim, 128)\n",
    "        self.cat_bn = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Continuous features pathway\n",
    "        self.cont_fc = nn.Linear(cont_dim, 64)\n",
    "        self.cont_bn = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Combined pathway\n",
    "        self.combined_dim = 128 + 128 + 64  # Combined dimensions\n",
    "        \n",
    "        # Main network after feature combination\n",
    "        self.fc1 = nn.Linear(self.combined_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into its component parts\n",
    "        cat_features = x[:, :cat_dim]\n",
    "        cont_features = x[:, cat_dim:cat_dim+cont_dim]\n",
    "        emb_features = x[:, cat_dim+cont_dim:]\n",
    "        \n",
    "        # Process embeddings pathway\n",
    "        emb = self.relu(self.emb_bn(self.emb_fc(emb_features)))\n",
    "        emb = self.emb_dropout(emb)\n",
    "        \n",
    "        # Process categorical features\n",
    "        cat = self.relu(self.cat_bn(self.cat_fc(cat_features)))\n",
    "        cat = self.dropout(cat)\n",
    "        \n",
    "        # Process continuous features\n",
    "        cont = self.relu(self.cont_bn(self.cont_fc(cont_features)))\n",
    "        cont = self.dropout(cont)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat((emb, cat, cont), dim=1)\n",
    "        \n",
    "        # Main network\n",
    "        x = self.dropout(self.relu(self.bn1(self.fc1(combined))))\n",
    "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
    "        logits = self.fc3(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model with correct dimensions\n",
    "print(\"Initializing model...\")\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(label_encoder.classes_)\n",
    "print(f\"Total input size: {input_size}, Output size (classes): {output_size}\")\n",
    "model = ImprovedTagClassifier(cat_dim, cont_dim, emb_dim, output_size).to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model created with {total_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define loss function and optimizer with improved learning rate and weight decay\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Improved learning rate scheduler with warmup\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Define number of steps\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * 200  # 200 epochs max\n",
    "\n",
    "# Use OneCycleLR for better convergence\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.005,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,  # Use 10% of iterations for warmup\n",
    "    div_factor=25,  # Initial lr = max_lr/div_factor\n",
    "    final_div_factor=1000,  # Final lr = max_lr/final_div_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Setup mixed precision training\n",
    "scaler = GradScaler(device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Saving best model with validation loss: 0.0567\n",
      "Epoch [5/200], Loss: 0.1406, Time: 8.72s\n",
      "Epoch [10/200], Loss: 0.1063, Time: 8.23s\n",
      "Validation Loss: 0.0671, Accuracy: 0.9913\n",
      "Epoch [15/200], Loss: 0.1320, Time: 8.27s\n",
      "Epoch [20/200], Loss: 0.0858, Time: 9.50s\n",
      "Epoch [25/200], Loss: 0.0808, Time: 9.04s\n",
      "Validation Loss: 0.0711, Accuracy: 0.9971\n",
      "Epoch [30/200], Loss: 0.0683, Time: 8.54s\n",
      "Epoch [35/200], Loss: 0.0870, Time: 10.08s\n",
      "Epoch [40/200], Loss: 0.1119, Time: 9.27s\n",
      "Validation Loss: 0.2726, Accuracy: 0.9982\n",
      "Epoch [45/200], Loss: 0.0531, Time: 9.19s\n",
      "Early stopping at epoch 46\n",
      "Total training time: 597.87 seconds\n"
     ]
    }
   ],
   "source": [
    "# 9. Training loop with timing and early stopping\n",
    "print(\"Starting training...\")\n",
    "best_loss = float('inf')\n",
    "patience = 15  # Increased patience\n",
    "counter = 0\n",
    "early_stop = False\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move batch to device\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use mixed precision for faster training\n",
    "        with torch.amp.autocast('cuda', enabled=device.type=='cuda'):\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Scale gradients and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Add gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Early stopping with validation evaluation\n",
    "    if epoch % 3 == 0:  # Validate every 3 epochs to save time\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=256):\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_batch_loss = criterion(outputs, batch_y)\n",
    "                val_loss += val_batch_loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        val_avg_loss = val_loss / len(DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=256))\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Validation Loss: {val_avg_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save based on validation loss instead of training loss\n",
    "        if val_avg_loss < best_loss:\n",
    "            best_loss = val_avg_loss\n",
    "            counter = 0\n",
    "            print(f\"Saving best model with validation loss: {val_avg_loss:.4f}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_avg_loss,\n",
    "                'accuracy': val_accuracy\n",
    "            }, \"best_tag_classifier.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                early_stop = True\n",
    "    \n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "# Save the final model\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'final_loss': avg_loss\n",
    "}, \"final_tag_classifier.pth\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_3748\\1841807080.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"best_tag_classifier.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9813\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score        support\n",
      "BUTTON         0.706566  0.986013  0.823221    4933.000000\n",
      "DIV            0.999478  0.981080  0.990194  119133.000000\n",
      "INPUT          0.591736  0.986226  0.739669     363.000000\n",
      "accuracy       0.981291  0.981291  0.981291       0.981291\n",
      "macro avg      0.765927  0.984439  0.851028  124429.000000\n",
      "weighted avg   0.986676  0.981291  0.982843  124429.000000\n",
      "\n",
      "Model Confidence Analysis:\n",
      "Mean prediction confidence: 0.9895\n",
      "Mean confidence for correct predictions: 0.9928\n",
      "Mean confidence for incorrect predictions: 0.8167\n",
      "\n",
      "Model information saved to model_info.json\n",
      "\n",
      "Analyzing feature importance...\n",
      "\n",
      "Top 50 most important features:\n",
      "width: 0.1570\n",
      "border_radius: 0.1554\n",
      "aspect_ratio: 0.1500\n",
      "sibling_count: 0.1453\n",
      "has_background_color: 0.1452\n",
      "child_1_percentage_of_parent: 0.1438\n",
      "chars_count_to_end: 0.1358\n",
      "height: 0.1343\n",
      "num_children_to_end: 0.1332\n",
      "child_2_percentage_of_parent: 0.1328\n",
      "text_length: 0.1237\n",
      "nearest_text_node_dist: 0.1188\n",
      "distinct_background: 0.1148\n",
      "prev_sibling_html_tag_IMG: 0.0995\n",
      "type_ELLIPSE: 0.0978\n",
      "child_1_html_tag_BUTTON: 0.0973\n",
      "prev_sibling_html_tag_BUTTON: 0.0954\n",
      "prev_sibling_html_tag_DIALOG: 0.0939\n",
      "prev_sibling_html_tag_DIV: 0.0935\n",
      "prev_sibling_html_tag_INPUT: 0.0934\n",
      "child_2_html_tag_INPUT: 0.0930\n",
      "child_2_html_tag_SVG: 0.0915\n",
      "child_2_html_tag_IMG: 0.0912\n",
      "child_1_html_tag_A: 0.0910\n",
      "child_1_html_tag_HR: 0.0909\n",
      "child_2_html_tag_BUTTON: 0.0906\n",
      "prev_sibling_html_tag_SVG: 0.0905\n",
      "child_1_html_tag_INPUT: 0.0904\n",
      "type_RECTANGLE: 0.0903\n",
      "child_1_html_tag_P: 0.0888\n",
      "child_1_html_tag_DIALOG: 0.0882\n",
      "prev_sibling_html_tag_HR: 0.0881\n",
      "child_1_html_tag_nan: 0.0873\n",
      "child_1_html_tag_IMG: 0.0865\n",
      "type_GROUP: 0.0864\n",
      "child_2_html_tag_DIV: 0.0859\n",
      "child_2_html_tag_A: 0.0855\n",
      "prev_sibling_html_tag_nan: 0.0852\n",
      "prev_sibling_html_tag_A: 0.0844\n",
      "type_FRAME: 0.0839\n",
      "child_2_html_tag_HR: 0.0836\n",
      "child_1_html_tag_SVG: 0.0821\n",
      "child_2_html_tag_nan: 0.0817\n",
      "prev_sibling_html_tag_P: 0.0803\n",
      "child_2_html_tag_P: 0.0794\n",
      "child_1_html_tag_DIV: 0.0768\n",
      "embedding_319: 0.0328\n",
      "embedding_252: 0.0325\n",
      "embedding_281: 0.0323\n",
      "embedding_156: 0.0318\n",
      "\n",
      "Feature importances saved to feature_importances.json\n",
      "\n",
      "Prediction function saved to predict_function.pkl\n",
      "Model training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluation on the test set\n",
    "print(\"Evaluating model...\")\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"best_tag_classifier.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Process test data in batches for memory efficiency\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "        all_probabilities.append(probabilities.cpu().numpy())\n",
    "\n",
    "y_pred = np.array(all_predictions)\n",
    "y_test_np = np.array(all_labels)\n",
    "all_probabilities = np.vstack(all_probabilities)\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_test_np, \n",
    "    y_pred,\n",
    "    labels=np.unique(y_test_np),\n",
    "    target_names=label_encoder.inverse_transform(np.unique(y_test_np)),\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for better formatting\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(report_df)\n",
    "\n",
    "# Calculate confidence metrics\n",
    "confidence_scores = np.max(all_probabilities, axis=1)\n",
    "mean_confidence = np.mean(confidence_scores)\n",
    "mean_confidence_correct = np.mean(confidence_scores[y_pred == y_test_np])\n",
    "mean_confidence_incorrect = np.mean(confidence_scores[y_pred != y_test_np]) if np.any(y_pred != y_test_np) else 0\n",
    "\n",
    "print(f\"\\nModel Confidence Analysis:\")\n",
    "print(f\"Mean prediction confidence: {mean_confidence:.4f}\")\n",
    "print(f\"Mean confidence for correct predictions: {mean_confidence_correct:.4f}\")\n",
    "print(f\"Mean confidence for incorrect predictions: {mean_confidence_incorrect:.4f}\")\n",
    "\n",
    "# Save model architecture and parameters info for future reference\n",
    "model_info = {\n",
    "    'input_size': input_size,\n",
    "    'categorical_dim': cat_dim,\n",
    "    'continuous_dim': cont_dim,\n",
    "    'embedding_dim': emb_dim,\n",
    "    'output_size': output_size,\n",
    "    'model_params': total_params,\n",
    "    'accuracy': accuracy,\n",
    "    'best_val_loss': best_loss\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f)\n",
    "\n",
    "print(\"\\nModel information saved to model_info.json\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "# Combine importance from all pathways\n",
    "importances = {}\n",
    "\n",
    "# Process embedding pathway\n",
    "with torch.no_grad():\n",
    "    emb_weights = model.emb_fc.weight.cpu().numpy()\n",
    "    emb_importance = np.abs(emb_weights).mean(axis=0)\n",
    "    \n",
    "    # Process categorical pathway\n",
    "    cat_weights = model.cat_fc.weight.cpu().numpy()\n",
    "    cat_importance = np.abs(cat_weights).mean(axis=0)\n",
    "    \n",
    "    # Process continuous pathway\n",
    "    cont_weights = model.cont_fc.weight.cpu().numpy()\n",
    "    cont_importance = np.abs(cont_weights).mean(axis=0)\n",
    "    \n",
    "    # Get categorical feature names (from one-hot encoder)\n",
    "    cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Map weights to feature names\n",
    "    for i, feat in enumerate(cat_feature_names):\n",
    "        importances[feat] = float(cat_importance[i])\n",
    "    \n",
    "    for i, feat in enumerate(continuous_cols):\n",
    "        importances[feat] = float(cont_importance[i])\n",
    "    \n",
    "    # For embeddings, create generic names\n",
    "    for i in range(emb_dim):\n",
    "        importances[f'embedding_{i}'] = float(emb_importance[i])\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 50 most important features:\")\n",
    "    for feature, imp in sorted_importances[:50]:\n",
    "        print(f\"{feature}: {imp:.4f}\")\n",
    "\n",
    "# Save feature importances\n",
    "with open('feature_importances.json', 'w') as f:\n",
    "    json.dump(dict(sorted_importances), f)\n",
    "\n",
    "print(\"\\nFeature importances saved to feature_importances.json\")\n",
    "\n",
    "# Function to make predictions on new data\n",
    "def predict_tag(new_data, model, label_encoder, ohe, imputer, scaler, scaler_emb):\n",
    "    # Prepare the data\n",
    "    categorical_data = new_data[categorical_cols].astype(str).fillna('unknown')\n",
    "    continuous_data = new_data[continuous_cols]\n",
    "    \n",
    "    # Process categorical features\n",
    "    cat_encoded = ohe.transform(categorical_data)\n",
    "    \n",
    "    # Process continuous features\n",
    "    cont_imputed = imputer.transform(continuous_data)\n",
    "    cont_scaled = scaler.transform(cont_imputed)\n",
    "    \n",
    "    # Process embeddings\n",
    "    if 'nearest_text_semantic' in new_data.columns:\n",
    "        emb_data = np.vstack(new_data['nearest_text_semantic'].apply(eval).values)\n",
    "        emb_scaled = scaler_emb.transform(emb_data)\n",
    "    else:\n",
    "        emb_scaled = np.zeros((len(new_data), embedding_dim))\n",
    "    \n",
    "    # Combine features\n",
    "    X_processed = np.concatenate([cat_encoded, cont_scaled, emb_scaled], axis=1)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    X_tensor = torch.tensor(X_processed, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert to original labels\n",
    "    predicted_tags = label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "    prediction_probs = probabilities.cpu().numpy()\n",
    "    \n",
    "    return predicted_tags, prediction_probs\n",
    "\n",
    "# Save the prediction function\n",
    "import dill\n",
    "with open('predict_function.pkl', 'wb') as f:\n",
    "    dill.dump(predict_tag, f)\n",
    "\n",
    "print(\"\\nPrediction function saved to predict_function.pkl\")\n",
    "print(\"Model training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
