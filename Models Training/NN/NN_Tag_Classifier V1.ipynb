{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory (where utils.py is located) to sys.path to ensure utils module is found\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), \"../../Utils/\"))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "from model_utils import ImprovedTagClassifier  # Import our model from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cuDNN benchmarking for faster training\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset and remove rows with '-' in the tag column\n",
    "df = pd.read_csv(\"../Dataset/figma_dataset.csv\")\n",
    "\n",
    "# Remove rows with weird tags (like '-' or specific unwanted ones)\n",
    "df = df[~df['tag'].str.contains(r'[-:]', regex=True)]\n",
    "df = df[~df['tag'].str.contains(r'\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|A|HR|SEARCH|DETAILS|FIELDSET|SLOT|SVG|AD|ADSLOT|AUDIO|BLINK|BOLD|COL|COLGROUP|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MARK|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|OPTION|PATH|PROGRESS|STRIKE|SWAL|TEXT|TFOOT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1|UNK)\\b', regex=True)]\n",
    "\n",
    "# Pattern to spot unwanted tags in other columns\n",
    "pattern = r'[-:]|\\b(CNX|ADDRESS|ASIDE|CANVAS|CITE|DD|DL|DT|ICON|S|VECTOR|DEL|LEGEND|BDI|LOGO|OBJECT|OPTGROUP|CENTER|CODE|BLOCKQUOTE|FRONT|Q|IFRAME|SEARCH|DETAILS|FIELDSET|SLOT|AD|ADSLOT|AUDIO|BLINK|BOLD|COL|COLGROUP|COMMENTS|DATA|DIALOG|EMBED|EMPHASIS|FONT|H7|HGROUP|INS|INTERACTION|ITALIC|ITEMTEMPLATE|MARK|MATH|MENU|MI|MN|MO|MROW|MSUP|NOBR|OFFER|OPTION|PATH|PROGRESS|STRIKE|SWAL|TEXT|TFOOT|TITLE|TT|VAR|VEV|W|WBR|COUNTRY|ESI:INCLUDE|HTTPS:|LOGIN|NOCSRIPT|PERSONAL|STONG|CONTENT|DELIVERY|LEFT|MSUBSUP|KBD|ROOT|PARAGRAPH|BE|AI2SVELTEWRAP|BANNER|PHOTO1|UNK)\\b'\n",
    "\n",
    "# Replace unwanted tags with 'DIV' in related columns\n",
    "for col in ['prev_sibling_html_tag', 'child_1_html_tag', 'child_2_html_tag']:\n",
    "    df[col] = np.where(df[col].str.contains(pattern, regex=True, na=False), 'DIV', df[col])\n",
    "\n",
    "# Map similar tags to simpler ones (e.g., 'H1' to 'P')\n",
    "tag_mapping = {\n",
    "    \"ARTICLE\": \"DIV\", \"DIV\": \"DIV\", \"FIGURE\": \"DIV\", \"FOOTER\": \"DIV\", \"HEADER\": \"DIV\", \"NAV\": \"DIV\", \"MAIN\": \"DIV\",\n",
    "    \"BODY\": \"DIV\", \"FORM\": \"DIV\", \"OL\": \"DIV\", \"UL\": \"DIV\", \"TABLE\": \"DIV\", \"THEAD\": \"DIV\", \"TBODY\": \"DIV\", \"SECTION\": \"DIV\",\n",
    "    \"H1\": \"P\", \"H2\": \"P\", \"H3\": \"P\", \"H4\": \"P\", \"H5\": \"P\", \"H6\": \"P\", \"SUP\": \"P\", \"SUB\": \"P\", \"BIG\": \"P\",\n",
    "    \"P\": \"P\", \"CAPTION\": \"P\", \"FIGCAPTION\": \"P\", \"B\": \"P\", \"EM\": \"P\", \"I\": \"P\", \"TD\": \"P\", \"TH\": \"P\", \"TR\": \"P\", \"PRE\": \"P\",\n",
    "    \"U\": \"P\", \"TIME\": \"P\", \"TXT\": \"P\", \"ABBR\": \"P\", \"SMALL\": \"P\", \"STRONG\": \"P\", \"SUMMARY\": \"P\", \"SPAN\": \"P\", \"LABEL\": \"P\", \"LI\": \"P\",\n",
    "    \"PICTURE\": \"IMG\", \"VIDEO\": \"IMG\",\n",
    "    \"SELECT\": \"INPUT\", \"TEXTAREA\": \"INPUT\",\n",
    "    \"VECTOR\": \"SVG\", \"ICON\": \"SVG\",\n",
    "    \"UNK\": \"CONTAINER\"\n",
    "}\n",
    "\n",
    "# Fix some specific tags based on conditions\n",
    "df.loc[(df[\"tag\"] == \"SPAN\") & ((df[\"type\"] == \"RECTANGLE\") | (df[\"type\"] == \"GROUP\")), \"tag\"] = \"DIV\"\n",
    "\n",
    "# Replace '-' in child tags with 'DIV'\n",
    "children_cols = ['child_1_html_tag', 'child_2_html_tag']\n",
    "for col in children_cols:\n",
    "    df[col] = df[col].apply(lambda x: \"DIV\" if isinstance(x, str) and '-' in x else x)\n",
    "\n",
    "# Make all tags uppercase for consistency\n",
    "df['tag'] = df['tag'].str.upper()\n",
    "df['prev_sibling_html_tag'] = df['prev_sibling_html_tag'].str.upper()\n",
    "df['child_1_html_tag'] = df['child_1_html_tag'].str.upper()\n",
    "df['child_2_html_tag'] = df['child_2_html_tag'].str.upper()\n",
    "\n",
    "# Apply the tag mapping to all relevant columns\n",
    "df['tag'] = df['tag'].replace(tag_mapping)\n",
    "df['prev_sibling_html_tag'] = df['prev_sibling_html_tag'].replace(tag_mapping)\n",
    "df['child_1_html_tag'] = df['child_1_html_tag'].replace(tag_mapping)\n",
    "df['child_2_html_tag'] = df['child_2_html_tag'].replace(tag_mapping)\n",
    "\n",
    "# Remove some tags we donâ€™t want\n",
    "df = df[~df['tag'].str.contains(r'\\b(P|IMG|CHECKBOX|RADIO)\\b', regex=True)]\n",
    "\n",
    "# Show some info about the cleaned data\n",
    "print(df['tag'].unique())\n",
    "print(df.count())\n",
    "print(\"Unique Child 1:\", df['child_1_html_tag'].unique())\n",
    "print(\"Unique Child 2:\", df['child_2_html_tag'].unique())\n",
    "print(\"Prev HTML tag:\", df['prev_sibling_html_tag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "y = df[\"tag\"]\n",
    "X = df.drop(columns=[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and continuous columns\n",
    "categorical_cols = ['type','prev_sibling_html_tag','child_1_html_tag','child_2_html_tag']\n",
    "continuous_cols = [col for col in X.columns if col not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process categorical features with OneHotEncoder\n",
    "X[categorical_cols] = X[categorical_cols].astype(str).fillna('unknown')\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_cat_encoded = ohe.fit_transform(X[categorical_cols])\n",
    "joblib.dump(ohe, \"../models/ohe_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better missing value handling with imputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_continuous_imputed = imputer.fit_transform(X[continuous_cols])\n",
    "joblib.dump(imputer, \"../models/imputer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous features\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X_continuous_imputed)\n",
    "joblib.dump(scaler, \"../models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine one-hot encoded categorical features with scaled continuous features\n",
    "X_processed = np.concatenate([X_cat_encoded, X_continuous_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "joblib.dump(label_encoder, \"../models/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(y_encoded)\n",
    "\n",
    "# Find classes with only 1 sample\n",
    "rare_classes = [cls for cls, count in class_counts.items() if count < 2]\n",
    "\n",
    "# Duplicate rare class samples\n",
    "for cls in rare_classes:\n",
    "    idx = np.where(y_encoded == cls)[0][0]  # Get the index of the rare sample\n",
    "    original_class_name = label_encoder.inverse_transform([cls])[0]  # Convert back to original label\n",
    "    print(f\"Duplicating class '{original_class_name}' (only 1 sample present).\")\n",
    "\n",
    "    X_processed = np.vstack([X_processed, X_processed[idx]])  # Duplicate features\n",
    "    y_encoded = np.append(y_encoded, y_encoded[idx])  # Duplicate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split => remove stratification if there are classes with too few samples\n",
    "unique_counts = np.unique(y_encoded, return_counts=True)\n",
    "min_samples = min(unique_counts[1])\n",
    "\n",
    "if min_samples < 2:\n",
    "    print(f\"Warning: The least populated class has only {min_samples} sample(s). Removing stratification.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512,  # Larger batch size\n",
    "    shuffle=True, \n",
    "    num_workers=8,   # Parallel loading\n",
    "    pin_memory=True,  # Faster data transfer to GPU\n",
    "    prefetch_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "print(\"Computing class weights...\")\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "for c in np.unique(y_train):\n",
    "    print(label_encoder.inverse_transform([c])[0])\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(label_encoder.classes_)\n",
    "model = ImprovedTagClassifier(input_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Setup mixed precision training\n",
    "scaler = GradScaler(device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Training loop with timing and early stopping\n",
    "print(\"Starting training...\")\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "early_stop = False\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move batch to device\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use mixed precision for faster training\n",
    "        with torch.amp.autocast('cuda', enabled=device.type=='cuda'):\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Scale gradients and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"../models/best_tag_classifier.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "    \n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"../models/tag_classifier.pth\")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Evaluation on the test set\n",
    "print(\"Evaluating model...\")\n",
    "model.load_state_dict(torch.load(\"../models/best_tag_classifier.pth\"))  # Load best model\n",
    "model.eval()\n",
    "\n",
    "# Process test data in batches for memory efficiency\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "y_pred = np.array(all_predictions)\n",
    "y_test_np = np.array(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test_np, \n",
    "    y_pred,\n",
    "    labels=np.unique(y_test_np),\n",
    "    target_names=label_encoder.inverse_transform(np.unique(y_test_np))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importances from the model\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "with torch.no_grad():\n",
    "    weights = model.fc1.weight.cpu().numpy()\n",
    "    importance = np.abs(weights).mean(axis=0)\n",
    "    \n",
    "    # Get feature names (both categorical encoded and continuous)\n",
    "    cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "    all_feature_names = np.concatenate([cat_feature_names, np.array(continuous_cols)])\n",
    "    \n",
    "    feature_importance = list(zip(all_feature_names, importance))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 100 most important features:\")\n",
    "    for feature, imp in feature_importance[:100]:\n",
    "        print(f\"{feature}: {imp:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
