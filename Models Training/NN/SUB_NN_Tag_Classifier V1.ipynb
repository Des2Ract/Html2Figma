{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting multi-level tag classification training...\n",
      "Loading and cleaning data...\n",
      "\n",
      "============================================================\n",
      "Training DIV sub-classifier\n",
      "============================================================\n",
      "\n",
      "=== Preparing data for DIV sub-classification ===\n",
      "Subtags: ['DIV', 'FOOTER', 'NAVBAR', 'LIST', 'CARD']\n",
      "Total samples: 2098\n",
      "Distribution: \n",
      "tag\n",
      "DIV       1970\n",
      "CARD        54\n",
      "LIST        39\n",
      "NAVBAR      23\n",
      "FOOTER      12\n",
      "Name: count, dtype: int64\n",
      "Augmenting class 'FOOTER' with 3 additional samples\n",
      "\n",
      "=== Training DIV sub-classifier ===\n",
      "Training set size: 1517\n",
      "SandardScaler set size: 268\n",
      "Test set size: 316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80] - Train Loss: 7.2919, Val Loss: 1.0290, Val Accuracy: 0.5299\n",
      "Epoch [2/80] - Train Loss: 4.0481, Val Loss: 0.5282, Val Accuracy: 0.5970\n",
      "Epoch [3/80] - Train Loss: 2.5149, Val Loss: 0.2100, Val Accuracy: 0.4701\n",
      "Epoch [4/80] - Train Loss: 2.6815, Val Loss: 0.2224, Val Accuracy: 0.3918\n",
      "Epoch [5/80] - Train Loss: 1.7974, Val Loss: 0.2520, Val Accuracy: 0.3284\n",
      "Epoch [6/80] - Train Loss: 1.1615, Val Loss: 0.2208, Val Accuracy: 0.3582\n",
      "Epoch [7/80] - Train Loss: 1.8507, Val Loss: 0.1706, Val Accuracy: 0.4179\n",
      "Epoch [8/80] - Train Loss: 1.3387, Val Loss: 0.1560, Val Accuracy: 0.5075\n",
      "Epoch [9/80] - Train Loss: 0.8938, Val Loss: 0.1340, Val Accuracy: 0.5448\n",
      "Epoch [10/80] - Train Loss: 0.8741, Val Loss: 0.1269, Val Accuracy: 0.5970\n",
      "Epoch [11/80] - Train Loss: 0.8273, Val Loss: 0.1184, Val Accuracy: 0.6306\n",
      "Epoch [12/80] - Train Loss: 1.0643, Val Loss: 0.1290, Val Accuracy: 0.6381\n",
      "Epoch [13/80] - Train Loss: 1.0421, Val Loss: 0.1274, Val Accuracy: 0.6866\n",
      "Epoch [14/80] - Train Loss: 0.7879, Val Loss: 0.1108, Val Accuracy: 0.7388\n",
      "Epoch [15/80] - Train Loss: 0.5791, Val Loss: 0.1030, Val Accuracy: 0.7724\n",
      "Epoch [16/80] - Train Loss: 0.5280, Val Loss: 0.0976, Val Accuracy: 0.7575\n",
      "Epoch [17/80] - Train Loss: 0.4826, Val Loss: 0.0905, Val Accuracy: 0.8060\n",
      "Epoch [18/80] - Train Loss: 0.5086, Val Loss: 0.1041, Val Accuracy: 0.7985\n",
      "Epoch [19/80] - Train Loss: 0.4201, Val Loss: 0.1142, Val Accuracy: 0.8022\n",
      "Epoch [20/80] - Train Loss: 0.5191, Val Loss: 0.1079, Val Accuracy: 0.8097\n",
      "Epoch [21/80] - Train Loss: 0.4519, Val Loss: 0.1056, Val Accuracy: 0.8097\n",
      "Epoch [22/80] - Train Loss: 0.3445, Val Loss: 0.1015, Val Accuracy: 0.8806\n",
      "Epoch [23/80] - Train Loss: 0.5312, Val Loss: 0.0795, Val Accuracy: 0.8769\n",
      "Epoch [24/80] - Train Loss: 0.4327, Val Loss: 0.0717, Val Accuracy: 0.8806\n",
      "Epoch [25/80] - Train Loss: 0.1868, Val Loss: 0.0783, Val Accuracy: 0.8881\n",
      "Epoch [26/80] - Train Loss: 0.3801, Val Loss: 0.1095, Val Accuracy: 0.8918\n",
      "Epoch [27/80] - Train Loss: 0.4962, Val Loss: 0.1147, Val Accuracy: 0.8470\n",
      "Epoch [28/80] - Train Loss: 0.2740, Val Loss: 0.1055, Val Accuracy: 0.8582\n",
      "Epoch [29/80] - Train Loss: 0.3816, Val Loss: 0.0865, Val Accuracy: 0.8545\n",
      "Epoch [30/80] - Train Loss: 0.1821, Val Loss: 0.0734, Val Accuracy: 0.8284\n",
      "Epoch [31/80] - Train Loss: 0.3768, Val Loss: 0.0771, Val Accuracy: 0.7948\n",
      "Epoch [32/80] - Train Loss: 0.2657, Val Loss: 0.0751, Val Accuracy: 0.8022\n",
      "Epoch [33/80] - Train Loss: 0.4528, Val Loss: 0.0611, Val Accuracy: 0.8209\n",
      "Epoch [34/80] - Train Loss: 0.1796, Val Loss: 0.0535, Val Accuracy: 0.8246\n",
      "Epoch [35/80] - Train Loss: 0.2316, Val Loss: 0.0539, Val Accuracy: 0.8246\n",
      "Epoch [36/80] - Train Loss: 0.1273, Val Loss: 0.0579, Val Accuracy: 0.8433\n",
      "Epoch [37/80] - Train Loss: 0.2929, Val Loss: 0.0447, Val Accuracy: 0.8507\n",
      "Epoch [38/80] - Train Loss: 0.1577, Val Loss: 0.0444, Val Accuracy: 0.8433\n",
      "Epoch [39/80] - Train Loss: 0.1737, Val Loss: 0.0469, Val Accuracy: 0.8284\n",
      "Epoch [40/80] - Train Loss: 0.1230, Val Loss: 0.0528, Val Accuracy: 0.8396\n",
      "Epoch [41/80] - Train Loss: 0.1334, Val Loss: 0.0536, Val Accuracy: 0.8358\n",
      "Epoch [42/80] - Train Loss: 0.1401, Val Loss: 0.0472, Val Accuracy: 0.8433\n",
      "Epoch [43/80] - Train Loss: 0.1281, Val Loss: 0.0460, Val Accuracy: 0.8433\n",
      "Epoch [44/80] - Train Loss: 0.2358, Val Loss: 0.0348, Val Accuracy: 0.8284\n",
      "Epoch [45/80] - Train Loss: 0.2656, Val Loss: 0.0312, Val Accuracy: 0.8358\n",
      "Epoch [46/80] - Train Loss: 0.2064, Val Loss: 0.0350, Val Accuracy: 0.8619\n",
      "Epoch [47/80] - Train Loss: 0.2117, Val Loss: 0.0399, Val Accuracy: 0.8619\n",
      "Epoch [48/80] - Train Loss: 0.1288, Val Loss: 0.0573, Val Accuracy: 0.8731\n",
      "Epoch [49/80] - Train Loss: 0.0655, Val Loss: 0.0584, Val Accuracy: 0.8806\n",
      "Epoch [50/80] - Train Loss: 0.1152, Val Loss: 0.0645, Val Accuracy: 0.8843\n",
      "Epoch [51/80] - Train Loss: 0.1789, Val Loss: 0.0603, Val Accuracy: 0.8731\n",
      "Epoch [52/80] - Train Loss: 0.1261, Val Loss: 0.0597, Val Accuracy: 0.8843\n",
      "Epoch [53/80] - Train Loss: 0.2236, Val Loss: 0.0539, Val Accuracy: 0.8731\n",
      "Epoch [54/80] - Train Loss: 0.0757, Val Loss: 0.0471, Val Accuracy: 0.8843\n",
      "Epoch [55/80] - Train Loss: 0.0871, Val Loss: 0.0448, Val Accuracy: 0.8806\n",
      "Early stopping triggered after 55 epochs\n",
      "\n",
      "DIV Model Test Accuracy: 0.8734\n",
      "\n",
      "DIV Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CARD       0.20      1.00      0.33         8\n",
      "         DIV       1.00      0.86      0.93       296\n",
      "      FOOTER       0.25      1.00      0.40         2\n",
      "        LIST       1.00      1.00      1.00         6\n",
      "      NAVBAR       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.87       316\n",
      "   macro avg       0.62      0.97      0.69       316\n",
      "weighted avg       0.97      0.87      0.91       316\n",
      "\n",
      "Saved DIV model to ../models/sub_classifiers/div_classifier.pth\n",
      "\n",
      "============================================================\n",
      "Training P sub-classifier\n",
      "============================================================\n",
      "\n",
      "=== Preparing data for P sub-classification ===\n",
      "Subtags: ['P', 'LABEL', 'LI', 'A']\n",
      "Total samples: 1252\n",
      "Distribution: \n",
      "tag\n",
      "P        798\n",
      "LI       186\n",
      "A        134\n",
      "LABEL    134\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Training P sub-classifier ===\n",
      "Training set size: 904\n",
      "SandardScaler set size: 160\n",
      "Test set size: 188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80] - Train Loss: 6.6669, Val Loss: 1.3201, Val Accuracy: 0.4313\n",
      "Epoch [2/80] - Train Loss: 4.7864, Val Loss: 0.7754, Val Accuracy: 0.4562\n",
      "Epoch [3/80] - Train Loss: 2.3131, Val Loss: 0.5958, Val Accuracy: 0.5188\n",
      "Epoch [4/80] - Train Loss: 1.9708, Val Loss: 0.5887, Val Accuracy: 0.5687\n",
      "Epoch [5/80] - Train Loss: 1.6785, Val Loss: 0.6509, Val Accuracy: 0.6062\n",
      "Epoch [6/80] - Train Loss: 1.7827, Val Loss: 0.6145, Val Accuracy: 0.6438\n",
      "Epoch [7/80] - Train Loss: 1.4199, Val Loss: 0.5894, Val Accuracy: 0.7438\n",
      "Epoch [8/80] - Train Loss: 1.5362, Val Loss: 0.6452, Val Accuracy: 0.7625\n",
      "Epoch [9/80] - Train Loss: 1.1072, Val Loss: 0.6708, Val Accuracy: 0.7562\n",
      "Epoch [10/80] - Train Loss: 1.1506, Val Loss: 0.7494, Val Accuracy: 0.7625\n",
      "Epoch [11/80] - Train Loss: 1.2501, Val Loss: 0.7688, Val Accuracy: 0.7500\n",
      "Epoch [12/80] - Train Loss: 0.7622, Val Loss: 0.7397, Val Accuracy: 0.7500\n",
      "Epoch [13/80] - Train Loss: 0.9261, Val Loss: 0.7283, Val Accuracy: 0.7375\n",
      "Epoch [14/80] - Train Loss: 0.9343, Val Loss: 0.7911, Val Accuracy: 0.7375\n",
      "Early stopping triggered after 14 epochs\n",
      "\n",
      "P Model Test Accuracy: 0.7287\n",
      "\n",
      "P Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.43      0.80      0.56        20\n",
      "       LABEL       0.41      0.95      0.58        20\n",
      "          LI       1.00      0.96      0.98        28\n",
      "           P       0.96      0.62      0.76       120\n",
      "\n",
      "    accuracy                           0.73       188\n",
      "   macro avg       0.70      0.83      0.72       188\n",
      "weighted avg       0.85      0.73      0.75       188\n",
      "\n",
      "Saved P model to ../models/sub_classifiers/p_classifier.pth\n",
      "\n",
      "============================================================\n",
      "Training INPUT sub-classifier\n",
      "============================================================\n",
      "\n",
      "=== Preparing data for INPUT sub-classification ===\n",
      "Subtags: ['INPUT', 'DROPDOWN']\n",
      "Total samples: 109\n",
      "Distribution: \n",
      "tag\n",
      "INPUT       93\n",
      "DROPDOWN    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Training INPUT sub-classifier ===\n",
      "Training set size: 78\n",
      "SandardScaler set size: 14\n",
      "Test set size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80] - Train Loss: 5.0011, Val Loss: 0.4749, Val Accuracy: 0.8571\n",
      "Epoch [2/80] - Train Loss: 3.9266, Val Loss: 0.3897, Val Accuracy: 0.8571\n",
      "Epoch [3/80] - Train Loss: 3.3801, Val Loss: 0.2996, Val Accuracy: 0.8571\n",
      "Epoch [4/80] - Train Loss: 2.6273, Val Loss: 0.3540, Val Accuracy: 0.8571\n",
      "Epoch [5/80] - Train Loss: 1.7198, Val Loss: 0.3191, Val Accuracy: 0.8571\n",
      "Epoch [6/80] - Train Loss: 1.8836, Val Loss: 0.2551, Val Accuracy: 0.8571\n",
      "Epoch [7/80] - Train Loss: 1.2177, Val Loss: 0.2041, Val Accuracy: 0.7857\n",
      "Epoch [8/80] - Train Loss: 0.9887, Val Loss: 0.1126, Val Accuracy: 0.8571\n",
      "Epoch [9/80] - Train Loss: 0.5414, Val Loss: 0.0870, Val Accuracy: 0.8571\n",
      "Epoch [10/80] - Train Loss: 0.9007, Val Loss: 0.0877, Val Accuracy: 0.8571\n",
      "Epoch [11/80] - Train Loss: 1.1559, Val Loss: 0.0749, Val Accuracy: 0.9286\n",
      "Epoch [12/80] - Train Loss: 0.2304, Val Loss: 0.0776, Val Accuracy: 0.9286\n",
      "Epoch [13/80] - Train Loss: 0.2676, Val Loss: 0.0732, Val Accuracy: 0.9286\n",
      "Epoch [14/80] - Train Loss: 0.2585, Val Loss: 0.0510, Val Accuracy: 0.9286\n",
      "Epoch [15/80] - Train Loss: 0.1433, Val Loss: 0.0293, Val Accuracy: 0.9286\n",
      "Epoch [16/80] - Train Loss: 0.1451, Val Loss: 0.0298, Val Accuracy: 0.9286\n",
      "Epoch [17/80] - Train Loss: 0.1754, Val Loss: 0.0028, Val Accuracy: 1.0000\n",
      "Epoch [18/80] - Train Loss: 0.0757, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [19/80] - Train Loss: 0.0265, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [20/80] - Train Loss: 0.0750, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [21/80] - Train Loss: 0.0041, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [22/80] - Train Loss: 0.1832, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [23/80] - Train Loss: 0.0707, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [24/80] - Train Loss: 0.1086, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [25/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [26/80] - Train Loss: 0.0039, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [27/80] - Train Loss: 0.0107, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [28/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [29/80] - Train Loss: 0.0947, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [30/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [31/80] - Train Loss: 0.0412, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [32/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [33/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [34/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [35/80] - Train Loss: 0.0209, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [36/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [37/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [38/80] - Train Loss: 0.0001, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [39/80] - Train Loss: 0.0001, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [40/80] - Train Loss: 0.1028, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [41/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [42/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [43/80] - Train Loss: 0.0084, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [44/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [45/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [46/80] - Train Loss: 0.0502, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [47/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [48/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [49/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [50/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [51/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [52/80] - Train Loss: 0.0003, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [53/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [54/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [55/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [56/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [57/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [58/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [59/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [60/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [61/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [62/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [63/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [64/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [65/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [66/80] - Train Loss: 0.0004, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [67/80] - Train Loss: 0.0468, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [68/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [69/80] - Train Loss: 0.0315, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [70/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [71/80] - Train Loss: 0.0220, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [72/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [73/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [74/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [75/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [76/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [77/80] - Train Loss: 0.0000, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Epoch [78/80] - Train Loss: 0.0023, Val Loss: 0.0000, Val Accuracy: 1.0000\n",
      "Early stopping triggered after 78 epochs\n",
      "\n",
      "INPUT Model Test Accuracy: 1.0000\n",
      "\n",
      "INPUT Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    DROPDOWN       1.00      1.00      1.00         2\n",
      "       INPUT       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        17\n",
      "   macro avg       1.00      1.00      1.00        17\n",
      "weighted avg       1.00      1.00      1.00        17\n",
      "\n",
      "Saved INPUT model to ../models/sub_classifiers/input_classifier.pth\n",
      "\n",
      "Training completed!\n",
      "Models saved in '../models/sub_classifiers/' directory\n",
      "\n",
      "Loading trained models...\n",
      "Loading DIV model from ../models/sub_classifiers/div_classifier.pth\n",
      "Loaded DIV model (Test Accuracy: 0.8734)\n",
      "Loading P model from ../models/sub_classifiers/p_classifier.pth\n",
      "Loaded P model (Test Accuracy: 0.7287)\n",
      "Loading INPUT model from ../models/sub_classifiers/input_classifier.pth\n",
      "Loaded INPUT model (Test Accuracy: 1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_36632\\332203975.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "class ImprovedTagClassifier(nn.Module):\n",
    "    \"\"\"Base classifier architecture used for all sub-classification tasks\"\"\"\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.4):\n",
    "        super(ImprovedTagClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.skip1_3 = nn.Linear(512, 128)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.leaky_relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = self.fc2(x1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.leaky_relu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        x3 = self.fc3(x2)\n",
    "        skip_x1 = self.skip1_3(x1)\n",
    "        x3 = x3 + skip_x1\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = self.leaky_relu(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        \n",
    "        output = self.fc4(x3)\n",
    "        return output\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce_loss(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class MultiLevelTagClassifier:\n",
    "    \"\"\"Multi-level classification system for hierarchical tag prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.models = {}\n",
    "        self.preprocessors = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "        self.tag_hierarchy = {\n",
    "            'DIV': ['DIV', 'FOOTER', 'NAVBAR', 'LIST', 'CARD'],\n",
    "            'P': ['P', 'LABEL', 'LI', 'A'],\n",
    "            'INPUT': ['INPUT', 'DROPDOWN']\n",
    "        }\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def prepare_data_for_subtask(self, df, parent_tag, subtags):\n",
    "        \"\"\"Prepare data for a specific sub-classification task\"\"\"\n",
    "        filtered_df = df[df['tag'].isin(subtags)].copy()\n",
    "        \n",
    "        print(f\"\\n=== Preparing data for {parent_tag} sub-classification ===\")\n",
    "        print(f\"Subtags: {subtags}\")\n",
    "        print(f\"Total samples: {len(filtered_df)}\")\n",
    "        print(f\"Distribution: \\n{filtered_df['tag'].value_counts()}\")\n",
    "        \n",
    "        if len(filtered_df) == 0:\n",
    "            print(f\"No data found for {parent_tag} subtags!\")\n",
    "            return None, None, None, None, None, None\n",
    "        \n",
    "        y = filtered_df[\"tag\"]\n",
    "        X = filtered_df.drop(columns=[\"tag\"])\n",
    "        \n",
    "        # Define feature columns consistent with feature extraction\n",
    "        categorical_cols = ['type','prev_sibling_html_tag','child_1_html_tag','child_2_html_tag','parent_tag_html']\n",
    "        continuous_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "        \n",
    "        # Ensure all expected columns are present\n",
    "        missing_cols = [col for col in categorical_cols + continuous_cols if col not in X.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing columns {missing_cols} in data for {parent_tag}\")\n",
    "            for col in missing_cols:\n",
    "                X[col] = 'unknown' if col in categorical_cols else 0\n",
    "        \n",
    "        X[categorical_cols] = X[categorical_cols].astype(str).fillna('unknown')\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        X_cat_encoded = ohe.fit_transform(X[categorical_cols])\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_continuous_imputed = imputer.fit_transform(X[continuous_cols])\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_continuous_scaled = scaler.fit_transform(X_continuous_imputed)\n",
    "        \n",
    "        X_processed = np.concatenate([X_cat_encoded, X_continuous_scaled], axis=1)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "        class_counts = Counter(y_encoded)\n",
    "        min_samples_threshold = max(10, len(subtags) * 3)\n",
    "        \n",
    "        rare_classes = [cls for cls, count in class_counts.items() if count < min_samples_threshold]\n",
    "        \n",
    "        for cls in rare_classes:\n",
    "            idx = np.where(y_encoded == cls)[0]\n",
    "            original_class_name = label_encoder.inverse_transform([cls])[0]\n",
    "            samples_needed = min_samples_threshold - len(idx)\n",
    "            \n",
    "            print(f\"Augmenting class '{original_class_name}' with {samples_needed} additional samples\")\n",
    "            \n",
    "            for _ in range(samples_needed):\n",
    "                sample_idx = np.random.choice(idx)\n",
    "                new_sample = X_processed[sample_idx].copy()\n",
    "                \n",
    "                continuous_start = X_cat_encoded.shape[1]\n",
    "                noise = np.random.normal(0, 0.05, size=X_continuous_scaled.shape[1])\n",
    "                new_sample[continuous_start:] += noise\n",
    "                \n",
    "                X_processed = np.vstack([X_processed, new_sample])\n",
    "                y_encoded = np.append(y_encoded, cls)\n",
    "        \n",
    "        preprocessors = {\n",
    "            'ohe': ohe,\n",
    "            'imputer': imputer,\n",
    "            'scaler': scaler,\n",
    "            'label_encoder': label_encoder,\n",
    "            'categorical_cols': categorical_cols,\n",
    "            'continuous_cols': continuous_cols\n",
    "        }\n",
    "        \n",
    "        return X_processed, y_encoded, preprocessors, categorical_cols, continuous_cols, label_encoder\n",
    "    \n",
    "    def train_subtask_model(self, X, y, preprocessors, parent_tag, epochs=100):\n",
    "        \"\"\"Train a model for a specific sub-classification task\"\"\"\n",
    "        print(f\"\\n=== Training {parent_tag} sub-classifier ===\")\n",
    "        \n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=0.15, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set size: {X_train.shape[0]}\")\n",
    "        print(f\"SandardScaler set size: {X_val.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\")\n",
    "        \n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', \n",
    "            classes=np.unique(y_train), \n",
    "            y=y_train\n",
    "        )\n",
    "        \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "        \n",
    "        input_size = X_train.shape[1]\n",
    "        output_size = len(np.unique(y))\n",
    "        model = ImprovedTagClassifier(input_size, output_size).to(self.device)\n",
    "        \n",
    "        criterion = FocalLoss(weight=class_weights_tensor, gamma=2.0)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        \n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 15\n",
    "        counter = 0\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with autocast(device_type=self.device.type):\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            \n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                    \n",
    "                    with autocast(device_type=self.device.type):\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                    \n",
    "                    val_running_loss += loss.item()\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(batch_y.cpu().numpy())\n",
    "            \n",
    "            val_loss = val_running_loss / len(val_loader)\n",
    "            val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                outputs = model(batch_X)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "        print(f\"\\n{parent_tag} Model Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"\\n{parent_tag} Classification Report:\")\n",
    "        print(classification_report(\n",
    "            test_labels, \n",
    "            test_preds,\n",
    "            target_names=preprocessors['label_encoder'].classes_,\n",
    "            zero_division=0\n",
    "        ))\n",
    "        \n",
    "        return model, (train_losses, val_losses, val_accuracies), test_accuracy\n",
    "    \n",
    "    def train_all_models(self, df_path, epochs=100):\n",
    "        \"\"\"Train all three sub-classification models\"\"\"\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        df = pd.read_csv(df_path)\n",
    "        \n",
    "        df.loc[(df[\"tag\"] == \"SPAN\") & ((df[\"type\"] == \"RECTANGLE\") | (df[\"type\"] == \"GROUP\")), \"tag\"] = \"DIV\"\n",
    "        \n",
    "        children_cols = ['child_1_html_tag', 'child_2_html_tag']\n",
    "        for col in children_cols:\n",
    "            df[col] = df[col].apply(lambda x: \"DIV\" if isinstance(x, str) and '-' in x else x)\n",
    "        \n",
    "        for col in ['tag', 'prev_sibling_html_tag', 'child_1_html_tag', 'child_2_html_tag']:\n",
    "            df[col] = df[col].str.upper()\n",
    "        \n",
    "        os.makedirs('../models/sub_classifiers', exist_ok=True)\n",
    "        \n",
    "        for parent_tag, subtags in self.tag_hierarchy.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Training {parent_tag} sub-classifier\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            result = self.prepare_data_for_subtask(df, parent_tag, subtags)\n",
    "            if result[0] is None:\n",
    "                print(f\"Skipping {parent_tag} due to insufficient data\")\n",
    "                continue\n",
    "            \n",
    "            X, y, preprocessors, cat_cols, cont_cols, label_encoder = result\n",
    "            \n",
    "            model, training_history, test_accuracy = self.train_subtask_model(\n",
    "                X, y, preprocessors, parent_tag, epochs\n",
    "            )\n",
    "            \n",
    "            self.models[parent_tag] = model\n",
    "            self.preprocessors[parent_tag] = preprocessors\n",
    "            self.label_encoders[parent_tag] = label_encoder\n",
    "            \n",
    "            model_path = f'../models/sub_classifiers/{parent_tag.lower()}_classifier.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'input_size': X.shape[1],\n",
    "                'output_size': len(np.unique(y)),\n",
    "                'preprocessors': preprocessors,\n",
    "                'test_accuracy': test_accuracy\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"Saved {parent_tag} model to {model_path}\")\n",
    "            \n",
    "            self.plot_training_history(training_history, parent_tag)\n",
    "    \n",
    "    def plot_training_history(self, history, parent_tag):\n",
    "        \"\"\"Plot training history for a model\"\"\"\n",
    "        train_losses, val_losses, val_accuracies = history\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title(f'{parent_tag} Model: Loss over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "        plt.title(f'{parent_tag} Model: Accuracy over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'models/sub_classifiers/{parent_tag.lower()}_training_history.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def load_models(self, model_dir='../models/sub_classifiers'):\n",
    "        \"\"\"Load all trained sub-classification models\"\"\"\n",
    "        for parent_tag in self.tag_hierarchy.keys():\n",
    "            model_path = f'{model_dir}/{parent_tag.lower()}_classifier.pth'\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"Loading {parent_tag} model from {model_path}\")\n",
    "                checkpoint = torch.load(model_path, map_location=self.device)\n",
    "                \n",
    "                model = ImprovedTagClassifier(\n",
    "                    checkpoint['input_size'], \n",
    "                    checkpoint['output_size']\n",
    "                ).to(self.device)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model.eval()\n",
    "                \n",
    "                self.models[parent_tag] = model\n",
    "                self.preprocessors[parent_tag] = checkpoint['preprocessors']\n",
    "                self.label_encoders[parent_tag] = checkpoint['preprocessors']['label_encoder']\n",
    "                \n",
    "                print(f\"Loaded {parent_tag} model (Test Accuracy: {checkpoint['test_accuracy']:.4f})\")\n",
    "            else:\n",
    "                print(f\"Model file {model_path} not found!\")\n",
    "    \n",
    "    def predict_hierarchical(self, sample_data, base_prediction):\n",
    "        \"\"\"Make hierarchical predictions using the sub-classifiers\"\"\"\n",
    "        if base_prediction not in self.tag_hierarchy:\n",
    "            return base_prediction, 1.0\n",
    "        \n",
    "        if base_prediction not in self.models:\n",
    "            print(f\"No sub-classifier found for {base_prediction}\")\n",
    "            return base_prediction, 1.0\n",
    "        \n",
    "        preprocessors = self.preprocessors[base_prediction]\n",
    "        sample_df = pd.DataFrame([sample_data])\n",
    "        \n",
    "        cat_cols = preprocessors['categorical_cols']\n",
    "        cont_cols = preprocessors['continuous_cols']\n",
    "        \n",
    "        # Ensure all required columns are present\n",
    "        for col in cat_cols + cont_cols:\n",
    "            if col not in sample_df.columns:\n",
    "                sample_df[col] = 'unknown' if col in cat_cols else 0\n",
    "        \n",
    "        sample_df[cat_cols] = sample_df[cat_cols].astype(str).fillna('unknown')\n",
    "        X_cat = preprocessors['ohe'].transform(sample_df[cat_cols])\n",
    "        \n",
    "        X_cont = preprocessors['imputer'].transform(sample_df[cont_cols])\n",
    "        X_cont = preprocessors['scaler'].transform(X_cont)\n",
    "        \n",
    "        X_processed = np.concatenate([X_cat, X_cont], axis=1)\n",
    "        X_tensor = torch.tensor(X_processed, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        model = self.models[base_prediction]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        predicted_label = preprocessors['label_encoder'].inverse_transform([predicted.cpu().numpy()[0]])[0]\n",
    "        confidence = probabilities.max().item()\n",
    "        \n",
    "        return predicted_label, confidence\n",
    "\n",
    "def main():\n",
    "    classifier = MultiLevelTagClassifier()\n",
    "    \n",
    "    print(\"Starting multi-level tag classification training...\")\n",
    "    classifier.train_all_models(\"../Dataset/new_figma_dataset3.csv\", epochs=80)\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(\"Models saved in '../models/sub_classifiers/' directory\")\n",
    "    \n",
    "    print(\"\\nLoading trained models...\")\n",
    "    classifier.load_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
