{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib  # For saving encoders and scalers\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset and remove rows with '-' in the tag column\n",
    "df = pd.read_csv(\"../../feature_extraction/figma_dataset.csv\")\n",
    "df = df[~df['tag'].str.contains('-')]\n",
    "\n",
    "unique_tags = df['tag'].unique().tolist()\n",
    "\n",
    "# Define tag replacement rules\n",
    "txt_tags = ['B', 'CAPTION', 'EM', 'FIGCAPTION', 'I', 'H1', 'H2', 'H3', 'H4', 'H5', 'H6', \n",
    "            'LABEL', 'LI', 'TIME', 'TD', 'TH', 'U', 'P',  'SPAN', 'A', 'TXT', 'SMALL', 'ADDRESS', 'STRONG',\n",
    "            'SUMMARY', 'SUP']\n",
    "\n",
    "div_tags = ['ARTICLE', 'FIGURE', 'FOOTER', 'HEADER', 'MAIN', 'NAV', 'OL', 'UL', 'FORM', 'DETAILS', 'SECTION']\n",
    "\n",
    "\n",
    "button_tags = ['SELECT']\n",
    "\n",
    "# Apply replacements\n",
    "df['tag'] = df['tag'].apply(\n",
    "    lambda x: 'TEXT' if x in txt_tags else \n",
    "              'DIV' if x in div_tags else \n",
    "              'BUTTON' if x in div_tags else \n",
    "              x\n",
    ")\n",
    "\n",
    "allowed_tags = [\"DIV\", \"BUTTON\", \"INPUT\"]\n",
    "df = df[df[\"tag\"].isin(allowed_tags)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "columns_to_drop = [\n",
    "    'type', 'num_children', 'parent_tag', 'parent_tag_html', 'prev_sibling_tag', 'parent_prev_sibling_tag', \n",
    "    'sibling_count', 'has_background_color', 'text_length', 'nearest_text_distance', 'nearest_image_distance']\n",
    "\n",
    "# Safely drop columns that exist\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "# Save the modified dataframe to a new Excel file\n",
    "df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for nearest_text_content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1920/1920 [00:23<00:00, 80.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (61420, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Generate embeddings\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "text_contents = df['nearest_text_content'].fillna('').tolist()\n",
    "\n",
    "\n",
    "print(\"Generating embeddings for nearest_text_content...\")\n",
    "text_embeddings = sentence_model.encode(text_contents, show_progress_bar=True)\n",
    "print(f\"Embedding shape: {text_embeddings.shape}\")  # Should be (n_samples, embedding_dim)\n",
    "\n",
    "# 5. Create column names for the embeddings\n",
    "embedding_dim = text_embeddings.shape[1]\n",
    "embedding_cols = [f'text_embedding_{i}' for i in range(embedding_dim)]\n",
    "\n",
    "# 6. Add embeddings to the dataframe\n",
    "embedding_df = pd.DataFrame(text_embeddings, columns=embedding_cols)\n",
    "df_with_embeddings = pd.concat([df, embedding_df], axis=1)\n",
    "\n",
    "# 7. Drop the original text content column since we now have embeddings\n",
    "df_with_embeddings = df_with_embeddings.drop(columns=['nearest_text_content'])\n",
    "\n",
    "# 8. Optionally, handle the nearest_image_size column\n",
    "# If it's not numeric, you might want to process it separately or drop it\n",
    "if 'nearest_image_size' in df_with_embeddings.columns and df_with_embeddings['nearest_image_size'].dtype == 'object':\n",
    "    # If it contains dimensions like '324x240', extract features\n",
    "    try:\n",
    "        # Try to convert to numeric directly\n",
    "        df_with_embeddings['nearest_image_size'] = pd.to_numeric(df_with_embeddings['nearest_image_size'])\n",
    "    except:\n",
    "        # If that fails, it might be in the format of dimensions, so drop it or process it\n",
    "        df_with_embeddings = df_with_embeddings.drop(columns=['nearest_image_size'])\n",
    "\n",
    "# 9. Save the dataframe with embeddings\n",
    "df = df_with_embeddings\n",
    "\n",
    "# 10. Now continue with model training as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate features and target\n",
    "y = df[\"tag\"]\n",
    "X = df.drop(columns=[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identify categorical and continuous columns\n",
    "categorical_cols = []\n",
    "continuous_cols = [col for col in X.columns if col not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process categorical features with LabelEncoder\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    # If you need to save individual encoders, consider saving them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values in continuous columns and scale them\n",
    "X[continuous_cols] = X[continuous_cols].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X[continuous_cols])\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace continuous columns in X with their scaled values\n",
    "X_scaled = X.copy()\n",
    "X_scaled[continuous_cols] = X_continuous_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "# Note: X_train and X_test are DataFrames, so use .values to convert to NumPy arrays.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define the Neural Network Model with non-linear activations between linear layers\n",
    "class TagClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TagClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 128)           # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 256)            # Third hidden layer\n",
    "        self.fc4 = nn.Linear(256, 512)            # Fourth hidden layer\n",
    "        self.fc5 = nn.Linear(512, 512)            # Fifth hidden layer\n",
    "        self.fc6 = nn.Linear(512, 512)            # Sixth hidden layer\n",
    "        self.fc7 = nn.Linear(512, 256)            # Seventh hidden layer\n",
    "        self.fc8 = nn.Linear(256, 128)           # Eighth hidden layer\n",
    "        self.fc9 = nn.Linear(128, output_size)   # Output layer\n",
    "        self.relu = nn.ReLU()                  # Non-linear activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.relu(self.fc8(x))\n",
    "        logits = self.fc9(x)  # No activation here: CrossEntropyLoss expects raw logits.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = len(label_encoder.classes_)\n",
    "model = TagClassifier(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Internally applies softmax on logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.1395\n",
      "Epoch [20/100], Loss: 0.1375\n",
      "Epoch [30/100], Loss: 0.1355\n",
      "Epoch [40/100], Loss: 0.1336\n",
      "Epoch [50/100], Loss: 0.1317\n",
      "Epoch [60/100], Loss: 0.1296\n",
      "Epoch [70/100], Loss: 0.1276\n",
      "Epoch [80/100], Loss: 0.1254\n",
      "Epoch [90/100], Loss: 0.1233\n",
      "Epoch [100/100], Loss: 0.1211\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 9. Training loop\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"tag_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9604\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BUTTON       0.00      0.00      0.00       358\n",
      "         DIV       0.96      1.00      0.98     11798\n",
      "       INPUT       0.00      0.00      0.00       128\n",
      "\n",
      "    accuracy                           0.96     12284\n",
      "   macro avg       0.32      0.33      0.33     12284\n",
      "weighted avg       0.92      0.96      0.94     12284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kareem alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kareem alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kareem alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            labels=np.unique(y_test),\n",
    "                            target_names=label_encoder.inverse_transform(np.unique(y_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
