{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AOZ\\AppData\\Local\\Temp\\ipykernel_27168\\3379318441.py:2: DtypeWarning: Columns (18,19,23,24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('figma_dataset3.csv', encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7641391\n"
     ]
    }
   ],
   "source": [
    "# Load dataset with appropriate encoding to avoid decoding errors\n",
    "df = pd.read_csv('figma_dataset3.csv', encoding='latin1')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tag   type  x  y  width   height characters  depth  num_children  \\\n",
      "0  BODY  FRAME  0  0  800.0  11403.0        NaN      0             3   \n",
      "1   DIV  GROUP  0  0  800.0      0.0        NaN      1             0   \n",
      "2   DIV  GROUP  0  0  800.0  11403.0        NaN      1             1   \n",
      "3   DIV  GROUP  0  0  800.0  11403.0        NaN      2             8   \n",
      "4   DIV  GROUP  0  0  800.0      0.0        NaN      3             1   \n",
      "\n",
      "  parent_tag  ...  x_normalized  y_normalized  x_center y_center x_quarter  \\\n",
      "0        NaN  ...             1          9999     400.0   5701.5       0.5   \n",
      "1       BODY  ...             1          9999     400.0      0.0       0.5   \n",
      "2       BODY  ...             1          9999     400.0   5701.5       0.5   \n",
      "3        DIV  ...             1          9999     400.0   5701.5       0.5   \n",
      "4        DIV  ...             1          9999     400.0      0.0       0.5   \n",
      "\n",
      "  y_quarter  aspect_ratio       area normalized_width normalized_height  \n",
      "0       0.5      0.070157  9122400.0              1.0               1.0  \n",
      "1       0.0           NaN        0.0              1.0               0.0  \n",
      "2       0.5      0.070157  9122400.0              1.0               1.0  \n",
      "3       0.5      0.070157  9122400.0              1.0               1.0  \n",
      "4       0.0           NaN        0.0              1.0               0.0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows to understand the structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column categories based on the dataset attributes\n",
    "categorical_cols = ['type', 'characters', 'parent_tag', 'is_leaf','font_weight','color','contains_special_chars','background_color','border_color','shadow_color','contains_number', 'text_type', 'visibility', 'border_type', 'border_pattern', 'shadow_type','shadow_offset']\n",
    "numerical_cols = ['x', 'y', 'width', 'height', 'depth', 'num_children', 'sibling_count', 'font_size', 'border_radius',  'border_opacity', 'border_weight', 'shadow_radius', 'text_length', 'word_count',  'x_normalized', 'y_normalized', 'x_center', 'y_center', 'x_quarter', 'y_quarter', 'aspect_ratio', 'area','normalized_width', 'normalized_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric values from numerical columns to prevent conversion errors\n",
    "df[numerical_cols] = df[numerical_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all categorical columns are treated as strings before encoding\n",
    "df[categorical_cols] = df[categorical_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using Label Encoding\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "tag_encoder = LabelEncoder()\n",
    "df['tag'] = tag_encoder.fit_transform(df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into features and target\n",
    "X = df.drop(columns=['tag'])  # Features\n",
    "y = df['tag']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN\n",
    "X[numerical_cols] = X[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
    "X[numerical_cols] = X[numerical_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[numerical_cols].isnull().sum())\n",
    "# print(df[numerical_cols].median())\n",
    "# print(X[numerical_cols].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a pre-trained model exists\n",
    "model_filename = \"models/html_tag_model.pkl\"\n",
    "scaler_filename = \"models/scaler.pkl\"\n",
    "tag_encoder_filename = \"models/tag_encoder.pkl\"\n",
    "label_encoders_filename = \"models/label_encoders.pkl\"\n",
    "\n",
    "try:\n",
    "    model = joblib.load(model_filename)\n",
    "    print(\"Loaded pre-trained model.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-trained model found, training a new one.\")\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=200, \n",
    "        learning_rate=0.1, \n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Helps to address the class imbalance\n",
    "        force_col_wise=True\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model and encoders\n",
    "    joblib.dump(model, model_filename)\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    joblib.dump(tag_encoder, tag_encoder_filename)\n",
    "    joblib.dump(label_encoders, label_encoders_filename)\n",
    "    print(\"Model saved for future use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate model performance\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test, y_pred, labels=np.unique(y_test), target_names=tag_encoder.inverse_transform(np.unique(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define column categories\n",
    "categorical_cols = ['type', 'characters', 'parent_tag', 'is_leaf', 'font_weight', 'color', 'contains_special_chars',\n",
    "                    'background_color', 'border_color', 'shadow_color', 'contains_number', 'text_type', 'visibility', \n",
    "                    'border_type', 'border_pattern', 'shadow_type', 'shadow_offset']\n",
    "numerical_cols = ['x', 'y', 'width', 'height', 'depth', 'num_children', 'sibling_count', 'font_size', 'border_radius',  \n",
    "                  'border_opacity', 'border_weight', 'shadow_radius', 'text_length', 'word_count',  'x_normalized', \n",
    "                  'y_normalized', 'x_center', 'y_center', 'x_quarter', 'y_quarter', 'aspect_ratio', 'area',\n",
    "                  'normalized_width', 'normalized_height']\n",
    "\n",
    "# Function to load and preprocess data in chunks using Dask\n",
    "def load_and_preprocess_in_chunks(file_path, chunk_size=100000):\n",
    "    # Load the dataset in chunks with dtypes specified\n",
    "    ddf = dd.read_csv(file_path, encoding='latin1')\n",
    "    \n",
    "    # Filter the columns we need\n",
    "    ddf = ddf[categorical_cols + numerical_cols + ['tag']]\n",
    "    \n",
    "    # Convert categorical columns to string and then to category type\n",
    "    for col in categorical_cols:\n",
    "        ddf[col] = ddf[col].astype('str')  # Ensure that categorical columns are treated as strings\n",
    "        ddf[col] = ddf[col].astype('category')  # Convert to categorical\n",
    "        ddf[col] = ddf[col].cat.as_known()  # Set known categories explicitly\n",
    "    \n",
    "    # Preprocess categorical columns using one-hot encoding\n",
    "    ddf_categorical = dd.get_dummies(ddf[categorical_cols], dummy_na=True)\n",
    "    \n",
    "    # Normalize numerical columns (fit the scaler on the entire data for correct mean/std)\n",
    "    scaler = StandardScaler()\n",
    "    ddf_numerical = ddf[numerical_cols].map_partitions(scaler.fit_transform)\n",
    "    \n",
    "    # Combine categorical and numerical features\n",
    "    X_processed = dd.concat([ddf_categorical, ddf_numerical], axis=1)\n",
    "    \n",
    "    # Encode the target column\n",
    "    tag_encoder = LabelEncoder()\n",
    "    y_encoded = tag_encoder.fit_transform(ddf['tag'])\n",
    "    \n",
    "    # Compute the Dask DataFrame and return the result\n",
    "    X_processed = X_processed.compute()\n",
    "    y_encoded = y_encoded.compute()\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, tag_encoder\n",
    "\n",
    "\n",
    "# Create and compile model\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test, tag_encoder):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes, labels=np.unique(y_test),\n",
    "                                target_names=tag_encoder.inverse_transform(np.unique(y_test))))\n",
    "\n",
    "# Train and evaluate model\n",
    "def train_and_evaluate_model(file_path):\n",
    "    # Load and preprocess the data\n",
    "    X_train, X_val, y_train, y_val, tag_encoder = load_and_preprocess_in_chunks(file_path)\n",
    "    \n",
    "    # Create the model\n",
    "    num_classes = len(tag_encoder.classes_)\n",
    "    model = create_model(input_shape=(X_train.shape[1],), num_classes=num_classes)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "    \n",
    "    # Save the model\n",
    "    model.save('tag_predictor_model.h5')\n",
    "    print(\"Model saved as 'tag_predictor_model.h5'\")\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    evaluate_model(model, X_val, y_val, tag_encoder)\n",
    "    \n",
    "    return model, tag_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model('figma_dataset3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
